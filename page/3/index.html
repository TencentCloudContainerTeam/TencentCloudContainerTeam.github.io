<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>腾讯云容器团队</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="container kubernetes tencentcloud">
<meta property="og:type" content="website">
<meta property="og:title" content="腾讯云容器团队">
<meta property="og:url" content="https://TencentCloudContainerTeam.github.io/page/3/index.html">
<meta property="og:site_name" content="腾讯云容器团队">
<meta property="og:locale" content="zh-cn">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="腾讯云容器团队">
  
    <link rel="alternate" href="/atom.xml" title="腾讯云容器团队" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">腾讯云容器团队</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://TencentCloudContainerTeam.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-istio-analysis-1" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/11/istio-analysis-1/" class="article-date">
  <time datetime="2019-03-11T07:30:00.000Z" itemprop="datePublished">2019-03-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/11/istio-analysis-1/">istio 庖丁解牛(一) 组件概览</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imfox.me/" target="_blank" rel="noopener">钟华</a></p>
<p>Istio 作为 Service Mesh 领域的集大成者, 提供了流控, 安全, 遥测等模型, 其功能复杂, 模块众多,  有较高的学习和使用门槛,  本文会对istio 1.1 的各组件进行分析, 希望能帮助读者了解istio各组件的职责、以及相互的协作关系.</p>
<meta name="referrer" content="no-referrer">

<h2 id="1-istio-组件构成"><a href="#1-istio-组件构成" class="headerlink" title="1. istio 组件构成"></a>1. istio 组件构成</h2><p>以下是istio 1.1 官方架构图:</p>
<p><img src="https://preliminary.istio.io/docs/concepts/what-is-istio/arch.svg" width="80%"></p>
<p>虽然Istio 支持多个平台, 但将其与 Kubernetes 结合使用，其优势会更大, Istio 对Kubernetes 平台支持也是最完善的, 本文将基于Istio + Kubernetes 进行展开.</p>
<p>如果安装了grafana, prometheus, kiali, jaeger等组件的情况下, 一个完整的控制面组件包括以下pod:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">% kubectl -n istio-system get pod</span><br><span class="line">NAME                                          READY     STATUS</span><br><span class="line">grafana-5f54556df5-s4xr4                      1/1       Running</span><br><span class="line">istio-citadel-775c6cfd6b-8h5gt                1/1       Running</span><br><span class="line">istio-galley-675d75c954-kjcsg                 1/1       Running</span><br><span class="line">istio-ingressgateway-6f7b477cdd-d8zpv         1/1       Running</span><br><span class="line">istio-pilot-7dfdb48fd8-92xgt                  2/2       Running</span><br><span class="line">istio-policy-544967d75b-p6qkk                 2/2       Running</span><br><span class="line">istio-sidecar-injector-5f7894f54f-w7f9v       1/1       Running</span><br><span class="line">istio-telemetry-777876dc5d-msclx              2/2       Running</span><br><span class="line">istio-tracing-5fbc94c494-558fp                1/1       Running</span><br><span class="line">kiali-7c6f4c9874-vzb4t                        1/1       Running</span><br><span class="line">prometheus-66b7689b97-w9glt                   1/1       Running</span><br></pre></td></tr></table></figure>
<p>将istio系统组件细化到进程级别, 大概是这个样子:</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g187gshs79j315m0u0qct.jpg" referrerpolicy="no-referrer"></p>
<p><a href="https://ws4.sinaimg.cn/large/006tKfTcgy1g187dn7s1tj315m0u0x6t.jpg" referrerpolicy="no-referrer" target="_blank">查看高清原图</a></p>
<p>Service Mesh 的Sidecar 模式要求对数据面的用户Pod进行代理的注入, 注入的代理容器会去处理服务治理领域的各种「脏活累活」, 使得用户容器可以专心处理业务逻辑.</p>
<p>从上图可以看出, Istio 控制面本身就是一个复杂的微服务系统, 该系统包含多个组件Pod, 每个组件 各司其职, 既有单容器Pod, 也有多容器Pod, 既有单进程容器, 也有多进程容器,   每个组件会调用不同的命令, 各组件之间会通过RPC进行协作, 共同完成对数据面用户服务的管控.</p>
<hr>
<h2 id="2-Istio-源码-镜像和命令"><a href="#2-Istio-源码-镜像和命令" class="headerlink" title="2. Istio 源码, 镜像和命令"></a>2. Istio 源码, 镜像和命令</h2><p>Isito 项目代码主要由以下2个git 仓库组成:</p>
<table>
<thead>
<tr>
<th>仓库地址</th>
<th>语言</th>
<th>模块</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/istio/istio" target="_blank" rel="noopener">https://github.com/istio/istio</a></td>
<td>Go</td>
<td>包含istio控制面的大部分组件: pilot, mixer, citadel, galley, sidecar-injector等,</td>
</tr>
<tr>
<td><a href="https://github.com/istio/proxy" target="_blank" rel="noopener">https://github.com/istio/proxy</a></td>
<td>C++</td>
<td>包含 istio 使用的边车代理, 这个边车代理包含envoy和mixer client两块功能</td>
</tr>
</tbody>
</table>
<h3 id="2-1-istio-istio"><a href="#2-1-istio-istio" class="headerlink" title="2.1 istio/istio"></a>2.1 istio/istio</h3><p><a href="https://github.com/istio/istio" target="_blank" rel="noopener">https://github.com/istio/istio</a> 包含的主要的镜像和命令:</p>
<table>
<thead>
<tr>
<th>容器名</th>
<th>镜像名</th>
<th>启动命令</th>
<th>源码入口</th>
</tr>
</thead>
<tbody>
<tr>
<td>Istio_init</td>
<td>istio/proxy_init</td>
<td>istio-iptables.sh</td>
<td>istio/tools/deb/istio-iptables.sh</td>
</tr>
<tr>
<td>istio-proxy</td>
<td>istio/proxyv2</td>
<td>pilot-agent</td>
<td>istio/pilot/cmd/pilot-agent</td>
</tr>
<tr>
<td>sidecar-injector-webhook</td>
<td>istio/sidecar_injector</td>
<td>sidecar-injector</td>
<td>istio/pilot/cmd/sidecar-injector</td>
</tr>
<tr>
<td>discovery</td>
<td>istio/pilot</td>
<td>pilot-discovery</td>
<td>istio/pilot/cmd/pilot-discovery</td>
</tr>
<tr>
<td>galley</td>
<td>istio/galley</td>
<td>galley</td>
<td>istio/galley/cmd/galley</td>
</tr>
<tr>
<td>mixer</td>
<td>istio/mixer</td>
<td>mixs</td>
<td>istio/mixer/cmd/mixs</td>
</tr>
<tr>
<td>citadel</td>
<td>istio/citadel</td>
<td>istio_ca</td>
<td>istio/security/cmd/istio_ca</td>
</tr>
</tbody>
</table>
<p>另外还有2个命令不在上图中使用:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>源码入口</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>mixc</td>
<td>istio/mixer/cmd/mixc</td>
<td>用于和Mixer server 交互的客户端</td>
</tr>
<tr>
<td>node_agent</td>
<td>istio/security/cmd/node_agent</td>
<td>用于node上安装安全代理, 这在Mesh Expansion特性中会用到, 即k8s和vm打通.</td>
</tr>
</tbody>
</table>
<h3 id="2-2-istio-proxy"><a href="#2-2-istio-proxy" class="headerlink" title="2.2 istio/proxy"></a>2.2 istio/proxy</h3><p><a href="https://github.com/istio/proxy" target="_blank" rel="noopener">https://github.com/istio/proxy</a>  该项目本身不会产出镜像, 它可以编译出一个<code>name = &quot;Envoy&quot;</code>的二进制程序, 该二进制程序会被ADD到istio的边车容器镜像<code>istio/proxyv2</code>中.</p>
<p>istio proxy 项目使用的编译方式是Google出品的bazel,  bazel可以直接在编译中引入第三方库，加载第三方源码.</p>
<p>这个项目包含了对Envoy源码的引用，还在此基础上进行了扩展，这些扩展是通过Envoy filter（过滤器）的形式来提供，这样做的目的是让边车代理将策略执行决策委托给Mixer，因此可以理解istio proxy 这个项目有2大功能模块:</p>
<ol>
<li>Envoy: 使用到Envoy的全部功能</li>
<li>mixer client: 测量和遥测相关的客户端实现, 基于Envoy做扩展，通过RPC和Mixer server 进行交互,  实现策略管控和遥测</li>
</ol>
<p>后续我将对以上各个模块、命令以及它们之间的协作进行探究.</p>
<hr>
<h2 id="3-Istio-Pod-概述"><a href="#3-Istio-Pod-概述" class="headerlink" title="3. Istio Pod 概述"></a>3. Istio Pod 概述</h2><h3 id="3-1-数据面用户Pod"><a href="#3-1-数据面用户Pod" class="headerlink" title="3.1 数据面用户Pod"></a>3.1 数据面用户Pod</h3><p>数据面用户Pod注入的内容包括:</p>
<ol>
<li><p>initContainer <code>istio-init</code>:  通过配置iptables来劫持Pod中的流量, 转发给envoy</p>
</li>
<li><p>sidecar container <code>istio-proxy</code>:  包含2个进程, 父进程pliot-agent 初始化并管控envoy, 子进程envoy除了包含原生envoy的功能外, 还加入了mixer client的逻辑.</p>
<p>主要端口:</p>
<ul>
<li><code>--statusPort</code>  status server 端口, 默认为0, 表示不启动, istio启动时通常传递为15020, 由pliot-agent监听</li>
<li><code>--proxyAdminPort</code> 代理管理端口, 默认 15000, 由子进程envoy监听.</li>
</ul>
</li>
</ol>
<h3 id="3-2-istio-sidecar-injector"><a href="#3-2-istio-sidecar-injector" class="headerlink" title="3.2 istio-sidecar-injector"></a>3.2 istio-sidecar-injector</h3><p>包含一个单容器,  <code>sidecar-injector-webhook</code>: 启动一个http server, 接受kube api server 的Admission Webhook 请求, 对用户pod进行sidecar注入.</p>
<p>进程为<code>sidecar-injector</code>, 主要监听端口:</p>
<ul>
<li><code>--port</code> Webhook服务端口,  默认443, 通过k8s service<code>istio-sidecar-injector</code> 对外提供服务.</li>
</ul>
<h3 id="3-3-istio-galley"><a href="#3-3-istio-galley" class="headerlink" title="3.3 istio-galley"></a>3.3 istio-galley</h3><p>包含一个单容器 <code>galley</code>: 提供 istio 中的配置管理服务, 验证Istio的CRD 资源的合法性.</p>
<p>进程为<code>galley server ......</code>, 主要监听端口:</p>
<ul>
<li><code>--server-address</code> galley gRPC 地址, 默认是tcp://0.0.0.0:9901</li>
<li><p><code>--validation-port</code>  https端口, 提供验证crd合法性服务的端口, 默认443.</p>
</li>
<li><p><code>--monitoringPort</code> http 端口, self-monitoring 端口, 默认 15014</p>
</li>
</ul>
<p>以上端口通过k8s service<code>istio-galley</code>对外提供服务</p>
<h3 id="3-4-istio-pilot"><a href="#3-4-istio-pilot" class="headerlink" title="3.4 istio-pilot"></a>3.4 istio-pilot</h3><p>pilot组件核心Pod, 对接平台适配层, 抽象服务注册信息、流量控制模型等, 封装统一的 API，供 Envoy 调用获取.</p>
<p>包含以下容器:</p>
<ol>
<li><p>sidecar container <code>istio-proxy</code></p>
</li>
<li><p>container <code>discovery</code>:  进程为<code>pilot-discovery discovery ......</code></p>
<p>主要监听端口:</p>
<ul>
<li>15010: 通过grpc 提供的 xds 获取接口</li>
<li><p>15011: 通过https 提供的 xds 获取接口</p>
</li>
<li><p>8080:  通过http 提供的 xds 获取接口, 兼容v1版本, 另外 http readiness 探针 <code>/ready</code>也在该端口</p>
</li>
<li><code>--monitoringPort</code> http self-monitoring 端口, 默认 15014</li>
</ul>
<p>以上端口通过k8s service<code>istio-pilot</code>对外提供服务</p>
</li>
</ol>
<h3 id="3-5-istio-telemetry-和istio-policy"><a href="#3-5-istio-telemetry-和istio-policy" class="headerlink" title="3.5 istio-telemetry 和istio-policy"></a>3.5 istio-telemetry 和istio-policy</h3><p>mixer 组件包含2个pod, istio-telemetry 和 istio-policy, istio-telemetry负责遥测功能,  istio-policy 负责策略控制, 它们分别包含2个容器:</p>
<ol>
<li><p>sidecar container<code>istio-proxy</code></p>
</li>
<li><p><code>mixer</code>: 进程为 <code>mixs server ……</code></p>
<p>主要监听端口:</p>
<ul>
<li>9091: grpc-mixer</li>
<li><p>15004: grpc-mixer-mtls</p>
</li>
<li><p><code>--monitoring-port</code>: http self-monitoring 端口, 默认 15014, liveness 探针<code>/version</code></p>
</li>
</ul>
</li>
</ol>
<h3 id="3-7-istio-citadel"><a href="#3-7-istio-citadel" class="headerlink" title="3.7 istio-citadel"></a>3.7 istio-citadel</h3><p>负责安全和证书管理的Pod, 包含一个单容器 <code>citadel</code></p>
<p>启动命令<code>/usr/local/bin/istio_ca --self-signed-ca ......</code> 主要监听端口:</p>
<ul>
<li><p><code>--grpc-port</code> citadel grpc 端口, 默认8060</p>
</li>
<li><p><code>--monitoring-port</code>: http self-monitoring 端口, 默认 15014, liveness 探针<code>/version</code></p>
</li>
</ul>
<p>以上端口通过k8s service<code>istio-citadel</code>对外提供服务</p>
<hr>
<p>后续将对各组件逐一进行分析.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/03/11/istio-analysis-1/" data-id="ckac6j62d0009a6u66pmxr3pg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-servicemesh-istio" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/31/servicemesh-istio/" class="article-date">
  <time datetime="2019-01-31T07:30:00.000Z" itemprop="datePublished">2019-01-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/31/servicemesh-istio/">Istio 服务网格领域的新王者</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://github.com/zhongfox" target="_blank" rel="noopener">钟华</a></p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/istio/title.png" alt="istio"></p>
<p>今天分享的内容主要包括以下4个话题:</p>
<ul>
<li>1 Service Mesh: 下一代微服务</li>
<li>2 Istio: 第二代 Service Mesh</li>
<li>3 Istio 数据面</li>
<li>4 Istio 控制面</li>
</ul>
<p>首先我会和大家一起过一下 Service Mesh的发展历程, 并看看Istio 为 Service Mesh 带来了什么, 这部分相对比较轻松. 接下来我将和大家分析一下Istio的主要架构, 重点是数据面和控制面的实现, 包括sidecar的注入, 流量拦截, xDS介绍, Istio流量模型, 分布式跟踪, Mixer 的适配器模型等等, 中间也会穿插着 istio的现场使用demo.</p>
<hr>
<h1 id="1-Service-Mesh-下一代微服务"><a href="#1-Service-Mesh-下一代微服务" class="headerlink" title="1. Service Mesh: 下一代微服务"></a>1. Service Mesh: 下一代微服务</h1><ul>
<li>应用通信模式演进</li>
<li>Service Mesh(服务网格)的出现</li>
<li>第二代 Service Mesh</li>
<li>Service Mesh 的定义</li>
<li>Service Mesh 产品简史</li>
<li>国内Service Mesh 发展情况</li>
</ul>
<hr>
<h2 id="1-1-应用通信模式演进-网络流控进入操作系统"><a href="#1-1-应用通信模式演进-网络流控进入操作系统" class="headerlink" title="1.1 应用通信模式演进: 网络流控进入操作系统"></a>1.1 应用通信模式演进: 网络流控进入操作系统</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.1.png"></p>
<p>在计算机网络发展的初期, 开发人员需要在自己的代码中处理服务器之间的网络连接问题, 包括流量控制, 缓存队列, 数据加密等. 在这段时间内底层网络逻辑和业务逻辑是混杂在一起.</p>
<p>随着技术的发展，TCP/IP 等网络标准的出现解决了流量控制等问题。尽管网络逻辑代码依然存在，但已经从应用程序里抽离出来，成为操作系统网络层的一部分, 形成了经典的网络分层模式.</p>
<hr>
<h2 id="1-2-应用通信模式演进-微服务架构的出现"><a href="#1-2-应用通信模式演进-微服务架构的出现" class="headerlink" title="1.2 应用通信模式演进: 微服务架构的出现"></a>1.2 应用通信模式演进: 微服务架构的出现</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.2.png"></p>
<p>微服务架构是更为复杂的分布式系统，它给运维带来了更多挑战, 这些挑战主要包括资源的有效管理和服务之间的治理, 如:</p>
<ul>
<li>服务注册, 服务发现</li>
<li>服务伸缩</li>
<li>健康检查</li>
<li>快速部署</li>
<li>服务容错: 断路器, 限流, 隔离舱, 熔断保护, 服务降级等等</li>
<li>认证和授权</li>
<li>灰度发布方案</li>
<li>服务调用可观测性, 指标收集</li>
<li>配置管理</li>
</ul>
<p>在微服务架构的实现中，为提升效率和降低门槛，应用开发者会基于微服务框架来实现微服务。微服务框架一定程度上为使用者屏蔽了底层网络的复杂性及分布式场景下的不确定性。通过API/SDK的方式提供服务注册发现、服务RPC通信、服务配置管理、服务负载均衡、路由限流、容错、服务监控及治理、服务发布及升级等通用能力, 比较典型的产品有:</p>
<ul>
<li>分布式RPC通信框架: COBRA, WebServices, Thrift, GRPC 等</li>
<li>服务治理特定领域的类库和解决方案: Hystrix, Zookeeper, Zipkin, Sentinel 等</li>
<li>对多种方案进行整合的微服务框架: SpringCloud、Finagle、Dubbox 等</li>
</ul>
<p>实施微服务的成本往往会超出企业的预期(内容多, 门槛高), 花在服务治理上的时间成本甚至可能高过进行产品研发的时间. 另外上述的方案会限制可用的工具、运行时和编程语言。微服务软件库一般专注于某个平台, 这使得异构系统难以兼容, 存在重复的工作, 系统缺乏可移植性.</p>
<p>Docker 和Kubernetes 技术的流行, 为Pass资源的分配管理和服务的部署提供了新的解决方案, 但是微服务领域的其他服务治理问题仍然存在.</p>
<hr>
<h2 id="1-3-Sidecar-模式的兴起"><a href="#1-3-Sidecar-模式的兴起" class="headerlink" title="1.3 Sidecar 模式的兴起"></a>1.3 Sidecar 模式的兴起</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.3.png"></p>
<p>Sidecar(有时会叫做agent) 在原有的客户端和服务端之间加多了一个代理, 为应用程序提供的额外的功能, 如服务发现, 路由代理, 认证授权, 链路跟踪 等等.</p>
<p>业界使用Sidecar 的一些先例:</p>
<ul>
<li>2013 年，Airbnb 开发了Synapse 和 Nerve，是sidecar的一种开源实现</li>
<li>2014 年, Netflix 发布了Prana，它也是一个sidecar，可以让非 JVM 应用接入他们的 NetflixOSS 生态系统</li>
</ul>
<hr>
<h2 id="1-4-Service-Mesh-服务网格-的出现"><a href="#1-4-Service-Mesh-服务网格-的出现" class="headerlink" title="1.4 Service Mesh(服务网格)的出现"></a>1.4 Service Mesh(服务网格)的出现</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.4.png"></p>
<p>直观地看, Sidecar 到 Service Mesh 是一个规模的升级, 不过Service Mesh更强调的是:</p>
<ul>
<li>不再将Sidecar(代理)视为单独的组件，而是强调由这些代理连接而形成的网络</li>
<li>基础设施, 对应用程序透明</li>
</ul>
<hr>
<h2 id="1-5-Service-Mesh-定义"><a href="#1-5-Service-Mesh-定义" class="headerlink" title="1.5 Service Mesh 定义"></a>1.5 Service Mesh 定义</h2><p>以下是Linkerd的CEO <a href="https://twitter.com/wm" target="_blank" rel="noopener">Willian Morgan</a>给出的Service Mesh的定义:</p>
<blockquote>
<p>A Service Mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the Service Mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.</p>
</blockquote>
<p>服务网格（Service Mesh）是致力于解决服务间通讯的<strong>基础设施层</strong>。它负责在现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，Service Mesh 通常是通过一组<strong>轻量级网络代理</strong>（Sidecar proxy），与应用程序代码部署在一起来实现，且<strong>对应用程序透明</strong>。</p>
<hr>
<h2 id="1-6-第二代-Service-Mesh"><a href="#1-6-第二代-Service-Mesh" class="headerlink" title="1.6 第二代 Service Mesh"></a>1.6 第二代 Service Mesh</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.6.png"></p>
<p>控制面板对每一个代理实例了如指掌，通过控制面板可以实现代理的访问控制和度量指标收集, 提升了服务网格的可观测性和管控能力, Istio 正是这类系统最为突出的代表.</p>
<hr>
<h2 id="1-7-Service-Mesh-产品简史"><a href="#1-7-Service-Mesh-产品简史" class="headerlink" title="1.7 Service Mesh 产品简史"></a>1.7 Service Mesh 产品简史</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.7.png"></p>
<ul>
<li><p>2016 年 1 月 15 日，前 Twitter 的基础设施工程师 <a href="https://twitter.com/wm" target="_blank" rel="noopener">William Morgan</a> 和 Oliver Gould，在 GitHub 上发布了 Linkerd 0.0.7 版本，采用Scala编写, 他们同时组建了一个创业小公司 Buoyant，这是业界公认的第一个Service Mesh </p>
</li>
<li><p>2016 年，<a href="https://twitter.com/mattklein123" target="_blank" rel="noopener">Matt Klein</a>在 Lyft 默默地进行 Envoy 的开发。Envoy 诞生的时间其实要比 Linkerd 更早一些，只是在 Lyft 内部不为人所知</p>
</li>
<li><p>2016 年 9 月 29 日在 SF Microservices 上，“Service Mesh”这个词汇第一次在公开场合被使用。这标志着“Service Mesh”这个词，从 Buoyant 公司走向社区.</p>
</li>
<li><p>2016 年 9 月 13 日，Matt Klein 宣布 Envoy 在 GitHub 开源，直接发布 1.0.0 版本。</p>
</li>
<li><p>2016 年下半年，Linkerd 陆续发布了 0.8 和 0.9 版本，开始支持 HTTP/2 和 gRPC，1.0 发布在即；同时，借助 Service Mesh 在社区的认可度，Linkerd 在年底开始申请加入 CNCF</p>
</li>
<li><p>2017 年 1 月 23 日，Linkerd 加入 CNCF。</p>
</li>
<li><p>2017 年 3 月 7 日，Linkerd 宣布完成千亿次产品请求</p>
</li>
<li><p>2017 年 4 月 25 日，Linkerd 1.0 版本发布</p>
</li>
<li><p>2017 年 7 月 11 日，Linkerd 发布版本 1.1.1，宣布和 Istio 项目集成</p>
</li>
<li><p>2017 年 9 月, nginx突然宣布要搞出一个Servicemesh来, Nginmesh: <a href="https://github.com/nginxinc/nginmesh" target="_blank" rel="noopener">https://github.com/nginxinc/nginmesh</a>, 可以作为istio的数据面, 不过这个项目目前处于不活跃开发(This project is no longer under active development)</p>
</li>
<li><p>2017 年 12 月 5 日，Conduit 0.1.0 版本发布</p>
</li>
</ul>
<p>Envoy 和 Linkerd 都是在数据面上的实现, 属于同一个层面的竞争, 前者是用 C++ 语言实现的，在性能和资源消耗上要比采用 Scala 语言实现的 Linkerd 小，这一点对于延迟敏感型和资源敏的服务尤为重要.</p>
<p>Envoy 对 作为 Istio 的标准数据面实现, 其最主要的贡献是提供了一套<a href="https://github.com/envoyproxy/data-plane-api/blob/master/API_OVERVIEW.md" target="_blank" rel="noopener">标准数据面API</a>, 将服务信息和流量规则下发到数据面的sidecar中, 另外Envoy还支持热重启. Istio早期采用了Envoy v1 API，目前的版本中则使用V2 API，V1已被废弃.</p>
<p>通过采用该标准API，Istio将控制面和数据面进行了解耦，为多种数据面sidecar实现提供了可能性。事实上基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现.</p>
<p>将控制面和数据面解耦是Istio后来居上，风头超过Service mesh鼻祖Linkerd的一招妙棋。Istio站在了控制面的高度上，而Linkerd则成为了可选的一种sidecar实现.</p>
<p>Conduit 的整体架构和 Istio 一致，借鉴了 Istio 数据平面 + 控制平面的设计，而且选择了 Rust 编程语言来实现数据平面，以达成 Conduit 宣称的更轻、更快和超低资源占用.</p>
<center>(参考: 敖小剑 <a href="https://skyao.io/publication/201801-service-mesh-2017-summary/" target="_blank" rel="noopener">Service Mesh年度总结：群雄逐鹿烽烟起</a>)</center>

<hr>
<h2 id="1-8-似曾相识的竞争格局"><a href="#1-8-似曾相识的竞争格局" class="headerlink" title="1.8 似曾相识的竞争格局"></a>1.8 似曾相识的竞争格局</h2><table>
<thead>
<tr>
<th></th>
<th>Kubernetes</th>
<th>Istio</th>
</tr>
</thead>
<tbody>
<tr>
<td>领域</td>
<td>容器编排</td>
<td>服务网格</td>
</tr>
<tr>
<td>主要竞品</td>
<td>Swarm, Mesos</td>
<td>Linkerd, Conduit</td>
</tr>
<tr>
<td>主要盟友</td>
<td>RedHat, CoreOS</td>
<td>IBM, Lyft</td>
</tr>
<tr>
<td>主要竞争对手</td>
<td>Docker 公司</td>
<td>Buoyant 公司</td>
</tr>
<tr>
<td>标准化</td>
<td>OCI: runtime spec, image spec</td>
<td>XDS</td>
</tr>
<tr>
<td>插件化</td>
<td>CNI, CRI</td>
<td>Istio CNI, Mixer Adapter</td>
</tr>
<tr>
<td>结果</td>
<td>Kubernetes 成为容器编排事实标准</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>google 主导的Kubernetes 在容器编排领域取得了完胜, 目前在服务网格领域的打法如出一辙, 社区对Istio前景也比较看好.</p>
<p>Istio CNI 计划在1.1 作为实验特性, 用户可以通过扩展方式定制sidecar的网络.</p>
<hr>
<h2 id="1-9-国内Service-Mesh-发展情况"><a href="#1-9-国内Service-Mesh-发展情况" class="headerlink" title="1.9 国内Service Mesh 发展情况"></a>1.9 国内Service Mesh 发展情况</h2><ul>
<li><p>蚂蚁金服开源SOFAMesh: <a href="https://github.com/alipay/sofa-mesh" target="_blank" rel="noopener">https://github.com/alipay/sofa-mesh</a></p>
<ul>
<li>从istio fork</li>
<li>使用Golang语言开发全新的Sidecar，替代Envoy</li>
<li>为了避免Mixer带来的性能瓶颈，合并Mixer部分功能进入Sidecar</li>
<li>Pilot和Citadel模块进行了大幅的扩展和增强</li>
<li>扩展RPC协议: SOFARPC/HSF/Dubbo</li>
</ul>
</li>
<li><p>华为:</p>
<ul>
<li>go-chassis: <a href="https://github.com/go-chassis/go-chassis" target="_blank" rel="noopener">https://github.com/go-chassis/go-chassis</a> golang 微服务框架, 支持istio平台</li>
<li>mesher: <a href="https://github.com/go-mesh/mesher" target="_blank" rel="noopener">https://github.com/go-mesh/mesher</a> mesh 数据面解决方案</li>
<li>国内首家提供Service Mesh公共服务的云厂商</li>
<li>目前(2019年1月)公有云Istio 产品线上已经支持申请公测, 产品形态比较完善</li>
</ul>
</li>
<li><p>腾讯云 TSF:</p>
<ul>
<li>基于 Istio、envoy 进行改造</li>
<li>支持 Kubernetes、虚拟机以及裸金属的服务</li>
<li>对 Istio 的能力进行了扩展和增强, 对 Consul 的完整适配</li>
<li>对于其他二进制协议进行扩展支持</li>
</ul>
</li>
<li><p>唯品会</p>
<ul>
<li>OSP (Open Service Platform)</li>
</ul>
</li>
<li><p>新浪:</p>
<ul>
<li>Motan: 是一套基于java开发的RPC框架,  Weibo Mesh 是基于Motan</li>
</ul>
</li>
</ul>
<hr>
<h1 id="2-Istio-第二代-Service-Mesh"><a href="#2-Istio-第二代-Service-Mesh" class="headerlink" title="2. Istio: 第二代 Service Mesh"></a>2. Istio: 第二代 Service Mesh</h1><p>Istio来自希腊语，英文意思是「sail」, 意为「启航」</p>
<ul>
<li>2.1 Istio 架构</li>
<li>2.2 核心功能</li>
<li>2.3 Istio 演示: BookInfo</li>
</ul>
<hr>
<h2 id="2-1-Istio-架构"><a href="#2-1-Istio-架构" class="headerlink" title="2.1 Istio 架构"></a>2.1 Istio 架构</h2><p><img width="90%" src="https://preliminary.istio.io/docs/concepts/what-is-istio/arch.svg"></p>
<center>Istio Architecture（图片来自<a href="https://istio.io/docs/concepts/what-is-istio/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<ul>
<li><p>数据面</p>
<ul>
<li>Sidecar</li>
</ul>
</li>
<li><p>控制面</p>
<ul>
<li>Pilot：服务发现、流量管理</li>
<li>Mixer：访问控制、遥测</li>
<li>Citadel：终端用户认证、流量加密</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-2-核心功能"><a href="#2-2-核心功能" class="headerlink" title="2.2 核心功能"></a>2.2 核心功能</h3><ul>
<li>流量管理</li>
<li>安全</li>
<li>可观察性</li>
<li>多平台支持</li>
<li>集成和定制</li>
</ul>
<p>下面是我对Istio架构总结的思维导图:</p>
<p><img src="https://zhongfox.github.io/assets/images/istio/naotu2.png"></p>
<hr>
<h2 id="2-3-Istio-演示-BookInfo"><a href="#2-3-Istio-演示-BookInfo" class="headerlink" title="2.3 Istio 演示: BookInfo"></a>2.3 Istio 演示: BookInfo</h2><p>以下是Istio官网经典的 BookInfo Demo, 这是一个多语言组成的异构微服务系统:</p>
<p><img width="80%" src="https://istio.io/docs/examples/bookinfo/withistio.svg"></p>
<center>Bookinfo Application（图片来自<a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<p>下面我将现场给大家进行演示, 从demo安装开始, 并体验一下istio的流控功能:</p>
<h4 id="使用helm管理istio"><a href="#使用helm管理istio" class="headerlink" title="使用helm管理istio"></a>使用helm管理istio</h4><p>下载istio release: <a href="https://istio.io/docs/setup/kubernetes/download-release/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/download-release/</a></p>
<h5 id="安装istio"><a href="#安装istio" class="headerlink" title="安装istio:"></a>安装istio:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml</span><br><span class="line">helm install install/kubernetes/helm/istio --name istio --namespace istio-system</span><br></pre></td></tr></table></figure>
<p>注意事项, 若要开启sidecar自动注入功能, 需要:</p>
<ul>
<li>确保 kube-apiserver 启动参数 开启了ValidatingAdmissionWebhook 和 MutatingAdmissionWebhook</li>
<li>给namespace 增加 label: <code>kubectl label namespace default istio-injection=enabled</code></li>
<li>同时还要保证 kube-apiserver 的 aggregator layer 开启: <code>--enable-aggregator-routing=true</code> 且证书和api server连通性正确设置.</li>
</ul>
<h5 id="如需卸载istio"><a href="#如需卸载istio" class="headerlink" title="如需卸载istio:"></a>如需卸载istio:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm delete --purge istio</span><br><span class="line">kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system</span><br></pre></td></tr></table></figure>
<p>更多安装选择请参考: <a href="https://istio.io/docs/setup/kubernetes/helm-install/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/helm-install/</a></p>
<h4 id="安装Bookinfo-Demo"><a href="#安装Bookinfo-Demo" class="headerlink" title="安装Bookinfo Demo:"></a>安装Bookinfo Demo:</h4><p>Bookinfo 是一个多语言异构的微服务demo, 其中 productpage 微服务会调用 details 和 reviews 两个微服务, reviews 会调用ratings 微服务, reviews 微服务有 3 个版本. 关于此项目更多细节请参考: <a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">https://istio.io/docs/examples/bookinfo/</a></p>
<h5 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用:"></a>部署应用:</h5><p><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</code></p>
<p>这将创建 productpage, details, ratings, reviews 对应的deployments 和 service, 其中reviews 有三个deployments, 代表三个不同的版本.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> % kubectl get pod</span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE</span><br><span class="line">details-v1-6865b9b99d-mnxbt    2/2       Running   0          1m</span><br><span class="line">productpage-v1-f8c8fb8-zjbhh   2/2       Running   0          59s</span><br><span class="line">ratings-v1-77f657f55d-95rcz    2/2       Running   0          1m</span><br><span class="line">reviews-v1-6b7f6db5c5-zqvkn    2/2       Running   0          59s</span><br><span class="line">reviews-v2-7ff5966b99-lw72l    2/2       Running   0          59s</span><br><span class="line">reviews-v3-5df889bcff-w9v7g    2/2       Running   0          59s</span><br><span class="line"></span><br><span class="line"> % kubectl get svc</span><br><span class="line">NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">details       ClusterIP   172.18.255.240   &lt;none&gt;        9080/TCP   1m</span><br><span class="line">productpage   ClusterIP   172.18.255.137   &lt;none&gt;        9080/TCP   1m</span><br><span class="line">ratings       ClusterIP   172.18.255.41    &lt;none&gt;        9080/TCP   1m</span><br><span class="line">reviews       ClusterIP   172.18.255.140   &lt;none&gt;        9080/TCP   1m</span><br></pre></td></tr></table></figure>
<p>对入口流量进行配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span><br></pre></td></tr></table></figure>
<p>该操作会创建bookinfo-gateway 的Gateway, 并将流量发送到productpage服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get gateway</span><br><span class="line">NAME               AGE</span><br><span class="line">bookinfo-gateway   1m</span><br></pre></td></tr></table></figure>
<p>此时通过bookinfo-gateway 对应的LB或者nodeport 访问/productpage 页面, 可以看到三个版本的reviews服务在随机切换</p>
<h4 id="基于权重的路由"><a href="#基于权重的路由" class="headerlink" title="基于权重的路由"></a>基于权重的路由</h4><p>通过CRD DestinationRule创建3 个reviews 子版本:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/destination-rule-reviews.yaml</span><br></pre></td></tr></table></figure>
<p>通过CRD VirtualService 调整个 reviews 服务子版本的流量比例, 设置 v1 和 v3 各占 50%</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml</span><br></pre></td></tr></table></figure>
<p>刷新页面, 可以看到无法再看到reviews v2的内容, 页面在v1和v3之间切换.</p>
<h4 id="基于内容路由"><a href="#基于内容路由" class="headerlink" title="基于内容路由"></a>基于内容路由</h4><p>修改reviews CRD, 将jason 登录的用户版本路由到v2, 其他用户路由到版本v3.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-jason-v2-v3.yaml</span><br></pre></td></tr></table></figure>
<p>刷新页面, 使用jason登录的用户, 将看到v2 黑色星星版本, 其他用户将看到v3 红色星星版本.</p>
<p>更多BookInfo 示例, 请参阅: <a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">https://istio.io/docs/examples/bookinfo/</a>, 若要删除应用: 执行脚本 <code>./samples/bookinfo/platform/kube/cleanup.sh</code></p>
<hr>
<h1 id="3-Istio-数据面"><a href="#3-Istio-数据面" class="headerlink" title="3. Istio 数据面"></a>3. Istio 数据面</h1><ul>
<li>3.1 数据面组件</li>
<li>3.2 sidecar 流量劫持原理</li>
<li>3.3 数据面标准API: xDS</li>
<li>3.4 分布式跟踪</li>
</ul>
<h2 id="3-1-数据面组件"><a href="#3-1-数据面组件" class="headerlink" title="3.1 数据面组件"></a>3.1 数据面组件</h2><p>Istio 注入sidecar实现:</p>
<ul>
<li>自动注入: 利用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">Kubernetes Dynamic Admission Webhooks</a> 对 新建的pod 进行注入: init container + sidecar</li>
<li>手动注入: 使用<code>istioctl kube-inject</code></li>
</ul>
<p>注入Pod内容:</p>
<ul>
<li>istio-init: 通过配置iptables来劫持Pod中的流量</li>
<li>istio-proxy: 两个进程pilot-agent和envoy, pilot-agent 进行初始化并启动envoy</li>
</ul>
<h4 id="Sidecar-自动注入实现"><a href="#Sidecar-自动注入实现" class="headerlink" title="Sidecar 自动注入实现"></a>Sidecar 自动注入实现</h4><p>Istio 利用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">Kubernetes Dynamic Admission Webhooks</a> 对pod 进行sidecar注入</p>
<p>查看istio 对这2个Webhooks 的配置 ValidatingWebhookConfiguration 和 MutatingWebhookConfiguration:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% kubectl get ValidatingWebhookConfiguration -oyaml</span><br><span class="line">% kubectl get mutatingWebhookConfiguration -oyaml</span><br></pre></td></tr></table></figure>
<p>可以看出:</p>
<ul>
<li>命名空间<code>istio-system</code> 中的服务 <code>istio-galley</code>, 通过路由<code>/admitpilot</code>, 处理config.istio.io部分, rbac.istio.io, authentication.istio.io, networking.istio.io等资源的Validating 工作</li>
<li>命名空间istio-system 中的服务 <code>istio-galley</code>, 通过路由<code>/admitmixer</code>, 处理其他config.istio.io资源的Validating 工作</li>
<li>命名空间istio-system 中的服务 <code>istio-sidecar-injector</code>, 通过路由<code>/inject</code>, 处理其他<code>v1/pods</code>的CREATE, 同时需要满足命名空间<code>istio-injection: enabled</code></li>
</ul>
<h4 id="istio-init"><a href="#istio-init" class="headerlink" title="istio-init"></a>istio-init</h4><p>数据面的每个Pod会被注入一个名为<code>istio-init</code> 的initContainer, initContrainer是K8S提供的机制，用于在Pod中执行一些初始化任务.在Initialcontainer执行完毕并退出后，才会启动Pod中的其它container.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">initContainers:</span><br><span class="line">- image: docker.io/istio/proxy_init:1.0.5</span><br><span class="line">  args:</span><br><span class="line">  - -p</span><br><span class="line">  - &quot;15001&quot;</span><br><span class="line">  - -u</span><br><span class="line">  - &quot;1337&quot;</span><br><span class="line">  - -m</span><br><span class="line">  - REDIRECT</span><br><span class="line">  - -i</span><br><span class="line">  - &apos;*&apos;</span><br><span class="line">  - -x</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  - -b</span><br><span class="line">  - &quot;9080&quot;</span><br><span class="line">  - -d</span><br><span class="line">  - &quot;&quot;</span><br></pre></td></tr></table></figure>
<p>istio-init ENTRYPOINT 和 args 组合的启动命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/bin/istio-iptables.sh -p 15001 -u 1337 -m REDIRECT -i &apos;*&apos; -x &quot;&quot; -b 9080 -d &quot;&quot;</span><br></pre></td></tr></table></figure>
<p>istio-iptables.sh 源码地址为 <a href="https://github.com/istio/istio/blob/master/tools/deb/istio-iptables.sh" target="_blank" rel="noopener">https://github.com/istio/istio/blob/master/tools/deb/istio-iptables.sh</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ istio-iptables.sh -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h]</span><br><span class="line">  -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001）</span><br><span class="line">  -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337）</span><br><span class="line">  -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值）</span><br><span class="line">  -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE)</span><br><span class="line">  -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS）</span><br><span class="line">  -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS）</span><br><span class="line">  -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR）</span><br><span class="line">  -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。</span><br><span class="line"></span><br><span class="line">环境变量位于 $ISTIO_SIDECAR_CONFIG（默认在：/var/lib/istio/envoy/sidecar.env）</span><br></pre></td></tr></table></figure>
<p>istio-init 通过配置iptable来劫持Pod中的流量:</p>
<ul>
<li>参数<code>-p 15001</code>: Pod中的数据流量被iptable拦截，并发向15001端口, 该端口将由 envoy 监听</li>
<li>参数<code>-u 1337</code>: 用于排除用户ID为1337，即Envoy自身的流量，以避免Iptable把Envoy发出的数据又重定向到Envoy, UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户, 见Sidecar <code>istio-proxy</code> 配置参数<code>securityContext.runAsUser</code></li>
<li>参数<code>-b 9080</code> <code>-d &quot;&quot;</code>: 入站端口控制, 将所有访问 9080 端口（即应用容器的端口）的流量重定向到 Envoy 代理</li>
<li>参数<code>-i &#39;*&#39;</code> <code>-x &quot;&quot;</code>: 出站IP控制, 将所有出站流量都重定向到 Envoy 代理</li>
</ul>
<p>Init 容器初始化完毕后就会自动终止，但是 Init 容器初始化结果(iptables)会保留到应用容器和 Sidecar 容器中.</p>
<h4 id="istio-proxy"><a href="#istio-proxy" class="headerlink" title="istio-proxy"></a>istio-proxy</h4><p>istio-proxy 以 sidecar 的形式注入到应用容器所在的pod中, 简化的注入yaml:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">- image: docker.io/istio/proxyv2:1.0.5</span><br><span class="line">  name: istio-proxy</span><br><span class="line">  args:</span><br><span class="line">  - proxy</span><br><span class="line">  - sidecar</span><br><span class="line">  - --configPath</span><br><span class="line">  - /etc/istio/proxy</span><br><span class="line">  - --binaryPath</span><br><span class="line">  - /usr/local/bin/envoy</span><br><span class="line">  - --serviceCluster</span><br><span class="line">  - ratings</span><br><span class="line">  - --drainDuration</span><br><span class="line">  - 45s</span><br><span class="line">  - --parentShutdownDuration</span><br><span class="line">  - 1m0s</span><br><span class="line">  - --discoveryAddress</span><br><span class="line">  - istio-pilot.istio-system:15007</span><br><span class="line">  - --discoveryRefreshDelay</span><br><span class="line">  - 1s</span><br><span class="line">  - --zipkinAddress</span><br><span class="line">  - zipkin.istio-system:9411</span><br><span class="line">  - --connectTimeout</span><br><span class="line">  - 10s</span><br><span class="line">  - --proxyAdminPort</span><br><span class="line">  - &quot;15000&quot;</span><br><span class="line">  - --controlPlaneAuthPolicy</span><br><span class="line">  - NONE</span><br><span class="line">  env:</span><br><span class="line">    ......</span><br><span class="line">  ports:</span><br><span class="line">  - containerPort: 15090</span><br><span class="line">    name: http-envoy-prom</span><br><span class="line">    protocol: TCP</span><br><span class="line">  securityContext:</span><br><span class="line">    runAsUser: 1337</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>istio-proxy容器中有两个进程pilot-agent和envoy:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~ % kubectl exec productpage-v1-f8c8fb8-wgmzk -c istio-proxy -- ps -ef</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">istio-p+     1     0  0 Jan03 ?        00:00:27 /usr/local/bin/pilot-agent proxy sidecar --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15007 --discoveryRefreshDelay 1s --zipkinAddress zipkin.istio-system:9411 --connectTimeout 10s --proxyAdminPort 15000 --controlPlaneAuthPolicy NONE</span><br><span class="line">istio-p+    21     1  0 Jan03 ?        01:26:24 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage --service-node sidecar~172.18.3.12~productpage-v1-f8c8fb8-wgmzk.default~default.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warn --v2-config-only</span><br></pre></td></tr></table></figure>
<p>可以看到:</p>
<ul>
<li><code>/usr/local/bin/pilot-agent</code> 是 <code>/usr/local/bin/envoy</code> 的父进程, Pilot-agent进程根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件(<code>/etc/istio/proxy/envoy-rev0.json</code>)，并负责启动Envoy进程</li>
<li>pilot-agent 的启动参数里包括: discoveryAddress(pilot服务地址), Envoy 二进制文件的位置, 服务集群名, 监控指标上报地址, Envoy 的管理端口, 热重启时间等</li>
</ul>
<p>Envoy配置初始化流程:</p>
<ol>
<li>Pilot-agent根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件envoy-rev0.json，该文件告诉Envoy从xDS server中获取动态配置信息，并配置了xDS server的地址信息，即控制面的Pilot</li>
<li>Pilot-agent使用envoy-rev0.json启动Envoy进程</li>
<li>Envoy根据初始配置获得Pilot地址，采用xDS接口从Pilot获取到Listener，Cluster，Route等d动态配置信息</li>
<li>Envoy根据获取到的动态配置启动Listener，并根据Listener的配置，结合Route和Cluster对拦截到的流量进行处理</li>
</ol>
<p>查看envoy 初始配置文件:</p>
<p><code>kubectl exec productpage-v1-f8c8fb8-wgmzk -c istio-proxy -- cat /etc/istio/proxy/envoy-rev0.json</code></p>
<hr>
<h2 id="3-2-sidecar-流量劫持原理"><a href="#3-2-sidecar-流量劫持原理" class="headerlink" title="3.2 sidecar 流量劫持原理"></a>3.2 sidecar 流量劫持原理</h2><p>sidecar 既要作为服务消费者端的正向代理，又要作为服务提供者端的反向代理, 具体拦截过程如下:</p>
<ul>
<li><p>Pod 所在的network namespace内, 除了envoy发出的流量外, iptables规则会对进入和发出的流量都进行拦截，通过nat redirect重定向到Envoy监听的15001端口.</p>
</li>
<li><p>envoy 会根据从Pilot拿到的 XDS 规则, 对流量进行转发.</p>
</li>
<li><p>envoy 的 listener 0.0.0.0:15001 接收进出 Pod 的所有流量，然后将请求移交给对应的virtual listener</p>
</li>
<li><p>对于本pod的服务, 有一个http listener <code>podIP+端口</code> 接受inbound 流量</p>
</li>
<li><p>每个service+非http端口, 监听器配对的 Outbound 非 HTTP 流量</p>
</li>
<li><p>每个service+http端口, 有一个http listener: <code>0.0.0.0+端口</code> 接受outbound流量</p>
</li>
</ul>
<p>整个拦截转发过程对业务容器是透明的, 业务容器仍然使用 Service 域名和端口进行通信, service 域名仍然会转换为service IP, 但service IP 在sidecar 中会被直接转换为 pod IP, 从容器中出去的流量已经使用了pod IP会直接转发到对应的Pod, 对比传统kubernetes 服务机制, service IP 转换为Pod IP 在node上进行, 由 kube-proxy维护的iptables实现.</p>
<hr>
<h2 id="3-3-数据面标准API-xDS"><a href="#3-3-数据面标准API-xDS" class="headerlink" title="3.3 数据面标准API: xDS"></a>3.3 数据面标准API: xDS</h2><p>xDS是一类发现服务的总称，包含LDS，RDS，CDS，EDS以及 SDS。Envoy通过xDS API可以动态获取Listener(监听器)， Route(路由)，Cluster(集群)，Endpoint(集群成员)以 及Secret(证书)配置</p>
<p>xDS API 涉及的概念:</p>
<ul>
<li>Host</li>
<li>Downstream</li>
<li>Upstream</li>
<li>Listener</li>
<li>Cluster</li>
</ul>
<p>Envoy 配置热更新: 配置的动态变更，而不需要重启 Envoy:</p>
<ol>
<li>新老进程采用基本的RPC协议使用Unix Domain Socket通讯.</li>
<li>新进程启动并完成所有初始化工作后，向老进程请求监听套接字的副本.</li>
<li>新进程接管套接字后，通知老进程关闭套接字.</li>
<li>通知老进程终止自己.</li>
</ol>
<h4 id="xDS-调试"><a href="#xDS-调试" class="headerlink" title="xDS 调试"></a>xDS 调试</h4><p>Pilot在9093端口提供了下述调试接口:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># What is sent to envoy</span><br><span class="line"># Listeners and routes</span><br><span class="line">curl $PILOT/debug/adsz</span><br><span class="line"></span><br><span class="line"># Endpoints</span><br><span class="line">curl $PILOT/debug/edsz</span><br><span class="line"></span><br><span class="line"># Clusters</span><br><span class="line">curl $PILOT/debug/cdsz</span><br></pre></td></tr></table></figure>
<p>Sidecar Envoy 也提供了管理接口，缺省为localhost的15000端口，可以获取listener，cluster以及完整的配置数据</p>
<p>可以通过以下命令查看支持的调试接口:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec productpage-v1-f8c8fb8-zjbhh -c istio-proxy curl http://127.0.0.1:15000/help</span><br></pre></td></tr></table></figure>
<p>或者forward到本地就行调试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl port-forward productpage-v1-f8c8fb8-zjbhh 15000</span><br></pre></td></tr></table></figure>
<p>相关的调试接口:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">http://127.0.0.1:15000</span><br><span class="line">http://127.0.0.1:15000/help</span><br><span class="line">http://127.0.0.1:15000/config_dump</span><br><span class="line">http://127.0.0.1:15000/listeners</span><br><span class="line">http://127.0.0.1:15000/clusters</span><br></pre></td></tr></table></figure>
<p>使用istioctl 查看代理配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">istioctl pc &#123;xDS类型&#125;  &#123;POD_NAME&#125; &#123;过滤条件&#125; &#123;-o json/yaml&#125;</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">istioctl pc routes productpage-v1-f8c8fb8-zjbhh --name 9080 -o json</span><br></pre></td></tr></table></figure>
<p>xDS 类型包括: listener, route, cluster, endpoint</p>
<h4 id="对xDS-进行分析-productpage-访问-reviews-服务"><a href="#对xDS-进行分析-productpage-访问-reviews-服务" class="headerlink" title="对xDS 进行分析: productpage 访问 reviews 服务"></a>对xDS 进行分析: productpage 访问 reviews 服务</h4><p>查看 product 的所有listener:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc listener  productpage-v1-f8c8fb8-zjbhh</span><br><span class="line">ADDRESS            PORT      TYPE</span><br><span class="line">172.18.255.178     15011     TCP</span><br><span class="line">172.18.255.194     44134     TCP</span><br><span class="line">172.18.255.110     443       TCP</span><br><span class="line">172.18.255.190     50000     TCP</span><br><span class="line">172.18.255.203     853       TCP</span><br><span class="line">172.18.255.2       443       TCP</span><br><span class="line">172.18.255.239     16686     TCP</span><br><span class="line">0.0.0.0            80        TCP</span><br><span class="line">172.18.255.215     3306      TCP</span><br><span class="line">172.18.255.203     31400     TCP</span><br><span class="line">172.18.255.111     443       TCP</span><br><span class="line">172.18.255.203     8060      TCP</span><br><span class="line">172.18.255.203     443       TCP</span><br><span class="line">172.18.255.40      443       TCP</span><br><span class="line">172.18.255.1       443       TCP</span><br><span class="line">172.18.255.53      53        TCP</span><br><span class="line">172.18.255.203     15011     TCP</span><br><span class="line">172.18.255.105     14268     TCP</span><br><span class="line">172.18.255.125     42422     TCP</span><br><span class="line">172.18.255.105     14267     TCP</span><br><span class="line">172.18.255.52      80        TCP</span><br><span class="line">0.0.0.0            15010     HTTP</span><br><span class="line">0.0.0.0            9411      HTTP</span><br><span class="line">0.0.0.0            8060      HTTP</span><br><span class="line">0.0.0.0            9080      HTTP</span><br><span class="line">0.0.0.0            15004     HTTP</span><br><span class="line">0.0.0.0            20001     HTTP</span><br><span class="line">0.0.0.0            9093      HTTP</span><br><span class="line">0.0.0.0            8080      HTTP</span><br><span class="line">0.0.0.0            15030     HTTP</span><br><span class="line">0.0.0.0            9091      HTTP</span><br><span class="line">0.0.0.0            9090      HTTP</span><br><span class="line">0.0.0.0            15031     HTTP</span><br><span class="line">0.0.0.0            3000      HTTP</span><br><span class="line">0.0.0.0            15001     TCP</span><br><span class="line">172.18.3.50        9080      HTTP 这是当前pod ip 暴露的服务地址, 会路由到回环地址, 各个pod 会不一样</span><br></pre></td></tr></table></figure>
<p>envoy 流量入口的listener:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc listener  productpage-v1-f8c8fb8-zjbhh --address 0.0.0.0 --port 15001 -o json</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;virtual&quot;,</span><br><span class="line">        &quot;address&quot;: &#123;</span><br><span class="line">            &quot;socketAddress&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;0.0.0.0&quot;,</span><br><span class="line">                &quot;portValue&quot;: 15001</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;filterChains&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;filters&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;name&quot;: &quot;envoy.tcp_proxy&quot;,</span><br><span class="line">                        &quot;config&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;BlackHoleCluster&quot;,</span><br><span class="line">                            &quot;stat_prefix&quot;: &quot;BlackHoleCluster&quot;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;useOriginalDst&quot;: true # 这意味着它将请求交给最符合请求原始目标的监听器。如果找不到任何匹配的虚拟监听器，它会将请求发送给返回 404 的 BlackHoleCluster</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>以下是reviews的所有pod IP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> % kubectl get ep reviews</span><br><span class="line">NAME      ENDPOINTS                                            AGE</span><br><span class="line">reviews   172.18.2.35:9080,172.18.3.48:9080,172.18.3.49:9080   1d</span><br></pre></td></tr></table></figure>
<p>对于目的地址是以上ip的http访问, 这些 ip 并没有对应的listener, 因此会通过端口9080 匹配到listener <code>0.0.0.0 9080</code></p>
<p>查看listener <code>0.0.0.0 9080</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc listener  productpage-v1-f8c8fb8-zjbhh --address 0.0.0.0 --port 9080 -ojson</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;0.0.0.0_9080&quot;,</span><br><span class="line">        &quot;address&quot;: &#123;</span><br><span class="line">            &quot;socketAddress&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;0.0.0.0&quot;,</span><br><span class="line">                &quot;portValue&quot;: 9080</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">                            &quot;rds&quot;: &#123;</span><br><span class="line">                                &quot;config_source&quot;: &#123;</span><br><span class="line">                                    &quot;ads&quot;: &#123;&#125;</span><br><span class="line">                                &#125;,</span><br><span class="line">                                &quot;route_config_name&quot;: &quot;9080&quot;</span><br><span class="line">                            &#125;,</span><br><span class="line">                            ......</span><br></pre></td></tr></table></figure>
<p>查看名为<code>9080</code> 的 route:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc routes  productpage-v1-f8c8fb8-zjbhh --name 9080 -o json</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;9080&quot;,</span><br><span class="line">        &quot;virtualHosts&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;details.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;details.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;details.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;details&quot;,</span><br><span class="line">                    &quot;details:9080&quot;,</span><br><span class="line">                    &quot;details.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;details.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;details.default.svc&quot;,</span><br><span class="line">                    &quot;details.default.svc:9080&quot;,</span><br><span class="line">                    &quot;details.default&quot;,</span><br><span class="line">                    &quot;details.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.240&quot;,</span><br><span class="line">                    &quot;172.18.255.240:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;match&quot;: &#123;</span><br><span class="line">                            &quot;prefix&quot;: &quot;/&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;route&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;outbound|9080||details.default.svc.cluster.local&quot;,</span><br><span class="line">                            &quot;timeout&quot;: &quot;0.000s&quot;,</span><br><span class="line">                            &quot;maxGrpcTimeout&quot;: &quot;0.000s&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        ......</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;productpage.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;productpage.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;productpage.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;productpage&quot;,</span><br><span class="line">                    &quot;productpage:9080&quot;,</span><br><span class="line">                    &quot;productpage.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;productpage.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;productpage.default.svc&quot;,</span><br><span class="line">                    &quot;productpage.default.svc:9080&quot;,</span><br><span class="line">                    &quot;productpage.default&quot;,</span><br><span class="line">                    &quot;productpage.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.137&quot;,</span><br><span class="line">                    &quot;172.18.255.137:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [ ...... ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;ratings.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;ratings.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;ratings.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;ratings&quot;,</span><br><span class="line">                    &quot;ratings:9080&quot;,</span><br><span class="line">                    &quot;ratings.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;ratings.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;ratings.default.svc&quot;,</span><br><span class="line">                    &quot;ratings.default.svc:9080&quot;,</span><br><span class="line">                    &quot;ratings.default&quot;,</span><br><span class="line">                    &quot;ratings.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.41&quot;,</span><br><span class="line">                    &quot;172.18.255.41:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [ ...... ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;reviews.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;reviews.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;reviews.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;reviews&quot;,</span><br><span class="line">                    &quot;reviews:9080&quot;,</span><br><span class="line">                    &quot;reviews.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;reviews.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;reviews.default.svc&quot;,</span><br><span class="line">                    &quot;reviews.default.svc:9080&quot;,</span><br><span class="line">                    &quot;reviews.default&quot;,</span><br><span class="line">                    &quot;reviews.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.140&quot;,</span><br><span class="line">                    &quot;172.18.255.140:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;match&quot;: &#123;</span><br><span class="line">                            &quot;prefix&quot;: &quot;/&quot;,</span><br><span class="line">                            &quot;headers&quot;: [</span><br><span class="line">                                &#123;</span><br><span class="line">                                    &quot;name&quot;: &quot;end-user&quot;,</span><br><span class="line">                                    &quot;exactMatch&quot;: &quot;jason&quot;</span><br><span class="line">                                &#125;</span><br><span class="line">                            ]</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;route&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;outbound|9080|v2|reviews.default.svc.cluster.local&quot;,</span><br><span class="line">                            &quot;timeout&quot;: &quot;0.000s&quot;,</span><br><span class="line">                            &quot;maxGrpcTimeout&quot;: &quot;0.000s&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        ......</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;match&quot;: &#123;</span><br><span class="line">                            &quot;prefix&quot;: &quot;/&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;route&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;outbound|9080|v3|reviews.default.svc.cluster.local&quot;,</span><br><span class="line">                            &quot;timeout&quot;: &quot;0.000s&quot;,</span><br><span class="line">                            &quot;maxGrpcTimeout&quot;: &quot;0.000s&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        .......</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;validateClusters&quot;: false</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>可以看到, 在9080 这个route 中, 包含所有这个端口的http 路由信息, 通过virtualHosts列表进行服务域名分发到各个cluster.</p>
<p>查看virtualHosts <code>reviews.default.svc.cluster.local:9080</code> 中的routes信息, 可以看到jason 路由到了cluster <code>outbound|9080|v2|reviews.default.svc.cluster.local</code></p>
<p>查看该cluster:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc cluster productpage-v1-f8c8fb8-zjbhh --fqdn reviews.default.svc.cluster.local --subset v2 -o json</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;outbound|9080|v2|reviews.default.svc.cluster.local&quot;,</span><br><span class="line">        &quot;type&quot;: &quot;EDS&quot;,</span><br><span class="line">        &quot;edsClusterConfig&quot;: &#123;</span><br><span class="line">            &quot;edsConfig&quot;: &#123;</span><br><span class="line">                &quot;ads&quot;: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;serviceName&quot;: &quot;outbound|9080|v2|reviews.default.svc.cluster.local&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;connectTimeout&quot;: &quot;1.000s&quot;,</span><br><span class="line">        &quot;lbPolicy&quot;: &quot;RANDOM&quot;,</span><br><span class="line">        &quot;circuitBreakers&quot;: &#123;</span><br><span class="line">            &quot;thresholds&quot;: [</span><br><span class="line">                &#123;&#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>查看其对应的endpoint:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> % istioctl pc endpoint productpage-v1-f8c8fb8-zjbhh --cluster &apos;outbound|9080|v2|reviews.default.svc.cluster.local&apos;</span><br><span class="line">ENDPOINT             STATUS      CLUSTER</span><br><span class="line">172.18.2.35:9080     HEALTHY     outbound|9080|v2|reviews.default.svc.cluster.local</span><br></pre></td></tr></table></figure>
<p>该endpoint 即为 reviews 服务 V2 对应的 pod IP</p>
<h4 id="XDS服务接口的最终一致性考虑"><a href="#XDS服务接口的最终一致性考虑" class="headerlink" title="XDS服务接口的最终一致性考虑"></a>XDS服务接口的最终一致性考虑</h4><p>遵循 make before break 模型</p>
<hr>
<h2 id="3-4-分布式跟踪"><a href="#3-4-分布式跟踪" class="headerlink" title="3.4 分布式跟踪"></a>3.4 分布式跟踪</h2><p>以下是分布式全链路跟踪示意图:</p>
<p><img width="40%" style="float: left" src="https://zhongfox.github.io/assets/images/hunter/opentracing_1.png"><br><img width="60%" style="float: left;margin-top:2cm;" src="https://zhongfox.github.io/assets/images/hunter/opentracing_2.png"></p>
<center>一个典型的Trace案例（图片来自<a href="https://wu-sheng.gitbooks.io/opentracing-io/content/" target="_blank" rel="noopener">opentracing文档中文版</a>)</center>

<hr>
<p>Jaeger 是Uber 开源的全链路跟踪系统, 符合OpenTracing协议, OpenTracing 和 Jaeger 均是CNCF 成员项目, 以下是Jaeger 架构的示意图:</p>
<p><img src="https://www.jaegertracing.io/img/architecture.png"></p>
<center>Jaeger 架构示意图（图片来自<a href="https://www.jaegertracing.io/docs/1.6/architecture/" target="_blank" rel="noopener">Jaeger官方文档</a>)</center>

<p>分布式跟踪系统让开发者能够得到可视化的调用流程展示。这对复杂的微服务系统进行问题排查和性能优化时至关重要.</p>
<p>Envoy 原生支持http 链路跟踪:</p>
<ul>
<li>生成 Request ID：Envoy 会在需要的时候生成 UUID，并操作名为 [x-request-id] 的 HTTP Header。应用可以转发这个 Header 用于统一的记录和跟踪.</li>
<li>支持集成外部跟踪服务：Envoy 支持可插接的外部跟踪可视化服务。目前支持有:<ul>
<li>LightStep</li>
<li>Zipkin 或者 Zipkin 兼容的后端（比如说 Jaeger）</li>
<li>Datadog</li>
</ul>
</li>
<li>客户端跟踪 ID 连接：x-client-trace-id Header 可以用来把不信任的请求 ID 连接到受信的 x-request-id Header 上</li>
</ul>
<h4 id="跟踪上下文信息的传播"><a href="#跟踪上下文信息的传播" class="headerlink" title="跟踪上下文信息的传播"></a>跟踪上下文信息的传播</h4><ul>
<li>不管使用的是哪个跟踪服务，都应该传播 x-request-id，这样在被调用服务中启动相关性的记录</li>
<li>如果使用的是 Zipkin，Envoy 要传播的是 <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#config-http-conn-man-headers-b3" target="_blank" rel="noopener">B3 Header</a>。（x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, 以及 x-b3-flags. x-b3-sampled）</li>
<li>上下文跟踪并非零修改, 在调用下游服务时, 上游应用应该自行传播跟踪相关的 HTTP Header</li>
</ul>
<hr>
<h1 id="4-Istio-控制面"><a href="#4-Istio-控制面" class="headerlink" title="4. Istio 控制面"></a>4. Istio 控制面</h1><ul>
<li>4.1 Pilot 架构</li>
<li>4.2 流量管理模型</li>
<li>4.3 故障处理</li>
<li>4.4 Mixer 架构</li>
<li>4.5 Mixer适配器模型</li>
<li>4.6 Mixer 缓存机制</li>
</ul>
<hr>
<h2 id="4-1-Pilot-架构"><a href="#4-1-Pilot-架构" class="headerlink" title="4.1 Pilot 架构"></a>4.1 Pilot 架构</h2><p><img src="https://preliminary.istio.io/docs/concepts/traffic-management/PilotAdapters.svg"></p>
<center>Pilot Architecture（图片来自<a href="https://istio.io/docs/concepts/traffic-management/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<ul>
<li>Rules API: 对外封装统一的 API，供服务的开发者或者运维人员调用，可以用于流量控制。</li>
<li>Envoy API: 对内封装统一的 API，供 Envoy 调用以获取注册信息、流量控制信息等。</li>
<li>抽象模型层: 对服务的注册信息、流量控制规则等进行抽象，使其描述与平台无关。</li>
<li>平台适配层: 用于适配各个平台如 Kubernetes、Mesos、Cloud Foundry 等，把平台特定的注册信息、资源信息等转换成抽象模型层定义的平台无关的描述。例如，Pilot 中的 Kubernetes 适配器实现必要的控制器来 watch Kubernetes API server 中 pod 注册信息、ingress 资源以及用于存储流量管理规则的第三方资源的更改</li>
</ul>
<hr>
<h2 id="4-2-流量管理模型"><a href="#4-2-流量管理模型" class="headerlink" title="4.2 流量管理模型"></a>4.2 流量管理模型</h2><ul>
<li>VirtualService</li>
<li>DestinationRule</li>
<li>ServiceEntry</li>
<li>Gateway</li>
</ul>
<h4 id="VirtualService"><a href="#VirtualService" class="headerlink" title="VirtualService"></a>VirtualService</h4><p>VirtualService 中定义了一系列针对指定服务的流量路由规则。每个路由规则都是针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务, 或者目标服务的子集或版本, 匹配规则中还包含了对流量发起方的定义，这样一来，规则还可以针对特定客户上下文进行定制.</p>
<h4 id="Gateway"><a href="#Gateway" class="headerlink" title="Gateway"></a>Gateway</h4><p>Gateway 描述了一个负载均衡器，用于承载网格边缘的进入和发出连接。这一规范中描述了一系列开放端口，以及这些端口所使用的协议、负载均衡的 SNI 配置等内容</p>
<h4 id="ServiceEntry"><a href="#ServiceEntry" class="headerlink" title="ServiceEntry"></a>ServiceEntry</h4><p>Istio 服务网格内部会维护一个与平台无关的使用通用模型表示的服务注册表，当你的服务网格需要访问外部服务的时候，就需要使用 ServiceEntry 来添加服务注册, 这类服务可能是网格外的 API，或者是处于网格内部但却不存在于平台的服务注册表中的条目（例如需要和 Kubernetes 服务沟通的一组虚拟机服务）.</p>
<h4 id="EnvoyFilter"><a href="#EnvoyFilter" class="headerlink" title="EnvoyFilter"></a>EnvoyFilter</h4><p>EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置.</p>
<h4 id="Kubernetes-Ingress-vs-Istio-Gateway"><a href="#Kubernetes-Ingress-vs-Istio-Gateway" class="headerlink" title="Kubernetes Ingress vs Istio Gateway"></a>Kubernetes Ingress vs Istio Gateway</h4><p><img src="https://zhongfox.github.io/assets/images/istio/gateway.png"></p>
<ul>
<li>合并了L4-6和L7的规范, 对传统技术栈用户的应用迁入不方便</li>
<li>表现力不足:<ul>
<li>只能对 service、port、HTTP 路径等有限字段匹配来路由流量</li>
<li>端口只支持默认80/443</li>
</ul>
</li>
</ul>
<p>Istio Gateway:·</p>
<ul>
<li>定义了四层到六层的负载均衡属性 (通常是SecOps或NetOps关注的内容)<ul>
<li>端口</li>
<li>端口所使用的协议(HTTP, HTTPS, GRPC, HTTP2, MONGO, TCP, TLS)</li>
<li>Hosts</li>
<li>TLS SNI header 路由支持</li>
<li>TLS 配置支持(http 自动301, 证书等)</li>
<li>ip / unix domain socket</li>
</ul>
</li>
</ul>
<h4 id="Kubernetes-Istio-Envoy-xDS-模型对比"><a href="#Kubernetes-Istio-Envoy-xDS-模型对比" class="headerlink" title="Kubernetes, Istio, Envoy xDS 模型对比"></a>Kubernetes, Istio, Envoy xDS 模型对比</h4><p>以下是对Kubernetes, Istio, Envoy xDS 模型的不严格对比</p>
<table>
<thead>
<tr>
<th></th>
<th>Kubernetes</th>
<th>Istio</th>
<th>Envoy xDS</th>
</tr>
</thead>
<tbody>
<tr>
<td>入口流量</td>
<td>Ingress</td>
<td>GateWay</td>
<td>Listener</td>
</tr>
<tr>
<td>服务定义</td>
<td>Service</td>
<td>-</td>
<td>Cluster+Listener</td>
</tr>
<tr>
<td>外部服务定义</td>
<td>-</td>
<td>ServiceEntry</td>
<td>Cluster+Listener</td>
</tr>
<tr>
<td>版本定义</td>
<td>-</td>
<td>DestinationRule</td>
<td>Cluster+Listener</td>
</tr>
<tr>
<td>版本路由</td>
<td>-</td>
<td>VirtualService</td>
<td>Route</td>
</tr>
<tr>
<td>实例</td>
<td>Endpoint</td>
<td>-</td>
<td>Endpoint</td>
</tr>
</tbody>
</table>
<h4 id="Kubernetes-和-Istio-服务寻址的区别"><a href="#Kubernetes-和-Istio-服务寻址的区别" class="headerlink" title="Kubernetes 和 Istio 服务寻址的区别:"></a>Kubernetes 和 Istio 服务寻址的区别:</h4><p><strong>Kubernetes</strong>:</p>
<ol>
<li>kube-dns: service domain -&gt; service ip</li>
<li>kube-proxy(node iptables): service ip -&gt; pod ip</li>
</ol>
<p><strong>Istio</strong>:</p>
<ol>
<li>kube-dns: service domain -&gt; service ip</li>
<li>sidecar envoy: service ip -&gt; pod ip</li>
</ol>
<hr>
<h2 id="4-3-故障处理"><a href="#4-3-故障处理" class="headerlink" title="4.3 故障处理"></a>4.3 故障处理</h2><p>随着微服务的拆分粒度增强, 服务调用会增多, 更复杂, 扇入 扇出,  调用失败的风险增加, 以下是常见的服务容错处理方式:</p>
<table>
<thead>
<tr>
<th></th>
<th>控制端</th>
<th>目的</th>
<th>实现</th>
<th>Istio</th>
</tr>
</thead>
<tbody>
<tr>
<td>超时</td>
<td>client</td>
<td>保护client</td>
<td>请求等待超时/请求运行超时</td>
<td>timeout</td>
</tr>
<tr>
<td>重试</td>
<td>client</td>
<td>容忍server临时错误, 保证业务整体可用性</td>
<td>重试次数/重试的超时时间</td>
<td>retries.attempts, retries.perTryTimeout</td>
</tr>
<tr>
<td>熔断</td>
<td>client</td>
<td>降低性能差的服务或实例的影响</td>
<td>通常会结合超时+重试, 动态进行服务状态决策(Open/Closed/Half-Open)</td>
<td>trafficPolicy.outlierDetection</td>
</tr>
<tr>
<td>降级</td>
<td>client</td>
<td>保证业务主要功能可用</td>
<td>主逻辑失败采用备用逻辑的过程(镜像服务分级, 调用备用服务, 或者返回mock数据)</td>
<td>暂不支持, 需要业务代码按需实现</td>
</tr>
<tr>
<td>隔离</td>
<td>client</td>
<td>防止异常server占用过多client资源</td>
<td>隔离对不同服务调用的资源依赖: 线程池隔离/信号量隔离</td>
<td>暂不支持</td>
</tr>
<tr>
<td>幂等</td>
<td>server</td>
<td>容忍client重试, 保证数据一致性</td>
<td>唯一ID/加锁/事务等手段</td>
<td>暂不支持, 需要业务代码按需实现</td>
</tr>
<tr>
<td>限流</td>
<td>server</td>
<td>保护server</td>
<td>常用算法: 计数器, 漏桶, 令牌桶</td>
<td>trafficPolicy.connectionPool</td>
</tr>
</tbody>
</table>
<p>Istio 没有无降级处理支持: Istio可以提高网格中服务的可靠性和可用性。但是，应用程序仍然需要处理故障（错误）并采取适当的回退操作。例如，当负载均衡池中的所有实例都失败时，Envoy 将返回 HTTP 503。应用程序有责任实现必要的逻辑，对这种来自上游服务的 HTTP 503 错误做出合适的响应。</p>
<hr>
<h2 id="4-4-Mixer-架构"><a href="#4-4-Mixer-架构" class="headerlink" title="4.4 Mixer 架构"></a>4.4 Mixer 架构</h2><p><img src="https://istio.io/docs/concepts/policies-and-telemetry/topology-without-cache.svg"></p>
<center>Mixer Topology（图片来自<a href="https://istio.io/docs/concepts/policies-and-telemetry/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<p>Istio 的四大功能点连接, 安全, 控制, 观察, 其中「控制」和「观察」的功能主要都是由Mixer组件来提供, Mixer 在Istio中角色:</p>
<ul>
<li>功能上: 负责策略控制和遥测收集</li>
<li>架构上:提供插件模型，可以扩展和定制</li>
</ul>
<hr>
<h2 id="4-5-Mixer-Adapter-模型"><a href="#4-5-Mixer-Adapter-模型" class="headerlink" title="4.5 Mixer Adapter 模型"></a>4.5 Mixer Adapter 模型</h2><ul>
<li>Attribute</li>
<li>Template</li>
<li>Adapter</li>
<li>Instance</li>
<li>Handler</li>
<li>Rule</li>
</ul>
<h4 id="Attribute"><a href="#Attribute" class="headerlink" title="Attribute"></a>Attribute</h4><p>Attribute 是策略和遥测功能中有关请求和环境的基本数据, 是用于描述特定服务请求或请求环境的属性的一小段数据。例如，属性可以指定特定请求的大小、操作的响应代码、请求来自的 IP 地址等.</p>
<ul>
<li>Istio 中的主要属性生产者是 Envoy，但专用的 Mixer 适配器也可以生成属性</li>
<li>属性词汇表见: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/attribute-vocabulary/" target="_blank" rel="noopener">Attribute Vocabulary</a></li>
<li>数据流向: envoy -&gt; mixer</li>
</ul>
<h4 id="Template"><a href="#Template" class="headerlink" title="Template"></a>Template</h4><p>Template 是对 adapter 的数据格式和处理接口的抽象, Template定义了:</p>
<ul>
<li>当处理请求时发送给adapter 的数据格式</li>
<li>adapter 必须实现的gRPC service 接口</li>
</ul>
<p>每个Template 通过 <code>template.proto</code> 进行定义:</p>
<ul>
<li>名为<code>Template</code> 的一个message</li>
<li>Name: 通过template所在的package name自动生成</li>
<li>template_variety: 可选Check, Report, Quota or AttributeGenerator, 决定了adapter必须实现的方法. 同时决定了在mixer的什么阶段要生成template对应的instance:<ul>
<li>Check: 在Mixer’s Check API call时创建并发送instance</li>
<li>Report: 在Mixer’s Report API call时创建并发送instance</li>
<li>Quota: 在Mixer’s Check  API call时创建并发送instance(查询配额时)</li>
<li>AttributeGenerator: for both Check, Report Mixer API calls</li>
</ul>
</li>
</ul>
<p>Istio 内置的Templates: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/templates/" target="_blank" rel="noopener">https://istio.io/docs/reference/config/policy-and-telemetry/templates/</a></p>
<h4 id="Adapter"><a href="#Adapter" class="headerlink" title="Adapter"></a>Adapter</h4><p>封装了 Mixer 和特定外部基础设施后端进行交互的必要接口，例如 Prometheus 或者 Stackdriver</p>
<ul>
<li>定义了需要处理的模板(在yaml中配置template)</li>
<li>定义了处理某个Template数据格式的GRPC接口</li>
<li>定义 Adapter需要的配置格式(Params)</li>
<li>可以同时处理多个数据(instance)</li>
</ul>
<p>Istio 内置的Adapter: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/adapters/" target="_blank" rel="noopener">https://istio.io/docs/reference/config/policy-and-telemetry/adapters/</a></p>
<h4 id="Instance"><a href="#Instance" class="headerlink" title="Instance"></a>Instance</h4><p>代表符合某个Template定义的数据格式的具体实现, 该具体实现由用户配置的 CRD,  CRD 定义了将Attributes 转换为具体instance 的规则, 支持属性表达式</p>
<ul>
<li>Instance CRD 是Template 中定义的数据格式 + 属性转换器</li>
<li>内置的Instance 类型(其实就是内置 Template): <a href="https://istio.io/docs/reference/config/policy-and-telemetry/templates/" target="_blank" rel="noopener">Templates</a></li>
<li>属性表达式见: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/expression-language/" target="_blank" rel="noopener">Expression Language</a></li>
<li>数据流向: mixer -&gt; adapter 实例</li>
</ul>
<h4 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h4><p>用户配置的 CRD, 为具体Adapter提供一个具体配置, 对应Adapter的可运行实例</p>
<h4 id="Rule"><a href="#Rule" class="headerlink" title="Rule"></a>Rule</h4><p>用户配置的 CRD, 配置一组规则，这些规则描述了何时调用特定(通过Handler对应的)适配器及哪些Instance</p>
<hr>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><blockquote>
<p>计算机科学中的所有问题，都可以用另一个层来解决，除了层数太多的问题</p>
</blockquote>
<p>Kubernetes 本身已经很复杂, Istio 为了更高层控制的抽象, 又增加了很多概念. 复杂度堪比kubernetes.</p>
<p>可以看出istio 设计精良, 在处理微服务的复杂场景有很多优秀之处, 不过目前istio目前的短板还是很明显, 高度的抽象带来了很多性能的损耗, 社区现在也有很多优化的方向, 像蚂蚁金服开源的SofaMesh 主要是去精简层, 试图在sidecar里去做很多mixer 的事情, 减少sidecar和mixer的同步请求依赖,  而一些其他的sidecar 网络方案, 更多的是考虑去优化层, 优化sidecar 这一层的性能开销.</p>
<p>在Istio 1.0 之前, 主要还是以功能的实现为主, 不过后面随着社区的积极投入, 相信Istio的性能会有长足的提升.</p>
<p>笔者之前从事过多年的服务治理相关的工作, 过程中切身体会到微服务治理的痛点, 所以也比较关注 service mesh的发展, 个人对istio也非常看好, 刚好今年我们中心容器产品今年也有这方面的计划, 期待我们能在这个方向进行一些产品和技术的深耕.</p>
<hr>
<p><img src="https://zhongfox.github.io/assets/images/istio/last.png"></p>
<hr>
<p>参考资料:</p>
<ul>
<li><a href="http://www.servicemesher.com/" target="_blank" rel="noopener">servicemesher 中文社区</a></li>
<li><a href="https://thenewstack.io/why-you-should-care-about-istio-gateways/" target="_blank" rel="noopener">Why You Should Care About Istio Gateways</a></li>
<li><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank" rel="noopener">Pattern: Service Mesh</a></li>
<li><a href="https://github.com/istio/istio/wiki/Mixer-Out-Of-Process-Adapter-Dev-Guide" target="_blank" rel="noopener">Mixer Out Of Process Adapter Dev Guide</a></li>
<li><a href="https://github.com/istio/istio/wiki/Mixer-Out-Of-Process-Adapter-Walkthrough" target="_blank" rel="noopener">Mixer Out of Process Adapter Walkthrough</a></li>
<li><a href="http://www.servicemesher.com/blog/envoy-xds-protocol" target="_blank" rel="noopener">Envoy 中的 xDS REST 和 gRPC 协议详解</a></li>
<li><a href="https://preliminary.istio.io/blog/2018/delayering-istio/delayering-istio/" target="_blank" rel="noopener">Delayering Istio with AppSwitch</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/01/31/servicemesh-istio/" data-id="ckac6j633000qa6u6jm7mx4km" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-k8s-traffic-copy" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/10/k8s-traffic-copy/" class="article-date">
  <time datetime="2019-01-10T02:17:37.000Z" itemprop="datePublished">2019-01-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/10/k8s-traffic-copy/">Kubernetes 流量复制方案</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者：田小康</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>测试环境没有真实的数据, 会导致很多测试工作难以展开, 尤其是一些测试任务需要使用生产环境来做时, 会极大影响现网的稳定性。</p>
<p>我们需要一个流量复制方案, 将现网流量复制到预发布/测试环境</p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/traffic-copy-diagram.png" alt="流量复制示意"></p>
<h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><ul>
<li>将线上请求拷贝一份到预发布/测试环境</li>
<li>不影响现网请求</li>
<li>可配置流量复制比例, 毕竟测试环境资源有限</li>
<li>零代码改动</li>
</ul>
<h1 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h1><p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/k8s-traffic-copy-diagram.png" alt="Kubernetes 流量复制方案"></p>
<ul>
<li>承载入口流量的 Pod 新增一个 <code>Nginx 容器</code> 接管流量</li>
<li><a href="http://nginx.org/en/docs/http/ngx_http_mirror_module.html" target="_blank" rel="noopener">Nginx Mirror</a> 模块会将流量复制一份并 proxy 到指定 URL (测试环境)</li>
<li><code>Nginx mirror</code> 复制流量不会影响正常请求处理流程, 镜像请求的 Resp 会被 Nginx 丢弃</li>
<li><code>K8s Service</code> 按照 <code>Label Selector</code> 去选择请求分发的 Pod, 意味着不同Pod, 只要有相同 <code>Label</code>, 就可以协同处理请求</li>
<li>通过控制有 <code>Mirror 功能的 Pod</code> 和 <code>正常的 Pod</code> 的比例, 便可以配置流量复制的比例</li>
</ul>
<p>我们的部署环境为 <a href="https://cloud.tencent.com/product/tke" target="_blank" rel="noopener">腾讯云容器服务</a>, 不过所述方案是普适于 <code>Kubernetes</code> 环境的.</p>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>PS: 下文假定读者了解</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/" target="_blank" rel="noopener">Kubernetes</a> 以及 YAML</li>
<li><a href="https://helm.sh/" target="_blank" rel="noopener">Helm</a></li>
<li><a href="https://www.nginx.com/" target="_blank" rel="noopener">Nginx</a></li>
</ul>
<h3 id="Nginx-镜像"><a href="#Nginx-镜像" class="headerlink" title="Nginx 镜像"></a>Nginx 镜像</h3><p>使用 Nginx 官方镜像便已经预装了 Mirror 插件</p>
<p>即: <code>docker pull nginx</code></p>
<p><code>yum install nginx</code> 安装的版本貌似没有 Mirror 插件的哦, 需要自己装</p>
<h3 id="Nginx-ConfigMap"><a href="#Nginx-ConfigMap" class="headerlink" title="Nginx ConfigMap"></a>Nginx ConfigMap</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">entrance-nginx-config</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">nginx.conf:</span> <span class="string">|-</span></span><br><span class="line">    <span class="string">worker_processes</span> <span class="string">auto;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">error_log</span> <span class="string">/data/athena/logs/entrance/nginx-error.log;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">events</span> <span class="string">&#123;</span></span><br><span class="line">      <span class="string">worker_connections</span>  <span class="number">1024</span><span class="string">;</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">http</span> <span class="string">&#123;</span></span><br><span class="line">      <span class="string">default_type</span>  <span class="string">application/octet-stream;</span></span><br><span class="line">      <span class="string">sendfile</span>        <span class="string">on;</span></span><br><span class="line">      <span class="string">keepalive_timeout</span>  <span class="number">65</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line">      <span class="string">server</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-access.log;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">listen</span>       <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;;</span></span><br><span class="line">        <span class="string">server_name</span>  <span class="string">entrance;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">root</span>   <span class="string">html;</span></span><br><span class="line">          <span class="string">index</span>  <span class="string">index.html</span> <span class="string">index.htm;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/entrance/</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">mirror</span> <span class="string">/mirror;</span></span><br><span class="line">          <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-entrance-access.log;</span></span><br><span class="line">          <span class="string">proxy_pass</span> <span class="attr">http://localhost:&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePortMirror</span> <span class="string">&#125;&#125;/;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/mirror</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">internal;</span></span><br><span class="line">          <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-mirror-access.log;</span></span><br><span class="line">          <span class="string">proxy_pass</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.mirrorProxyPass</span> <span class="string">&#125;&#125;;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  <span class="string">/50x.html;</span></span><br><span class="line">        <span class="string">location</span> <span class="string">=</span> <span class="string">/50x.html</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">root</span>   <span class="string">html;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line">      <span class="string">&#125;</span></span><br><span class="line">    <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>其中重点部分如下:</p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/nginx-config.png" alt=""></p>
<h3 id="业务方容器-Nginx-Mirror"><a href="#业务方容器-Nginx-Mirror" class="headerlink" title="业务方容器 + Nginx Mirror"></a>业务方容器 + Nginx Mirror</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.entrance.mirrorEnable</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">entrance-mirror</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.mirrorReplicaCount</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">            - weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">              podAffinityTerm:</span></span><br><span class="line"><span class="attr">                labelSelector:</span></span><br><span class="line"><span class="attr">                  matchExpressions:</span></span><br><span class="line"><span class="attr">                    - key:</span> <span class="string">"name"</span></span><br><span class="line"><span class="attr">                      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                      values:</span></span><br><span class="line"><span class="bullet">                        -</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">                topologyKey:</span> <span class="string">"kubernetes.io/hostname"</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">init-kafka</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"centos-dev"</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.delay</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'sleep 480s; until nslookup athena-cp-kafka; do echo "waiting for athena-cp-kafka"; sleep 2; done;'</span><span class="string">]</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">else</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'until nslookup athena-cp-kafka; do echo "waiting for athena-cp-kafka"; sleep 2; done;'</span><span class="string">]</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.entrance.image.repository &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.entrance.image.tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_KAFKA_BOOTSTRAP</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.kafka.kafkaBootstrap &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_KAFKA_SCHEMA_REGISTRY_URL</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.kafka.kafkaSchemaRegistryUrl &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_PG_CONN</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.pg.pgConn &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_COS_CONN</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.cos.cosConn &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_DEPLOY_TYPE</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.deployType &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_SYS_ID</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsSysId &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_SYS_SECRET</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsSysSecret &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_BASE_URL</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsBaseUrl &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_RESOURCE_FLOW_PERIOD_SEC</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsResourceFlowPeriodSec &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_CLUSTER</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.cluster &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_POD_NAME</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_HOST_IP</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">status.hostIP</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_POD_IP</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['/bin/bash',</span> <span class="string">'/data/service/go_workspace/script/start-entrance.sh'</span><span class="string">,</span> <span class="string">'-host 0.0.0.0:<span class="template-variable">&#123;&#123; .Values.entrance.service.nodePortMirror &#125;&#125;</span>'</span><span class="string">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/data/athena/</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">3000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">800</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span></span><br><span class="line"><span class="bullet">              -</span> <span class="string">bash</span></span><br><span class="line"><span class="bullet">              -</span> <span class="string">/data/service/go_workspace/script/health-check/check-entrance.sh</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">120</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.nginx.image.repository &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.nginx.image.tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance-mirror</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">          - containerPort:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">          - mountPath:</span> <span class="string">/data/athena/</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">            readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">          - mountPath:</span> <span class="string">/etc/nginx/nginx.conf</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">nginx-config</span></span><br><span class="line"><span class="attr">            subPath:</span> <span class="string">nginx.conf</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">1000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">500</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          tcpSocket:</span></span><br><span class="line"><span class="attr">            port:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">        entrance:</span> <span class="string">"true"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">"/data/athena/"</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nginx-config</span></span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">entrance-nginx-config</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      imagePullSecrets:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.imagePullSecrets &#125;&#125;</span>"</span></span><br><span class="line"><span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br></pre></td></tr></table></figure>
<p>上面为真实在业务中使用的 Deployment 配置, 有些地方可以参考:</p>
<ul>
<li><code>valueFrom.fieldRef.fieldPath</code> 可以取到容器运行时的一些字段, 如 <code>NodeIP</code>, <code>PodIP</code> 这些可以用于全链路监控</li>
<li><code>ConfigMap</code> 直接 Mount 到文件系统, 覆盖默认配置的例子</li>
<li><code>affinity.podAntiAffinity</code> 亲和性调度, 使 Pod 在主机间均匀分布</li>
<li>使用了 <code>tcpSocket</code> 和 <code>exec.command</code> 两种健康检查方式</li>
</ul>
<h3 id="Helm-Values"><a href="#Helm-Values" class="headerlink" title="Helm Values"></a>Helm Values</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># entrance, Athena 上报入口模块</span></span><br><span class="line"><span class="attr">entrance:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  replicaCount:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  mirrorEnable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  mirrorReplicaCount:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  mirrorProxyPass:</span> <span class="string">"http://10.16.0.147/entrance/"</span></span><br><span class="line"><span class="attr">  image:</span></span><br><span class="line"><span class="attr">    repository:</span> <span class="string">athena-go</span></span><br><span class="line"><span class="attr">    tag:</span> <span class="string">v1901091026</span></span><br><span class="line"><span class="attr">  service:</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30081</span></span><br><span class="line"><span class="attr">    nodePortMirror:</span> <span class="number">30082</span></span><br></pre></td></tr></table></figure>
<p>如上, <code>replicaCount: 3</code> + <code>mirrorReplicaCount: 1</code> = 4 个容器, 有 1/4 流量复制到 <code>http://10.16.0.147/entrance/</code></p>
<h3 id="内网负载均衡"><a href="#内网负载均衡" class="headerlink" title="内网负载均衡"></a>内网负载均衡</h3><p>流量复制到测试环境时, 尽量使用内网负载均衡, 为了成本, 安全及性能方面的考虑</p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/lb-inner.png" alt="LB-inner-config"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过下面几个步骤, 便可以实现流量复制啦</p>
<ul>
<li>建一个内网负载均衡, 暴漏测试环境的 <code>服务入口 Service</code></li>
<li><code>服务入口 Service</code> 需要有可以更换端口号的能力 (例如命令行参数/环境变量)</li>
<li>线上环境, 新增一个 Deployment, Label 和之前的 <code>服务入口 Service</code> 一样, 只是端口号分配一个新的</li>
<li>为新增的 Deployment 增加一个 Nginx 容器, 配置 nginx.conf</li>
<li>调节有 <code>Nginx Mirror</code> 的 Pod 和 正常的 <code>Pod</code> 比例, 便可以实现<code>按比例流量复制</code></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/01/10/k8s-traffic-copy/" data-id="ckac6j62n000fa6u6mm10bxqp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-cgroup-leaking" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/29/cgroup-leaking/" class="article-date">
  <time datetime="2018-12-29T09:00:00.000Z" itemprop="datePublished">2018-12-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/29/cgroup-leaking/">Cgroup泄漏--潜藏在你的集群中</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://github.com/honkiko" target="_blank" rel="noopener">洪志国</a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>绝大多数的kubernetes集群都有这个隐患。只不过一般情况下，泄漏得比较慢，还没有表现出来而已。 </p>
<p>一个pod可能泄漏两个memory cgroup数量配额。即使pod百分之百发生泄漏， 那也需要一个节点销毁过三万多个pod之后，才会造成后续pod创建失败。</p>
<p>一旦表现出来，这个节点就彻底不可用了，必须重启才能恢复。</p>
<h2 id="故障表现"><a href="#故障表现" class="headerlink" title="故障表现"></a>故障表现</h2><p>腾讯云SCF(Serverless Cloud Function)底层使用我们的TKE(Tencent Kubernetes Engine)，并且会在节点上频繁创建和消耗容器。</p>
<p>SCF发现很多节点会出现类似以下报错，创建POD总是失败：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dec 24 11:54:31 VM_16_11_centos dockerd[11419]: time=&quot;2018-12-24T11:54:31.195900301+08:00&quot; level=error msg=&quot;Handler for POST /v1.31/containers/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b/start returned error: OCI runtime create failed: container_linux.go:348: starting container process caused \&quot;process_linux.go:279: applying cgroup configuration for process caused \\\&quot;mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod79fe803c-072f-11e9-90ca-525400090c71/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b: no space left on device\\\&quot;\&quot;: unknown&quot;</span><br></pre></td></tr></table></figure>
<p>这个时候，到节点上尝试创建几十个memory cgroup (以root权限执行 <code>for i in</code>seq 1 20<code>;do mkdir /sys/fs/cgroup/memory/${i}; done</code>)，就会碰到失败：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir: cannot create directory &apos;/sys/fs/cgroup/memory/8&apos;: No space left on device</span><br></pre></td></tr></table></figure>
<p>其实，dockerd出现以上报错时， 手动创建<strong><em>一个</em></strong>memory cgroup都会失败的。 不过有时候随着一些POD的运行结束，可能会多出来一些“配额”，所以这里是尝试创建20个memory cgroup。</p>
<p>出现这样的故障以后，重启docker，释放内存等措施都没有效果，只有重启节点才能恢复。</p>
<h2 id="复现条件"><a href="#复现条件" class="headerlink" title="复现条件"></a>复现条件</h2><p>docker和kubernetes社区都有关于这个问题的issue:</p>
<ul>
<li><a href="https://github.com/moby/moby/issues/29638" target="_blank" rel="noopener">https://github.com/moby/moby/issues/29638</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/70324" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/70324</a></li>
</ul>
<p>网上有文章介绍了类似问题的分析和复现方法。如：<br><a href="http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage" target="_blank" rel="noopener">http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage</a></p>
<p>不过按照文中的复现方法，我在<code>3.10.0-862.9.1.el7.x86_64</code>版本内核上并没有复现出来。</p>
<p>经过反复尝试，总结出了必现的复现条件。 一句话感慨就是，把进程加入到一个开启了kmem accounting的memory cgroup<strong><em>并且执行fork系统调用</em></strong>。</p>
<ol>
<li>centos 3.10.0-862.9.1.el7.x86_64及以下内核， 4G以上空闲内存，root权限。</li>
<li><p>把系统memory cgroup配额占满</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in `seq 1 65536`;do mkdir /sys/fs/cgroup/memory/$&#123;i&#125;; done</span><br></pre></td></tr></table></figure>
<p> 会看到报错：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir: cannot create directory ‘/sys/fs/cgroup/memory/65530’: No space left on device</span><br></pre></td></tr></table></figure>
<p> 这是因为这个版本内核写死了，最多只能有65535个memory cgroup共存。 systemd已经创建了一些，所以这里创建不到65535个就会遇到报错。</p>
<p> 确认删掉一个memory cgroup, 就能腾出一个“配额”：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rmdir /sys/fs/cgroup/memory/1</span><br><span class="line">mkdir /sys/fs/cgroup/memory/test</span><br></pre></td></tr></table></figure>
</li>
<li><p>给一个memory cgroup开启kmem accounting</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /sys/fs/cgroup/memory/test/</span><br><span class="line">echo 1 &gt; memory.kmem.limit_in_bytes</span><br><span class="line">echo -1 &gt; memory.kmem.limit_in_bytes</span><br></pre></td></tr></table></figure>
</li>
<li><p>把一个进程加进某个memory cgroup, 并执行一次fork系统调用</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">最简单的就是把当前shell进程加进去：</span><br><span class="line">echo $$ &gt; /sys/fs/cgroup/memory/test/tasks</span><br><span class="line">sleep 100 &amp;</span><br><span class="line">cat /sys/fs/cgroup/memory/test/tasks</span><br></pre></td></tr></table></figure>
</li>
<li><p>把该memory cgroup里面的进程都挪走</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for p in `cat /sys/fs/cgroup/memory/test/tasks`;do echo $&#123;p&#125; &gt; /sys/fs/cgroup/memory/tasks; done</span><br><span class="line"></span><br><span class="line">cat /sys/fs/cgroup/memory/test/tasks  //这时候应该为空</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除这个memory cgroup</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmdir /sys/fs/cgroup/memory/test</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证刚才删除一个memory cgroup， 所占的配额并没有释放</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /sys/fs/cgroup/memory/xx</span><br></pre></td></tr></table></figure>
<p> 这时候会报错：<code>mkdir: cannot create directory ‘/sys/fs/cgroup/memory/xx’: No space left on device</code></p>
</li>
</ol>
<h2 id="什么版本的内核有这个问题"><a href="#什么版本的内核有这个问题" class="headerlink" title="什么版本的内核有这个问题"></a>什么版本的内核有这个问题</h2><p>搜索内核commit记录，有一个commit应该是解决类似问题的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4bdfc1c4a943: 2015-01-08 memcg: fix destination cgroup leak on task charges migration [Vladimir Davydov]</span><br></pre></td></tr></table></figure>
<p>这个commit在3.19以及4.x版本的内核中都已经包含。 不过从docker和kubernetes相关issue里面的反馈来看，内核中应该还有其他cgroup泄漏的代码路径， 4.14版本内核都还有cgroup泄漏问题。</p>
<h2 id="规避办法"><a href="#规避办法" class="headerlink" title="规避办法"></a>规避办法</h2><p>不开启kmem accounting (以上复现步骤的第3步)的话，是不会发生cgroup泄漏的。</p>
<p>kubelet和runc都会给memory cgroup开启kmem accounting。所以要规避这个问题，就要保证kubelet和runc，都别开启kmem accounting。下面分别进行说明。 </p>
<h2 id="runc"><a href="#runc" class="headerlink" title="runc"></a>runc</h2><p>查看代码，发现在commit fe898e7 (2017-2-25, PR #1350)以后的runc版本中，都会默认开启kmem accounting。代码在libcontainer/cgroups/fs/kmem.go: (老一点的版本，代码在libcontainer/cgroups/fs/memory.go)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">const cgroupKernelMemoryLimit = &quot;memory.kmem.limit_in_bytes&quot;</span><br><span class="line"></span><br><span class="line">func EnableKernelMemoryAccounting(path string) error &#123;</span><br><span class="line">        // Ensure that kernel memory is available in this kernel build. If it</span><br><span class="line">        // isn&apos;t, we just ignore it because EnableKernelMemoryAccounting is</span><br><span class="line">        // automatically called for all memory limits.</span><br><span class="line">        if !cgroups.PathExists(filepath.Join(path, cgroupKernelMemoryLimit)) &#123;</span><br><span class="line">                return nil</span><br><span class="line">        &#125;</span><br><span class="line">        // We have to limit the kernel memory here as it won&apos;t be accounted at all</span><br><span class="line">        // until a limit is set on the cgroup and limit cannot be set once the</span><br><span class="line">        // cgroup has children, or if there are already tasks in the cgroup.</span><br><span class="line">        for _, i := range []int64&#123;1, -1&#125; &#123;</span><br><span class="line">                if err := setKernelMemory(path, i); err != nil &#123;</span><br><span class="line">                        return err</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>runc社区也注意到这个问题，并做了比较灵活的修复： <a href="https://github.com/opencontainers/runc/pull/1921" target="_blank" rel="noopener">https://github.com/opencontainers/runc/pull/1921</a></p>
<p>这个修复给runc增加了”nokmem”编译选项。缺省的release版本没有使用这个选项。 自己使用nokmem选项编译runc的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $GO_PATH/src/github.com/opencontainers/runc/</span><br><span class="line">make BUILDTAGS=&quot;seccomp nokmem&quot;</span><br></pre></td></tr></table></figure>
<h2 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h2><p>kubelet在创建pod对应的cgroup目录时，也会调用libcontianer中的代码对cgroup做设置。在   <code>pkg/kubelet/cm/cgroup_manager_linux.go</code>的Create方法中，会调用Manager.Apply方法，最终调用<code>vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go</code>中的MemoryGroup.Apply方法，开启kmem accounting。</p>
<p>这里也需要进行处理，可以不开启kmem accounting， 或者通过命令行参数来控制是否开启。</p>
<p>kubernetes社区也有issue讨论这个问题：<a href="https://github.com/kubernetes/kubernetes/issues/70324" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/70324</a></p>
<p>但是目前还没有结论。我们TKE先直接把这部分代码注释掉了，不开启kmem accounting。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2018/12/29/cgroup-leaking/" data-id="ckac6j6290006a6u6ns9lgnjq" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-kernel-parameters-and-container" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/19/kernel-parameters-and-container/" class="article-date">
  <time datetime="2018-11-19T13:52:00.000Z" itemprop="datePublished">2018-11-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/19/kernel-parameters-and-container/">给容器设置内核参数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者： 洪志国</p>
<h1 id="sysctl"><a href="#sysctl" class="headerlink" title="sysctl"></a>sysctl</h1><p>/proc/sys/目录下导出了一些可以在运行时修改kernel参数的proc文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ls /proc/sys</span><br><span class="line">abi  crypto  debug  dev  fs  kernel  net  vm</span><br></pre></td></tr></table></figure>
<p>可以通过写proc文件来修改这些内核参数。例如， 要打开ipv4的路由转发功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure>
<p>也可以通过sysctl命令来完成(只是对以上写proc文件操作的简单包装)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.ip_forward=1</span><br></pre></td></tr></table></figure>
<p>其他常用sysctl命令：</p>
<p>显示本机所有sysctl内核参数及当前值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a</span><br></pre></td></tr></table></figure>
<p>从文件(缺省使用/etc/sysctl.conf)加载多个参数和取值，并写入内核</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -p [FILE]</span><br></pre></td></tr></table></figure>
<p>另外， 系统启动的时候， 会自动执行一下”sysctl -p”。 所以，希望重启之后仍然生效的参数值， 应该写到/etc/sysctl.conf文件里面。</p>
<h1 id="容器与sysctl"><a href="#容器与sysctl" class="headerlink" title="容器与sysctl"></a>容器与sysctl</h1><p>内核方面做了大量的工作，把一部分sysctl内核参数进行了namespace化(namespaced)。 也就是多个容器和主机可以各自独立设置某些内核参数。例如， 可以通过net.ipv4.ip_local_port_range，在不同容器中设置不同的端口范围。</p>
<p>如何判断一个参数是不是namespaced? </p>
<p>运行一个具有privileged权限的容器(参考下一节内容)， 然后在容器中修改该参数，看一下在host上能否看到容器在中所做的修改。如果看不到， 那就是namespaced， 否则不是。</p>
<p>目前已经namespace化的sysctl内核参数：</p>
<ul>
<li>kernel.shm*,</li>
<li>kernel.msg*,</li>
<li>kernel.sem,</li>
<li>fs.mqueue.*,</li>
<li>net.*.</li>
</ul>
<p>注意， vm.*并没有namespace化。 比如vm.max_map_count， 在主机或者一个容器中设置它， 其他所有容器都会受影响，都会看到最新的值。</p>
<h1 id="在docker容器中修改sysctl内核参数"><a href="#在docker容器中修改sysctl内核参数" class="headerlink" title="在docker容器中修改sysctl内核参数"></a>在docker容器中修改sysctl内核参数</h1><p>正常运行的docker容器中，是不能修改任何sysctl内核参数的。因为/proc/sys是以只读方式挂载到容器里面的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)</span><br></pre></td></tr></table></figure>
<p>要给容器设置不一样的sysctl内核参数，有多种方式。</p>
<h3 id="方法一-–privileged"><a href="#方法一-–privileged" class="headerlink" title="方法一 –privileged"></a>方法一 –privileged</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker run --privileged -it ubuntu bash</span><br></pre></td></tr></table></figure>
<p>整个/proc目录都是以”rw”权限挂载的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)</span><br></pre></td></tr></table></figure></p>
<p>在容器中，可以任意修改sysctl内核参赛。</p>
<p>注意：<br>如果修改的是namespaced的参数， 则不会影响host和其他容器。反之，则会影响它们。</p>
<p>如果想在容器中修改主机的net.ipv4.ip_default_ttl参数， 则除了–privileged， 还需要加上 –net=host。</p>
<h3 id="方法二-把-proc-sys-bind到容器里面"><a href="#方法二-把-proc-sys-bind到容器里面" class="headerlink" title="方法二 把/proc/sys bind到容器里面"></a>方法二 把/proc/sys bind到容器里面</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker run -v /proc/sys:/writable-sys -it ubuntu bash</span><br></pre></td></tr></table></figure>
<p>然后写bind到容器内的proc文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 62 &gt; /writable-sys/net/ipv4/ip_default_ttl</span><br></pre></td></tr></table></figure>
<p>注意：  这样操作，效果类似于”–privileged”, 对于namespaced的参数，不会影响host和其他容器。</p>
<h3 id="方法三-–sysctl"><a href="#方法三-–sysctl" class="headerlink" title="方法三 –sysctl"></a>方法三 –sysctl</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># docker run -it --sysctl &apos;net.ipv4.ip_default_ttl=63&apos; ubuntu sysctl net.ipv4.ip_default_ttl</span><br><span class="line">net.ipv4.ip_default_ttl = 63</span><br></pre></td></tr></table></figure>
<p>注意:</p>
<ul>
<li>只有namespaced参数才可以。否则会报错”invalid argument…”</li>
<li>这种方式只是在容器初始化过程中完成内核参数的修改，容器运行起来以后，/proc/sys仍然是以只读方式挂载的，在容器中不能再次修改sysctl内核参数。</li>
</ul>
<h1 id="kubernetes-与-sysctl"><a href="#kubernetes-与-sysctl" class="headerlink" title="kubernetes 与 sysctl"></a>kubernetes 与 sysctl</h1><h3 id="方法一-通过sysctls和unsafe-sysctls-annotation"><a href="#方法一-通过sysctls和unsafe-sysctls-annotation" class="headerlink" title="方法一 通过sysctls和unsafe-sysctls annotation"></a>方法一 通过sysctls和unsafe-sysctls annotation</h3><p>k8s还进一步把syctl参数分为safe和unsafe。 safe的条件：</p>
<ul>
<li>must not have any influence on any other pod on the node</li>
<li>must not allow to harm the node’s health</li>
<li>must not allow to gain CPU or memory resources outside of the resource limits of a pod.</li>
</ul>
<p>非namespaced的参数，肯定是unsafe。</p>
<p>namespaced参数，也只有一部分被认为是safe的。</p>
<p>在pkg/kubelet/sysctl/whitelist.go中维护了safe sysctl参数的名单。在1.7.8的代码中，只有三个参数被认为是safe的：</p>
<ul>
<li>kernel.shm_rmid_forced,</li>
<li>net.ipv4.ip_local_port_range,</li>
<li>net.ipv4.tcp_syncookies</li>
</ul>
<p>如果要设置一个POD中safe参数，通过security.alpha.kubernetes.io/sysctls这个annotation来传递给kubelet。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">metadata:</span><br><span class="line">  name: sysctl-example</span><br><span class="line">  annotations:</span><br><span class="line">    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=1</span><br></pre></td></tr></table></figure>
<p>如果要设置一个namespaced， 但是unsafe的参数，要使用另一个annotation: security.alpha.kubernetes.io/unsafe-sysctls， 另外还要给kubelet一个特殊的启动参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: sysctl-example</span><br><span class="line">  annotations:</span><br><span class="line">    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=1</span><br><span class="line">    security.alpha.kubernetes.io/unsafe-sysctls: net.ipv4.route.min_pmtu=1000,kernel.msgmax=1 2 3</span><br><span class="line">spec:</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>kubelet 增加–experimental-allowed-unsafe-sysctls启动参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubelet --experimental-allowed-unsafe-sysctls &apos;kernel.msg*,net.ipv4.route.min_pmtu&apos;</span><br></pre></td></tr></table></figure>
<h3 id="方法二-privileged-POD"><a href="#方法二-privileged-POD" class="headerlink" title="方法二 privileged POD"></a>方法二 privileged POD</h3><p>如果要修改的是非namespaced的参数， 如vm.*， 那就没办法使用以上方法。 可以给POD privileged权限，然后在容器的初始化脚本或代码中去修改sysctl参数。</p>
<p>创建POD/deployment/daemonset等对象时， 给容器的spec指定securityContext.privileged=true<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: nginx:alpine</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: true</span><br></pre></td></tr></table></figure></p>
<p>这样跟”docker run –privileged”效果一样，在POD中/proc是以”rw”权限mount的，可以直接修改相关sysctl内核参数。</p>
<h1 id="ulimit"><a href="#ulimit" class="headerlink" title="ulimit"></a>ulimit</h1><p>每个进程都有若干操作系统资源的限制， 可以通过 /proc/$PID/limits 来查看。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/1/limits </span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        0                    unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             62394                62394                processes </span><br><span class="line">Max open files            1024                 4096                 files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       62394                62394                signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us</span><br></pre></td></tr></table></figure>
<p>在bash中有个ulimit内部命令，可以查看当前bash进程的这些限制。</p>
<p>跟ulimit属性相关的配置文件是/etc/security/limits.conf。具体配置项和语法可以通过<code>man limits.conf</code> 命令查看。</p>
<h3 id="systemd给docker-daemon自身配置ulimit"><a href="#systemd给docker-daemon自身配置ulimit" class="headerlink" title="systemd给docker daemon自身配置ulimit"></a>systemd给docker daemon自身配置ulimit</h3><p>在service文件中(一般是/usr/lib/systemd/system/dockerd.service)中可以配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">LimitAS=infinity</span><br><span class="line">LimitRSS=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">ExecStart=...</span><br><span class="line">WorkingDirectory=...</span><br><span class="line">User=...</span><br><span class="line">Group=...</span><br></pre></td></tr></table></figure>
<h3 id="dockerd-给容器的-缺省ulimit设置"><a href="#dockerd-给容器的-缺省ulimit设置" class="headerlink" title="dockerd 给容器的 缺省ulimit设置"></a>dockerd 给容器的 缺省ulimit设置</h3><p>dockerd –default-ulimit nofile=65536:65536</p>
<p>冒号前面是soft limit, 后面是hard limit</p>
<h3 id="给容器指定ulimit设置"><a href="#给容器指定ulimit设置" class="headerlink" title="给容器指定ulimit设置"></a>给容器指定ulimit设置</h3><p>docker run -d –ulimit nofile=20480:40960 nproc=1024:2048 容器名</p>
<h3 id="在kubernetes中给pod设置ulimit参数"><a href="#在kubernetes中给pod设置ulimit参数" class="headerlink" title="在kubernetes中给pod设置ulimit参数"></a>在kubernetes中给pod设置ulimit参数</h3><p>有一个issue在讨论这个问题： <a href="https://github.com/kubernetes/kubernetes/issues/3595" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/3595</a></p>
<p>目前可行的办法，是在镜像中的初始化程序中调用setrlimit()系统调用来进行设置。子进程会继承父进程的ulimit参数。</p>
<h1 id="参考文档："><a href="#参考文档：" class="headerlink" title="参考文档："></a>参考文档：</h1><p><a href="http://tapd.oa.com/CCCM/prong/stories/view/1010166561060564549" target="_blank" rel="noopener">http://tapd.oa.com/CCCM/prong/stories/view/1010166561060564549</a></p>
<p><a href="https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/</a></p>
<p><a href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2018/11/19/kernel-parameters-and-container/" data-id="ckac6j62o000ga6u6vsm0bg0j" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-NodePort-SVC-LB直通容器性能测试对比" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/06/NodePort-SVC-LB直通容器性能测试对比/" class="article-date">
  <time datetime="2018-11-06T07:45:37.000Z" itemprop="datePublished">2018-11-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/06/NodePort-SVC-LB直通容器性能测试对比/">NodePort, svc, LB直通Pod性能测试对比</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者：郭志宏 </p>
<h3 id="1-测试背景："><a href="#1-测试背景：" class="headerlink" title="1. 测试背景："></a>1. 测试背景：</h3><p>目前基于k8s 服务的外网访问方式有以下几种：</p>
<ol>
<li>NodePort  </li>
<li>svc(通过k8s 的clusterip 访问)</li>
<li>自研 LB -&gt; Pod （比如pod ip 作为 nginx 的 upstream, 或者社区的nginx-ingress）</li>
</ol>
<p>其中第一种和第二种方案都要经过iptables 转发，第三种方案不经过iptables，本测试主要是为了测试这三种方案的性能损耗。</p>
<h3 id="2-测试方案"><a href="#2-测试方案" class="headerlink" title="2. 测试方案"></a>2. 测试方案</h3><p>为了做到测试的准确性和全面性，我们提供以下测试工具和测试数据：</p>
<ol>
<li><p>2核4G 的Pod</p>
</li>
<li><p>5个Node 的4核8G 集群</p>
</li>
<li><p>16核32G 的Nginx 作为统一的LB</p>
</li>
<li><p>一个测试应用，2个静态测试接口，分别对用不同大小的数据包（4k 和 100K）</p>
</li>
<li><p>测试1个pod ，10个pod的情况（service/pod 越多，一个机器上的iptables 规则数就越多，关于iptables规则数对转发性能的影响，在“ipvs和iptables模式下性能对⽐比测试报告” 已有结论： Iptables场景下，对应service在总数为2000内时，每个service 两个pod, 性能没有明显下降。当service总数达到3000、4000时，性能下降明显，service个数越多，性能越差。）所以这里就不考虑pod数太多的情况。</p>
</li>
<li><p>单独的16核32G 机器作作为压力机，使用wrk 作为压测工具, qps 作为评估标准，</p>
</li>
<li><p>那么每种访问方式对应以下4种情况</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>测试用例</th>
<th>Pod 数</th>
<th>数据包大小</th>
<th>平均QPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4k</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>100K</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>4k</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>10</td>
<td>100k</td>
</tr>
</tbody>
</table>
<ol start="8">
<li>每种情况测试5次，取平均值（qps），完善上表。</li>
</ol>
<h3 id="3-测试过程"><a href="#3-测试过程" class="headerlink" title="3. 测试过程"></a>3. 测试过程</h3><ol>
<li><p>准备一个测试应用（基于nginx），提供两个静态文件接口，分别返回4k的数据和100K 的数据。</p>
<p>镜像地址：ccr.ccs.tencentyun.com/caryguo/nginx:v0.1</p>
<p>接口：<a href="http://0.0.0.0/4k.html" target="_blank" rel="noopener">http://0.0.0.0/4k.html</a></p>
<p>​            <a href="http://0.0.0.0/100k.htm" target="_blank" rel="noopener">http://0.0.0.0/100k.htm</a></p>
</li>
<li><p>部署压测工具。<a href="https://github.com/wg/wrk" target="_blank" rel="noopener">https://github.com/wg/wrk</a></p>
</li>
<li><p>部署集群，5台Node来调度测试Pod, 10.0.4.6 这台用来独部署Nginx, 作为统一的LB, 将这台机器加入集群的目的是为了 将ClusterIP 作为nginx 的upstream .</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@VM-4-6-ubuntu:/etc/nginx# kubectl get node</span><br><span class="line">NAME        STATUS                     ROLES     AGE       VERSION</span><br><span class="line">10.0.4.12   Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.3    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.5    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.6    Ready,SchedulingDisabled   &lt;none&gt;    12m       v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.7    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.9    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据不同的测试场景，调整Nginx 的upstream, 根据不同的Pod, 调整压力，让请求的超时率控制在万分之一以内,  数据如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./wrk -c 200 -d 20 -t 10 http://carytest.pod.com/10k.html   单pod</span><br><span class="line">./wrk -c 1000 -d 20 -t 100 http://carytest.pod.com/4k.html  10 pod</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试wrk -&gt; nginx -&gt; Pod 场景，</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>测试用例</th>
<th>Pod 数</th>
<th>数据包大小</th>
<th>平均QPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4k</td>
<td>12498</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>100K</td>
<td>2037</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>4k</td>
<td>82752</td>
</tr>
<tr>
<td>4</td>
<td>10</td>
<td>100k</td>
<td>7743</td>
</tr>
</tbody>
</table>
<ol start="5">
<li>wrk -&gt; nginx -&gt; ClusterIP -&gt; Pod</li>
</ol>
<table>
<thead>
<tr>
<th>测试用例</th>
<th>Pod 数</th>
<th>数据包大小</th>
<th>平均QPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4k</td>
<td>12568</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>100K</td>
<td>2040</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>4k</td>
<td>81752</td>
</tr>
<tr>
<td>4</td>
<td>10</td>
<td>100k</td>
<td>7824</td>
</tr>
</tbody>
</table>
<ol start="6">
<li>NodePort 场景，wrk -&gt; nginx -&gt; NodePort -&gt; Pod</li>
</ol>
<table>
<thead>
<tr>
<th>测试用例</th>
<th>Pod 数</th>
<th>数据包大小</th>
<th>平均QPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>4k</td>
<td>12332</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>100K</td>
<td>2028</td>
</tr>
<tr>
<td>3</td>
<td>10</td>
<td>4k</td>
<td>76973</td>
</tr>
<tr>
<td>4</td>
<td>10</td>
<td>100k</td>
<td>5676</td>
</tr>
</tbody>
</table>
<p>压测过程中，4k 数据包的情况下，应用的负载都在80% -100% 之间， 100k 情况下，应用的负载都在20%-30%</p>
<p>之间，压力都在网络消耗上，没有到达服务后端。</p>
<h3 id="4-测试结论"><a href="#4-测试结论" class="headerlink" title="4. 测试结论"></a>4. 测试结论</h3><ol>
<li>在一个pod 的情况下（4k 或者100 数据包），3中网络方案差别不大，QPS 差距在3% 以内。</li>
<li>在10个pod，4k 数据包情况下，lb-&gt;pod 和 svc 差距不大，NodePort 损失近7% 左右。</li>
<li>10个Pod, 100k 数据包的情况下，lb-&gt;pod 和 svc 差距不大，NodePort 损失近 25% </li>
</ol>
<h3 id="5-附录"><a href="#5-附录" class="headerlink" title="5. 附录"></a>5. 附录</h3><ol>
<li>nginx 配置</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">user nginx;</span><br><span class="line">worker_processes 50;</span><br><span class="line">error_log /var/log/nginx/error.log;</span><br><span class="line">pid /run/nginx.pid;</span><br><span class="line"></span><br><span class="line"># Load dynamic modules. See /usr/share/nginx/README.dynamic.</span><br><span class="line">include /usr/share/nginx/modules/*.conf;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 100000;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">                      &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">                      &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    keepalive_timeout   65;</span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line"></span><br><span class="line">    include             /etc/nginx/mime.types;</span><br><span class="line">    default_type        application/octet-stream;</span><br><span class="line"></span><br><span class="line">    # Load modular configuration files from the /etc/nginx/conf.d directory.</span><br><span class="line">    # See http://nginx.org/en/docs/ngx_core_module.html#include</span><br><span class="line">    # for more information.</span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    </span><br><span class="line">     # pod ip</span><br><span class="line">    upstream  panda-pod &#123;</span><br><span class="line">          #ip_hash;</span><br><span class="line">          # Pod ip</span><br><span class="line">          #server   10.0.4.12:30734  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.1.5:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.2.3:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.3.5:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.4.6:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.4.5:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.3.6:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.1.4:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.0.7:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.0.6:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.2.2:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          </span><br><span class="line">          # svc ip</span><br><span class="line">          #server   172.16.255.121:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">           </span><br><span class="line">          # NodePort</span><br><span class="line">          server   10.0.4.12:30734   max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.3:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.5:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.7:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.9:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">		    </span><br><span class="line">          keepalive 256;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  carytest.pod.com;</span><br><span class="line">        # root   /usr/share/nginx/html;</span><br><span class="line">        charset utf-8;</span><br><span class="line"></span><br><span class="line">        # Load configuration files for the default server block.</span><br><span class="line">        include /etc/nginx/default.d/*.conf;</span><br><span class="line">        location / &#123;</span><br><span class="line">                    proxy_pass        http://panda-pod;</span><br><span class="line">                    proxy_http_version 1.1;</span><br><span class="line">                    proxy_set_header Connection &quot;&quot;;</span><br><span class="line">                    proxy_redirect off;</span><br><span class="line">                    proxy_set_header  Host  $host;</span><br><span class="line">                    proxy_set_header  X-Real-IP  $remote_addr;</span><br><span class="line">                    proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;</span><br><span class="line">                    proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page 404 /404.html;</span><br><span class="line">            location = /40x.html &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page 500 502 503 504 /50x.html;</span><br><span class="line">            location = /50x.html &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2018/11/06/NodePort-SVC-LB直通容器性能测试对比/" data-id="ckac6j61y0001a6u6tx5vdut6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-k8s-npc-kr-function" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/30/k8s-npc-kr-function/" class="article-date">
  <time datetime="2018-10-30T09:22:24.000Z" itemprop="datePublished">2018-10-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/30/k8s-npc-kr-function/">K8s Network Policy Controller之Kube-router功能介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Author: <a href="https://github.com/jimmy-zh" target="_blank" rel="noopener">Jimmy Zhang</a> (张浩)</p>
<h1 id="Network-Policy"><a href="#Network-Policy" class="headerlink" title="Network Policy"></a>Network Policy</h1><p><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" target="_blank" rel="noopener">Network Policy</a>是k8s提供的一种资源，用于定义基于pod的网络隔离策略。它描述了一组pod是否可以与其它组pod，以及其它network endpoints进行通信。</p>
<h1 id="Kube-router"><a href="#Kube-router" class="headerlink" title="Kube-router"></a>Kube-router</h1><ul>
<li>官网:  <a href="https://www.kube-router.io" target="_blank" rel="noopener">https://www.kube-router.io</a></li>
<li>项目:  <a href="https://github.com/cloudnativelabs/kube-router" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router</a></li>
<li>目前最新版本：<a href="https://github.com/cloudnativelabs/kube-router/releases/tag/v0.2.1" target="_blank" rel="noopener">v0.2.1</a></li>
</ul>
<p>kube-router项目的三大功能：</p>
<ul>
<li>Pod Networking</li>
<li>IPVS/LVS based service proxy  </li>
<li>Network Policy Controller </li>
</ul>
<p>在腾讯云TKE上，Pod Networking功能由基于IAAS层VPC的高性能容器网络实现，service proxy功能由kube-proxy所支持的ipvs/iptables两种模式实现。建议在TKE上，只使用kube-router的Network Policy功能。</p>
<h1 id="在TKE上部署kube-router"><a href="#在TKE上部署kube-router" class="headerlink" title="在TKE上部署kube-router"></a>在TKE上部署kube-router</h1><h3 id="腾讯云提供的kube-router版本"><a href="#腾讯云提供的kube-router版本" class="headerlink" title="腾讯云提供的kube-router版本"></a>腾讯云提供的kube-router版本</h3><p>腾讯云PAAS团队提供的镜像”ccr.ccs.tencentyun.com/library/kube-router:v1”基于官方的最新版本：<a href="https://github.com/cloudnativelabs/kube-router/releases/tag/v0.2.1" target="_blank" rel="noopener">v0.2.1</a></p>
<p>在该项目的开发过程中，腾讯云PAAS团队积极参与社区，持续贡献了一些feature support和bug fix, 列表如下（均已被社区合并）：</p>
<ul>
<li><a href="https://github.com/cloudnativelabs/kube-router/pull/488" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/488</a></li>
<li><a href="https://github.com/cloudnativelabs/kube-router/pull/498" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/498</a></li>
<li><a href="https://github.com/cloudnativelabs/kube-router/pull/527" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/527</a></li>
<li><a href="https://github.com/cloudnativelabs/kube-router/pull/529" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/529</a></li>
<li><a href="https://github.com/cloudnativelabs/kube-router/pull/543" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/543</a></li>
</ul>
<p>我们会继续贡献社区，并提供腾讯云镜像的版本升级。</p>
<h3 id="部署kube-router"><a href="#部署kube-router" class="headerlink" title="部署kube-router"></a>部署kube-router</h3><p>Daemonset yaml文件:</p>
<blockquote>
<p><a href="https://ask.qcloudimg.com/draft/982360/9wn7eu0bek.zip" target="_blank" rel="noopener">#kube-router-firewall-daemonset.yaml.zip#</a></p>
</blockquote>
<p>在<strong>能访问公网</strong>，也能访问TKE集群apiserver的机器上，执行以下命令即可完成kube-router部署。</p>
<p>如果集群节点开通了公网IP，则可以直接在集群节点上执行以下命令。</p>
<p>如果集群节点没有开通公网IP, 则可以手动下载和粘贴yaml文件内容到节点, 保存为kube-router-firewall-daemonset.yaml，再执行最后的kubectl create命令。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://ask.qcloudimg.com/draft/982360/9wn7eu0bek.zip</span><br><span class="line">unzip 9wn7eu0bek.zip</span><br><span class="line">kuebectl create -f kube-router-firewall-daemonset.yaml</span><br></pre></td></tr></table></figure>
<h3 id="yaml文件内容和参数说明"><a href="#yaml文件内容和参数说明" class="headerlink" title="yaml文件内容和参数说明"></a>yaml文件内容和参数说明</h3><p>kube-router-firewall-daemonset.yaml文件内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-router-cfg</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    tier: node</span><br><span class="line">    k8s-app: kube-router</span><br><span class="line">data:</span><br><span class="line">  cni-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;:&quot;kubernetes&quot;,</span><br><span class="line">      &quot;type&quot;:&quot;bridge&quot;,</span><br><span class="line">      &quot;bridge&quot;:&quot;kube-bridge&quot;,</span><br><span class="line">      &quot;isDefaultGateway&quot;:true,</span><br><span class="line">      &quot;ipam&quot;: &#123;</span><br><span class="line">        &quot;type&quot;:&quot;host-local&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-router</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-router</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kube-router</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kube-router</span><br><span class="line">        image: ccr.ccs.tencentyun.com/library/kube-router:v1</span><br><span class="line">        args: [&quot;--run-router=false&quot;, &quot;--run-firewall=true&quot;, &quot;--run-service-proxy=false&quot;, &quot;--kubeconfig=/var/lib/kube-router/kubeconfig&quot;, &quot;--iptables-sync-period=5m&quot;, &quot;--cache-sync-timeout=3m&quot;]</span><br><span class="line">        securityContext:</span><br><span class="line">          privileged: true</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        env:</span><br><span class="line">        - name: NODE_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: spec.nodeName</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /healthz</span><br><span class="line">            port: 20244</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          periodSeconds: 3</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: lib-modules</span><br><span class="line">          mountPath: /lib/modules</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: cni-conf-dir</span><br><span class="line">          mountPath: /etc/cni/net.d</span><br><span class="line">        - name: kubeconfig</span><br><span class="line">          mountPath: /var/lib/kube-router/kubeconfig</span><br><span class="line">          readOnly: true</span><br><span class="line">      initContainers:</span><br><span class="line">      - name: install-cni</span><br><span class="line">        image: busybox</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        command:</span><br><span class="line">        - /bin/sh</span><br><span class="line">        - -c</span><br><span class="line">        - set -e -x;</span><br><span class="line">          if [ ! -f /etc/cni/net.d/10-kuberouter.conf ]; then</span><br><span class="line">            TMP=/etc/cni/net.d/.tmp-kuberouter-cfg;</span><br><span class="line">            cp /etc/kube-router/cni-conf.json $&#123;TMP&#125;;</span><br><span class="line">            mv $&#123;TMP&#125; /etc/cni/net.d/10-kuberouter.conf;</span><br><span class="line">          fi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: cni-conf-dir</span><br><span class="line">          mountPath: /etc/cni/net.d</span><br><span class="line">        - name: kube-router-cfg</span><br><span class="line">          mountPath: /etc/kube-router</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: CriticalAddonsOnly</span><br><span class="line">        operator: Exists</span><br><span class="line">      - effect: NoSchedule</span><br><span class="line">        key: node-role.kubernetes.io/master</span><br><span class="line">        operator: Exists</span><br><span class="line">      volumes:</span><br><span class="line">      - name: lib-modules</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /lib/modules</span><br><span class="line">      - name: cni-conf-dir</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /etc/cni/net.d</span><br><span class="line">      - name: kube-router-cfg</span><br><span class="line">        configMap:</span><br><span class="line">          name: kube-router-cfg</span><br><span class="line">      - name: kubeconfig</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /root/.kube/config</span><br></pre></td></tr></table></figure>
<p>args说明：</p>
<ol>
<li>“–run-router=false”, “–run-firewall=true”, “–run-service-proxy=false”：只加载firewall模块；</li>
<li>kubeconfig：用于指定master信息，映射到主机上的kubectl配置目录/root/.kube/config；</li>
<li>–iptables-sync-period=5m：指定定期同步iptables规则的间隔时间，根据准确性的要求设置，默认5m；</li>
<li>–cache-sync-timeout=3m：指定启动时将k8s资源做缓存的超时时间，默认1m；</li>
</ol>
<h1 id="NetworkPolicy配置示例"><a href="#NetworkPolicy配置示例" class="headerlink" title="NetworkPolicy配置示例"></a>NetworkPolicy配置示例</h1><h3 id="1-nsa-namespace下的pod可互相访问，而不能被其它任何pod访问"><a href="#1-nsa-namespace下的pod可互相访问，而不能被其它任何pod访问" class="headerlink" title="1.nsa namespace下的pod可互相访问，而不能被其它任何pod访问"></a>1.nsa namespace下的pod可互相访问，而不能被其它任何pod访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  ingress: </span><br><span class="line">  - from:</span><br><span class="line">    - podSelector: &#123;&#125; </span><br><span class="line">  podSelector: &#123;&#125; </span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure>
<h3 id="2-nsa-namespace下的pod不能被任何pod访问"><a href="#2-nsa-namespace下的pod不能被任何pod访问" class="headerlink" title="2.nsa namespace下的pod不能被任何pod访问"></a>2.nsa namespace下的pod不能被任何pod访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure>
<h3 id="3-nsa-namespace下的pod只在6379-TCP端口可以被带有标签app-nsb的namespace下的pod访问，而不能被其它任何pod访问"><a href="#3-nsa-namespace下的pod只在6379-TCP端口可以被带有标签app-nsb的namespace下的pod访问，而不能被其它任何pod访问" class="headerlink" title="3.nsa namespace下的pod只在6379/TCP端口可以被带有标签app: nsb的namespace下的pod访问，而不能被其它任何pod访问"></a>3.nsa namespace下的pod只在6379/TCP端口可以被带有标签app: nsb的namespace下的pod访问，而不能被其它任何pod访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: nsb</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 6379</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure>
<h3 id="4-nsa-namespace下的pod可以访问CIDR为14-215-0-0-16的network-endpoint的5978-TCP端口，而不能访问其它任何network-endpoints（此方式可以用来为集群内的服务开访问外部network-endpoints的白名单）"><a href="#4-nsa-namespace下的pod可以访问CIDR为14-215-0-0-16的network-endpoint的5978-TCP端口，而不能访问其它任何network-endpoints（此方式可以用来为集群内的服务开访问外部network-endpoints的白名单）" class="headerlink" title="4.nsa namespace下的pod可以访问CIDR为14.215.0.0/16的network endpoint的5978/TCP端口，而不能访问其它任何network endpoints（此方式可以用来为集群内的服务开访问外部network endpoints的白名单）"></a>4.nsa namespace下的pod可以访问CIDR为14.215.0.0/16的network endpoint的5978/TCP端口，而不能访问其它任何network endpoints（此方式可以用来为集群内的服务开访问外部network endpoints的白名单）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  egress:</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 14.215.0.0/16</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 5978</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Egress</span><br></pre></td></tr></table></figure>
<h3 id="5-default-namespace下的pod只在80-TCP端口可以被CIDR为14-215-0-0-16的network-endpoint访问，而不能被其它任何network-endpoints访问"><a href="#5-default-namespace下的pod只在80-TCP端口可以被CIDR为14-215-0-0-16的network-endpoint访问，而不能被其它任何network-endpoints访问" class="headerlink" title="5.default namespace下的pod只在80/TCP端口可以被CIDR为14.215.0.0/16的network endpoint访问，而不能被其它任何network endpoints访问"></a>5.default namespace下的pod只在80/TCP端口可以被CIDR为14.215.0.0/16的network endpoint访问，而不能被其它任何network endpoints访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npd</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 14.215.0.0/16</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure>
<h1 id="附-测试情况"><a href="#附-测试情况" class="headerlink" title="附: 测试情况"></a>附: 测试情况</h1><table>
<thead>
<tr>
<th style="text-align:left">用例名称</th>
<th style="text-align:left">测试结果</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">不同namespace的pod互相隔离，同一namespace的pod互通</td>
<td style="text-align:left">通过</td>
</tr>
<tr>
<td style="text-align:left">不同namespace的pod互相隔离，同一namespace的pod隔离</td>
<td style="text-align:left">通过</td>
</tr>
<tr>
<td style="text-align:left">不同namespace的pod互相隔离，白名单指定B可以访问A</td>
<td style="text-align:left">通过</td>
</tr>
<tr>
<td style="text-align:left">允许某个namespace访问集群外某个CIDR，其他外部IP全部隔离</td>
<td style="text-align:left">通过</td>
</tr>
<tr>
<td style="text-align:left">不同namespace的pod互相隔离，白名单指定B可以访问A中对应的pod以及端口</td>
<td style="text-align:left">通过</td>
</tr>
<tr>
<td style="text-align:left">以上用例，当source pod 和 destination pod在一个node上时，隔离是否生效</td>
<td style="text-align:left">通过</td>
</tr>
</tbody>
</table>
<p>功能测试用例</p>
<blockquote>
<p><a href="https://ask.qcloudimg.com/draft/982360/dgs7x4hcly.zip" target="_blank" rel="noopener">#kube-router测试用例.xlsx.zip#</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2018/10/30/k8s-npc-kr-function/" data-id="ckac6j62l000ea6u6yhj2oem8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-DNS-5-seconds-delay" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/26/DNS-5-seconds-delay/" class="article-date">
  <time datetime="2018-10-26T07:52:37.000Z" itemprop="datePublished">2018-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/26/DNS-5-seconds-delay/">kubernetes集群中夺命的5秒DNS延迟</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者： 洪志国</p>
<h2 id="超时问题"><a href="#超时问题" class="headerlink" title="超时问题"></a>超时问题</h2><p>客户反馈从pod中访问服务时，总是有些请求的响应时延会达到5秒。正常的响应只需要毫秒级别的时延。</p>
<h2 id="DNS-5秒延时"><a href="#DNS-5秒延时" class="headerlink" title="DNS 5秒延时"></a>DNS 5秒延时</h2><p>在pod中(通过nsenter -n tcpdump)抓包，发现是有的DNS请求没有收到响应，超时5秒后，再次发送DNS请求才成功收到响应。</p>
<p>在kube-dns pod抓包，发现是有DNS请求没有到达kube-dns pod， 在中途被丢弃了。</p>
<p>为什么是5秒？ <code>man resolv.conf</code>可以看到glibc的resolver的缺省超时时间是5s。</p>
<h2 id="丢包原因"><a href="#丢包原因" class="headerlink" title="丢包原因"></a>丢包原因</h2><p>经过搜索发现这是一个普遍问题。<br>根本原因是内核conntrack模块的bug。</p>
<p>Weave works的工程师<a href="martynas@weave.works">Martynas Pumputis</a>对这个问题做了很详细的分析：<br><a href="https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts" target="_blank" rel="noopener">https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts</a></p>
<p>相关结论：</p>
<ul>
<li>只有多个线程或进程，并发从同一个socket发送相同五元组的UDP报文时，才有一定概率会发生</li>
<li>glibc, musl(alpine linux的libc库)都使用”parallel query”, 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃</li>
<li>由于ipvs也使用了conntrack, 使用kube-proxy的ipvs模式，并不能避免这个问题</li>
</ul>
<h2 id="问题的根本解决"><a href="#问题的根本解决" class="headerlink" title="问题的根本解决"></a>问题的根本解决</h2><p>Martynas向内核提交了两个patch来fix这个问题，不过他说如果集群中有多个DNS server的情况下，问题并没有完全解决。</p>
<p>其中一个patch已经在2018-7-18被合并到linux内核主线中: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ed07d9a021df6da53456663a76999189badc432a" target="_blank" rel="noopener">netfilter: nf_conntrack: resolve clash for matching conntracks</a></p>
<p>目前只有4.19.rc 版本包含这个patch。</p>
<h2 id="规避办法"><a href="#规避办法" class="headerlink" title="规避办法"></a>规避办法</h2><h4 id="规避方案一：使用TCP发送DNS请求"><a href="#规避方案一：使用TCP发送DNS请求" class="headerlink" title="规避方案一：使用TCP发送DNS请求"></a>规避方案一：使用TCP发送DNS请求</h4><p>由于TCP没有这个问题，有人提出可以在容器的resolv.conf中增加<code>options use-vc</code>, 强制glibc使用TCP协议发送DNS query。下面是这个man resolv.conf中关于这个选项的说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">use-vc (since glibc 2.14)</span><br><span class="line">                     Sets RES_USEVC in _res.options.  This option forces the</span><br><span class="line">                     use of TCP for DNS resolutions.</span><br></pre></td></tr></table></figure>
<p>笔者使用镜像”busybox:1.29.3-glibc” (libc 2.24)  做了试验，并没有见到这样的效果，容器仍然是通过UDP发送DNS请求。</p>
<h4 id="规避方案二：避免相同五元组DNS请求的并发"><a href="#规避方案二：避免相同五元组DNS请求的并发" class="headerlink" title="规避方案二：避免相同五元组DNS请求的并发"></a>规避方案二：避免相同五元组DNS请求的并发</h4><p>resolv.conf还有另外两个相关的参数： </p>
<ul>
<li>single-request-reopen (since glibc 2.9)</li>
<li>single-request (since glibc 2.10)</li>
</ul>
<p>man resolv.conf中解释如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">single-request-reopen (since glibc 2.9)</span><br><span class="line">                     Sets RES_SNGLKUPREOP in _res.options.  The resolver</span><br><span class="line">                     uses the same socket for the A and AAAA requests.  Some</span><br><span class="line">                     hardware mistakenly sends back only one reply.  When</span><br><span class="line">                     that happens the client system will sit and wait for</span><br><span class="line">                     the second reply.  Turning this option on changes this</span><br><span class="line">                     behavior so that if two requests from the same port are</span><br><span class="line">                     not handled correctly it will close the socket and open</span><br><span class="line">                     a new one before sending the second request.</span><br><span class="line">                     </span><br><span class="line">single-request (since glibc 2.10)</span><br><span class="line">                     Sets RES_SNGLKUP in _res.options.  By default, glibc</span><br><span class="line">                     performs IPv4 and IPv6 lookups in parallel since</span><br><span class="line">                     version 2.9.  Some appliance DNS servers cannot handle</span><br><span class="line">                     these queries properly and make the requests time out.</span><br><span class="line">                     This option disables the behavior and makes glibc</span><br><span class="line">                     perform the IPv6 and IPv4 requests sequentially (at the</span><br><span class="line">                     cost of some slowdown of the resolving process).</span><br></pre></td></tr></table></figure>
<p>笔者做了试验，发现效果是这样的：</p>
<ul>
<li>single-request-reopen<br>发送A类型请求和AAAA类型请求使用不同的源端口。这样两个请求在conntrack表中不占用同一个表项，从而避免冲突。</li>
<li>single-request<br>避免并发，改为串行发送A类型和AAAA类型请求。没有了并发，从而也避免了冲突。</li>
</ul>
<p>要给容器的resolv.conf加上options参数，有几个办法：</p>
<h5 id="1-在容器的”ENTRYPOINT”或者”CMD”脚本中，执行-bin-echo-39-options-single-request-reopen-39-gt-gt-etc-resolv-conf"><a href="#1-在容器的”ENTRYPOINT”或者”CMD”脚本中，执行-bin-echo-39-options-single-request-reopen-39-gt-gt-etc-resolv-conf" class="headerlink" title="1) 在容器的”ENTRYPOINT”或者”CMD”脚本中，执行/bin/echo &#39;options single-request-reopen&#39; &gt;&gt; /etc/resolv.conf"></a>1) 在容器的”ENTRYPOINT”或者”CMD”脚本中，执行<code>/bin/echo &#39;options single-request-reopen&#39; &gt;&gt; /etc/resolv.conf</code></h5><h5 id="2-在pod的postStart-hook中："><a href="#2-在pod的postStart-hook中：" class="headerlink" title="2) 在pod的postStart hook中："></a>2) 在pod的postStart hook中：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lifecycle:</span><br><span class="line">  postStart:</span><br><span class="line">    exec:</span><br><span class="line">      command:</span><br><span class="line">      - /bin/sh</span><br><span class="line">      - -c </span><br><span class="line">      - &quot;/bin/echo &apos;options single-request-reopen&apos; &gt;&gt; /etc/resolv.conf&quot;</span><br></pre></td></tr></table></figure>
<h5 id="3-使用template-spec-dnsConfig-k8s-v1-9-及以上才支持"><a href="#3-使用template-spec-dnsConfig-k8s-v1-9-及以上才支持" class="headerlink" title="3) 使用template.spec.dnsConfig (k8s v1.9 及以上才支持):"></a>3) 使用template.spec.dnsConfig (k8s v1.9 及以上才支持):</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">template:</span><br><span class="line">  spec:</span><br><span class="line">    dnsConfig:</span><br><span class="line">      options:</span><br><span class="line">        - name: single-request-reopen</span><br></pre></td></tr></table></figure>
<h5 id="4-使用ConfigMap覆盖POD里面的-etc-resolv-conf"><a href="#4-使用ConfigMap覆盖POD里面的-etc-resolv-conf" class="headerlink" title="4) 使用ConfigMap覆盖POD里面的/etc/resolv.conf"></a>4) 使用ConfigMap覆盖POD里面的/etc/resolv.conf</h5><p>configmap:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  resolv.conf: |</span><br><span class="line">    nameserver 1.2.3.4</span><br><span class="line">    search default.svc.cluster.local svc.cluster.local cluster.local ec2.internal</span><br><span class="line">    options ndots:5 single-request-reopen timeout:1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: resolvconf</span><br></pre></td></tr></table></figure></p>
<p>POD spec:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">        volumeMounts:</span><br><span class="line">        - name: resolv-conf</span><br><span class="line">          mountPath: /etc/resolv.conf</span><br><span class="line">          subPath: resolv.conf</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">      volumes:</span><br><span class="line">      - name: resolv-conf</span><br><span class="line">        configMap:</span><br><span class="line">          name: resolvconf</span><br><span class="line">          items:</span><br><span class="line">          - key: resolv.conf</span><br><span class="line">            path: resolv.conf</span><br></pre></td></tr></table></figure></p>
<h5 id="5-使用MutatingAdmissionWebhook"><a href="#5-使用MutatingAdmissionWebhook" class="headerlink" title="5) 使用MutatingAdmissionWebhook"></a>5) 使用MutatingAdmissionWebhook</h5><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook-beta-in-1-9" target="_blank" rel="noopener">MutatingAdmissionWebhook</a> 是1.9引入的Controller，用于对一个指定的Resource的操作之前，对这个resource进行变更。<br>istio的自动sidecar注入就是用这个功能来实现的。 我们也可以通过MutatingAdmissionWebhook，来自动给所有POD，注入以上3)或者4)所需要的相关内容。</p>
<hr>
<p>以上方法中， 1)和2)都需要修改镜像， 3)和4)则只需要修改POD的spec， 能适用于所有镜像。不过还是有不方便的地方：</p>
<ul>
<li>每个工作负载的yaml都要做修改，比较麻烦</li>
<li>对于通过helm创建的工作负载，需要修改helm charts</li>
</ul>
<p>方法5)对集群使用者最省事，照常提交工作负载即可。不过初期需要一定的开发工作量。</p>
<h4 id="规避方案三：使用本地DNS缓存"><a href="#规避方案三：使用本地DNS缓存" class="headerlink" title="规避方案三：使用本地DNS缓存"></a>规避方案三：使用本地DNS缓存</h4><p>容器的DNS请求都发往本地的DNS缓存服务(dnsmasq, nscd等)，不需要走DNAT，也不会发生conntrack冲突。另外还有个好处，就是避免DNS服务成为性能瓶颈。</p>
<p>使用本地DNS缓存有两种方式：</p>
<ul>
<li>每个容器自带一个DNS缓存服务</li>
<li>每个节点运行一个DNS缓存服务，所有容器都把本节点的DNS缓存作为自己的nameserver</li>
</ul>
<p>从资源效率的角度来考虑的话，推荐后一种方式。</p>
<h5 id="实施办法"><a href="#实施办法" class="headerlink" title="实施办法"></a>实施办法</h5><p>条条大路通罗马，不管怎么做，最终到达上面描述的效果即可。</p>
<p>POD中要访问节点上的DNS缓存服务，可以使用节点的IP。 如果节点上的容器都连在一个虚拟bridge上， 也可以使用这个bridge的三层接口的IP(在TKE中，这个三层接口叫cbr0)。 要确保DNS缓存服务监听这个地址。</p>
<p>如何把POD的/etc/resolv.conf中的nameserver设置为节点IP呢？</p>
<p>一个办法，是设置POD.spec.dnsPolicy为”Default”， 意思是POD里面的/etc/resolv.conf， 使用节点上的文件。缺省使用节点上的/etc/resolv.conf(如果kubelet通过参数–resolv-conf指定了其他文件，则使用–resolv-conf所指定的文件)。</p>
<p>另一个办法，是给每个节点的kubelet指定不同的–cluster-dns参数，设置为节点的IP，POD.spec.dnsPolicy仍然使用缺省值”ClusterFirst”。 kops项目甚至有个issue在讨论如何在部署集群时设置好–cluster-dns指向节点IP: <a href="https://github.com/kubernetes/kops/issues/5584" target="_blank" rel="noopener">https://github.com/kubernetes/kops/issues/5584</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2018/10/26/DNS-5-seconds-delay/" data-id="ckac6j61r0000a6u6k5q1mt87" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-开源项目" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/21/开源项目/" class="article-date">
  <time datetime="2018-10-21T10:27:56.000Z" itemprop="datePublished">2018-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/21/开源项目/">开源组件</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>腾讯云容器团队现有开源组件：</p>
<ul>
<li>基于 csi 的 <a href="https://github.com/TencentCloud/kubernetes-csi-tencentcloud" target="_blank" rel="noopener">kubernetes volume 插件</a></li>
<li>基于 cni 的 <a href="https://github.com/TencentCloud/cni-bridge-networking" target="_blank" rel="noopener">bridge 插件</a></li>
<li>适配黑石负载均衡的 <a href="https://github.com/TencentCloud/ingress-tke-bm" target="_blank" rel="noopener">ingress 插件</a></li>
<li>适配腾讯云 cvm/clb/vpc 的 <a href="https://github.com/TencentCloud/tencentcloud-cloud-controller-manager" target="_blank" rel="noopener">kubernetes cloud-controller-manager</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2018/10/21/开源项目/" data-id="ckac6j636000sa6u6m172ijc8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">三月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/24/k8s-configmap-volume/">大规模使用ConfigMap卷的负载分析及缓解方案</a>
          </li>
        
          <li>
            <a href="/2020/04/20/build-cloud-native-large-scale-distributed-monitoring-system-3/">打造云原生大型分布式监控系统(三): Thanos 部署与实践</a>
          </li>
        
          <li>
            <a href="/2020/04/06/build-cloud-native-large-scale-distributed-monitoring-system-2/">打造云原生大型分布式监控系统(二): Thanos 架构详解</a>
          </li>
        
          <li>
            <a href="/2020/03/27/build-cloud-native-large-scale-distributed-monitoring-system-1/">打造云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段</a>
          </li>
        
          <li>
            <a href="/2020/01/13/kubernetes-overflow-and-drop/">Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 腾讯云容器团队<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>