<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>腾讯云容器团队</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="container kubernetes tencentcloud">
<meta property="og:type" content="website">
<meta property="og:title" content="腾讯云容器团队">
<meta property="og:url" content="https://TencentCloudContainerTeam.github.io/page/2/index.html">
<meta property="og:site_name" content="腾讯云容器团队">
<meta property="og:locale" content="zh-cn">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="腾讯云容器团队">
  
    <link rel="alternate" href="/atom.xml" title="腾讯云容器团队" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">腾讯云容器团队</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://TencentCloudContainerTeam.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-capture-packets-in-container" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/19/capture-packets-in-container/" class="article-date">
  <time datetime="2019-05-19T05:04:00.000Z" itemprop="datePublished">2019-05-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/19/capture-packets-in-container/">Kubernetes 问题定位技巧：容器内抓包</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imroc.io/" target="_blank" rel="noopener">陈鹏</a></p>
<p>在使用 kubernetes 跑应用的时候，可能会遇到一些网络问题，比较常见的是服务端无响应(超时)或回包内容不正常，如果没找出各种配置上有问题，这时我们需要确认数据包到底有没有最终被路由到容器里，或者报文到达容器的内容和出容器的内容符不符合预期，通过分析报文可以进一步缩小问题范围。那么如何在容器内抓包呢？本文提供实用的脚本一键进入容器网络命名空间(netns)，使用宿主机上的tcpdump进行抓包。</p>
<h2 id="使用脚本一键进入-pod-netns-抓包"><a href="#使用脚本一键进入-pod-netns-抓包" class="headerlink" title="使用脚本一键进入 pod netns 抓包"></a>使用脚本一键进入 pod netns 抓包</h2><ul>
<li><p>发现某个服务不通，最好将其副本数调为1，并找到这个副本 pod 所在节点和 pod 名称</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -o wide</span><br></pre></td></tr></table></figure>
</li>
<li><p>登录 pod 所在节点，将如下脚本粘贴到 shell (注册函数到当前登录的 shell，我们后面用)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">e</span></span>() &#123;</span><br><span class="line">    <span class="built_in">set</span> -eu</span><br><span class="line">    ns=<span class="variable">$&#123;2-"default"&#125;</span></span><br><span class="line">    pod=`kubectl -n <span class="variable">$ns</span> describe pod <span class="variable">$1</span> | grep -Eo <span class="string">'docker://.*$'</span> | head -n 1 | sed <span class="string">'s/docker:\/\/\(.*\)$/\1/'</span>`</span><br><span class="line">    pid=`docker inspect -f &#123;&#123;.State.Pid&#125;&#125; <span class="variable">$pod</span>`</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"enter pod netns successfully for <span class="variable">$ns</span>/<span class="variable">$1</span>"</span></span><br><span class="line">    nsenter -n --target <span class="variable">$pid</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>一键进入 pod 所在的 netns，格式：<code>e POD_NAME NAMESPACE</code>，示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">e istio-galley-58c7c7c646-m6568 istio-system</span><br><span class="line">e proxy-5546768954-9rxg6 <span class="comment"># 省略 NAMESPACE 默认为 default</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>这时已经进入 pod 的 netns，可以执行宿主机上的 <code>ip a</code> 或 <code>ifconfig</code> 来查看容器的网卡，执行 <code>netstat -tunlp</code> 查看当前容器监听了哪些端口，再通过 <code>tcpdump</code> 抓包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 -w test.pcap port 80</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>ctrl-c</code> 停止抓包，再用 <code>scp</code> 或 <code>sz</code> 将抓下来的包下载到本地使用 <code>wireshark</code> 分析，提供一些常用的 <code>wireshark</code> 过滤语法：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 telnet 连上并发送一些测试文本，比如 "lbtest"，</span></span><br><span class="line"><span class="comment"># 用下面语句可以看发送的测试报文有没有到容器</span></span><br><span class="line">tcp contains <span class="string">"lbtest"</span></span><br><span class="line"><span class="comment"># 如果容器提供的是http服务，可以使用 curl 发送一些测试路径的请求，</span></span><br><span class="line"><span class="comment"># 通过下面语句过滤 uri 看报文有没有都容器</span></span><br><span class="line">http.request.uri==<span class="string">"/mytest"</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="脚本原理"><a href="#脚本原理" class="headerlink" title="脚本原理"></a>脚本原理</h3><p>我们解释下步骤二中用到的脚本的原理</p>
<ul>
<li><p>查看指定 pod 运行的容器 ID</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod&gt; -n mservice</span><br></pre></td></tr></table></figure>
</li>
<li><p>获得容器进程的 pid</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect -f &#123;&#123;.State.Pid&#125;&#125; &lt;container&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入该容器的 network namespace</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -n --target &lt;PID&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>依赖宿主机的命名：<code>kubectl</code>, <code>docker</code>, <code>nsenter</code>, <code>grep</code>, <code>head</code>, <code>sed</code></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/05/19/capture-packets-in-container/" data-id="ck89nwpba0003o8kbygh2zq14" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-kubernetes-best-practice-grace-update" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/08/kubernetes-best-practice-grace-update/" class="article-date">
  <time datetime="2019-05-08T12:48:00.000Z" itemprop="datePublished">2019-05-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/08/kubernetes-best-practice-grace-update/">kubernetes 最佳实践：优雅热更新</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imroc.io/" target="_blank" rel="noopener">陈鹏</a></p>
<p>当kubernetes对服务滚动更新的期间，默认配置的情况下可能会让部分连接异常（比如连接被拒绝），我们来分析下原因并给出最佳实践</p>
<h2 id="滚动更新场景"><a href="#滚动更新场景" class="headerlink" title="滚动更新场景"></a>滚动更新场景</h2><p>使用 deployment 部署服务并关联 service</p>
<ul>
<li>修改 deployment 的 replica 调整副本数量来滚动更新</li>
<li>升级程序版本(修改镜像tag)触发 deployment 新建 replicaset 启动新版本的 pod</li>
<li>使用 HPA (HorizontalPodAutoscaler) 来对 deployment 自动扩缩容</li>
</ul>
<h2 id="更新过程连接异常的原因"><a href="#更新过程连接异常的原因" class="headerlink" title="更新过程连接异常的原因"></a>更新过程连接异常的原因</h2><p>滚动更新时，service 对应的 pod 会被创建或销毁，也就是 service 对应的 endpoint 列表会新增或移除endpoint，更新期间可能让部分连接异常，主要原因是：</p>
<ol>
<li>pod 被创建，还没完全启动就被 endpoint controller 加入到 service 的 endpoint 列表，然后 kube-proxy 配置对应的路由规则(iptables/ipvs)，如果请求被路由到还没完全启动完成的 pod，这时 pod 还不能正常处理请求，就会导致连接异常</li>
<li>pod 被销毁，但是从 endpoint controller watch 到变化并更新 service 的 endpoint 列表到 kube-proxy 更新路由规则这期间有个时间差，pod可能已经完全被销毁了，但是路由规则还没来得及更新，造成请求依旧还能被转发到已经销毁的 pod ip，导致连接异常</li>
</ol>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><ul>
<li>针对第一种情况，可以给 pod 里的 container 加 readinessProbe (就绪检查)，这样可以让容器完全启动了才被endpoint controller加进 service 的 endpoint 列表，然后 kube-proxy 再更新路由规则，这时请求被转发到的所有后端 pod 都是正常运行，避免了连接异常</li>
<li>针对第二种情况，可以给 pod 里的 container 加 preStop hook，让 pod 真正销毁前先 sleep 等待一段时间，留点时间给 endpoint controller 和 kube-proxy 清理 endpoint 和路由规则，这段时间 pod 处于 Terminating 状态，在路由规则更新完全之前如果有请求转发到这个被销毁的 pod，请求依然可以被正常处理，因为它还没有被真正销毁</li>
</ul>
<p>最佳实践 yaml 示例:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      component:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        component:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"nginx"</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">          hostPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        readinessProbe:</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/healthz</span></span><br><span class="line"><span class="attr">            port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">            httpHeaders:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">X-Custom-Header</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">Awesome</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">        lifecycle:</span></span><br><span class="line"><span class="attr">          preStop:</span></span><br><span class="line"><span class="attr">            exec:</span></span><br><span class="line"><span class="attr">              command:</span> <span class="string">["/bin/bash",</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 30"</span><span class="string">]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>Container probes: <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes</a></li>
<li>Container Lifecycle Hooks: <a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/05/08/kubernetes-best-practice-grace-update/" data-id="ck89nwpbt000eo8kbzzw4nxrc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-kubernetes-vpa" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/30/kubernetes-vpa/" class="article-date">
  <time datetime="2019-04-30T08:00:00.000Z" itemprop="datePublished">2019-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/30/kubernetes-vpa/">如何使用 Kubernetes VPA 实现资源动态扩展和回收</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://github.com/xiaoxubeii" target="_blank" rel="noopener">徐蓓</a></p>
<h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>最近一段时间在研究和设计集群资源混合部署方案，以提高资源使用率。这其中一个重要的功能是资源动态扩展和回收。虽然方案是针对通用型集群管理软件，但由于 Kubernetes 目前是事实标准，所以先使用它来检验理论成果。</p>
<h2 id="资源动态扩展"><a href="#资源动态扩展" class="headerlink" title="资源动态扩展"></a>资源动态扩展</h2><p>资源动态扩展按照类型分为两种：纵向和横向。纵向指的是对资源的配置进行扩展，比如增加或减少 CPU 个数和内存大小等。横向扩展则是增加资源的数量，比如服务器个数。笔者研究方案的目的是为了提升集群资源使用率，所以这里单讨论资源纵向扩展。</p>
<p>不过坦白来讲，资源纵向扩展首要目标并不是为了提高集群利用率，而是为了优化集群资源、提高资源可用性和性能。</p>
<p>在 Kubernetes 中 <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler" target="_blank" rel="noopener">VPA</a> 项目主要是完成这项工作（主要针对 Pod）。</p>
<h3 id="Kubernetes-VPA"><a href="#Kubernetes-VPA" class="headerlink" title="Kubernetes VPA"></a>Kubernetes VPA</h3><blockquote>
<p>Vertical Pod Autoscaler (VPA) frees the users from necessity of setting up-to-date resource requests for the containers in their pods. When configured, it will set the requests automatically based on usage and thus allow proper scheduling onto nodes so that appropriate resource amount is available for each pod.</p>
</blockquote>
<p>以上是官方定义。简单来说是 Kubernetes VPA 可以根据实际负载动态设置 pod resource requests。</p>
<p>Kubernetes VPA 包含以下组件：</p>
<ul>
<li>Recommender：用于根据监控指标结合内置机制给出资源建议值</li>
<li>Updater：用于实时更新 pod resource requests</li>
<li>History Storage：用于采集和存储监控数据</li>
<li>Admission Controller: 用于在 pod 创建时修改 resource requests</li>
</ul>
<p>以下是架构图：</p>
<p><img src="/images/15550662807076.jpg" alt=""></p>
<p>主要流程是：<code>Recommender</code>在启动时从<code>History Storage</code>获取历史数据，根据内置机制修改<code>VPA API object</code>资源建议值。<code>Updater</code>监听<code>VPA API object</code>，依据建议值动态修改 pod resource requests。<code>VPA Admission Controller</code>则是用于 pod 创建时修改 pod resource requests。<code>History Storage</code>则是通过<code>Kubernetes Metrics API</code>采集和存储监控数据。</p>
<p>Kubernetes VPA 的整体架构比较简单，流程也很清晰，理解起来并不困难。但里面隐藏的几个功能点，却是方案的核心所在。它们的质量直接影响了方案的成熟度和评价效果：</p>
<p>1、如何设计 Recommendation model<br><code>Recommendation model</code>是集群优化的重中之重，它的好坏直接影响了集群资源优化的效果。就笔者目前了解，在 Kubernetes VPA 中这个模型是固定的，用户能做的是配置参数和数据源。</p>
<p>从官方描述看：</p>
<blockquote>
<p>The request is calculated based on analysis of the current and previous runs of the container and other containers with similar properties (name, image, command, args). The recommendation model (MVP) assumes that the memory and CPU consumption are independent random variables with distribution equal to the one observed in the last N days (recommended value is N=8 to capture weekly peaks). A more advanced model in future could attempt to detect trends, periodicity and other time-related patterns.</p>
</blockquote>
<p>CPU 和内存的建议值均是依据<strong>历史数据+固定机制</strong>计算而成，并没有一套解释引擎能让用户自定义规则。这在一定程度上影响了<code>Recommendation model</code>的准确性。就笔者理解，集群优化和混合部署的核心难点在于寻找能准确描述集群负载的指标，建立指标模型，并最终通过优化模型而达到最终目的 - 不论是为了优化集群或提高集群使用率。这个过程类似机器学习：先依旧经验或特征工程寻找特征变量，建立模型后使用数据不断优化参数，最后得到可用模型。所以仅靠单一指标 - 比如 CPU 或内存使用率 - 所建立的固定模型并不能准确描述集群状态和资源瓶颈。不管是从指标的颗粒度或固定模型上来看，最终效果都不会太好。</p>
<p>2、Pod 是否支持热更新<br>在 Kubernetes 中，pod resource requests 会影响 pod QoS 和容器的限制状态，比如驱逐策略、<code>OOM Score</code>和 cgroup 的限制参数等。如果不重建的话，单纯的修改 pod spec 只会影响调度策略。重建的话会导致 pod 重新调度，同时也在一定程度上降低了应用的可用性。官网列出一个更新策略<code>auto</code>，是可以<code>in-place</code>重建：</p>
<blockquote>
<p>“Auto”: VPA assigns resource requests on pod creation as well as updates them on existing pods using the preferred update mechanism. Currently this is equivalent to “Recreate” (see below). Once restart free (“in-place”) update of pod requests is available, it may be used as the preferred update mechanism by the “Auto” mode. NOTE: This feature of VPA is experimental and may cause downtime for your applications.</p>
</blockquote>
<p>目前应该没有完全实现。不过无论哪种方式，pod 重建貌似不可避免。</p>
<p>3、Pod 实时更新是否支持模糊控制<br>由于 Pod 更新会涉及重建，那么实时更新的触发条件就不应依据一个固定的值，比如值的变化触发更新重建（显然不可取）、依据逻辑表达式触发更新重建（也不可取，极端情况下会在设定值上下不断触发）。此时就需要在离散的值之间加入缓冲范围。而这个范围的设置高度依赖经验和实际集群情况，不然的话又会影响方案的最终效果。</p>
<p>总的来说，Kubernetes VPA 解决了资源纵向扩展的大部分工程问题。若应用于生产，还需做很多的个性化工作。</p>
<h2 id="资源回收"><a href="#资源回收" class="headerlink" title="资源回收"></a>资源回收</h2><p>既然 Kubernetes VPA 主要目标不是提升资源使用率，那它和混合部署又有何关系？别急，我们先来回顾下集群混合部署中提升资源使用率的关键是什么。</p>
<p>提升资源使用率最直观的方式，是在保证服务可用性的前提下尽量多的分配集群资源。我们知道在一般的集群管理软件中，调度器会为应用分配集群的可用资源。分配给应用的是逻辑资源，无需要和物理资源一一对应，比如可以超卖。并且应用持有的资源，一般情况下也不会全时段占用。在这种情况下，可将分配资源分为闲时和忙时。应用按照优先级区分，为高优先级的应用分配较多的资源。动态回收高优先级应用的闲时资源分配给低优先级应用使用，在高优先级应用负载升高时驱逐低优先级应用，从而达到提升资源使用率的目的。</p>
<p>在 Kubernetes VPA 中缺少资源回收的机制，但<code>Recommender</code>却可以配合<code>Updater</code>动态修改 pod resource requests 的值。也就是说 <strong>pod resource requests - 推荐值 = 资源回收值</strong>。这间接实现了资源回收的功能。那么 Kubernetes 调度器就可将这部分资源分配给其他应用使用。当然实际方案不会这么简单。比如<code>Recommender</code>就不需要使用<code>History Storage</code>中的历史数据和计算规则。初始值设为 pod resource requests，实时获取监控数据，加个 buffer 即可。这可以算是 Kubernetes 简陋版的资源回收功能。至于回收后，资源再分配和资源峰值驱逐等又是另一套流程了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>暂时还是打算基于 Kubernetes VPA 实现资源回收和混合部署功能，毕竟现成的轮子。至于集群负载指标和模型，就完全是一套经验工程了。只能在实际生产中慢慢积累，别无他法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/04/30/kubernetes-vpa/" data-id="ck89nwpbw000ho8kbrrufufrs" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-google-borg" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/17/google-borg/" class="article-date">
  <time datetime="2019-04-17T07:00:00.000Z" itemprop="datePublished">2019-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/17/google-borg/">Google Borg 浅析</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://github.com/xiaoxubeii" target="_blank" rel="noopener">徐蓓</a></p>
<h1 id="Google-Borg-浅析"><a href="#Google-Borg-浅析" class="headerlink" title="Google Borg 浅析"></a>Google Borg 浅析</h1><p>笔者的工作主要涉及集群资源调度和混合部署，对相关技术和论文有所研究，包括 Google Borg、Kubernetes、Firmament 和 Kubernetes Poseidon 等。尤其是这篇《Large-scale cluster management at Google with Borg》令笔者受益匪浅。下面本人就结合生产场景，尝试对 Google Borg 做些分析和延展。</p>
<h2 id="Google-Borg-简介"><a href="#Google-Borg-简介" class="headerlink" title="Google Borg 简介"></a>Google Borg 简介</h2><p>Google Borg 是一套资源管理系统，可用于管理和调度资源。在 Borg 中，资源的单位是 <strong>Job</strong> 和 <strong>Task</strong>。<strong>Job</strong> 包含一组 <strong>Task</strong>。<strong>Task</strong> 是 Borg 管理和调度的最小单元，它对应一组 Linux 进程。熟悉 Kubernetes 的读者，可以将 <strong>Job</strong> 和 <strong>Task</strong> 大致对应为 Kubernetes 的 <strong>Service</strong> 和 <strong>Pod</strong>。</p>
<p>在架构上，Borg 和 Kubernetes 类似，由 BorgMaster、Scheduler 和 Borglet 组成。</p>
<p><img src="/images/15414871166556.jpg" alt=""></p>
<h2 id="Allocs"><a href="#Allocs" class="headerlink" title="Allocs"></a>Allocs</h2><p>Borg Alloc 代表一组可用于运行 Task 的资源，如 CPU、内存、IO 和磁盘空间。它实际上是集群对物理资源的抽象。Alloc set 类似 Job，是一堆 Alloc 的集合。当一个 Alloc set 被创建时，一个或多个 Job 就可以运行在上面了。</p>
<h2 id="Priority-和-Quota"><a href="#Priority-和-Quota" class="headerlink" title="Priority 和 Quota"></a>Priority 和 Quota</h2><p>每个 Job 都可以设置 Priority。Priority 可用于标识 Job 的重要程度，并影响一些资源分配、调度和 Preemption 策略。比如在生产中，我们会将作业分为 Routine Job 和 Batch Job。Routine Job 为生产级的例行作业，优先级最高，它占用对应实际物理资源的 Alloc set。Batch Job 代表一些临时作业，优先级最低。当资源紧张时，集群会优先 Preempt Batch Job，将资源提供给 Routine Job 使用。这时 Preempted Batch Job 会回到调度队列等待重新调度。</p>
<p>Quota 代表资源配额，它约束 Job 的可用资源，比如 CPU、内存或磁盘。Quota 一般在调度之前进行检查。Job 若不满足，会立即在提交时被拒绝。生产中，我们一般依据实际物理资源配置 Routine Job Quota。这种方式可以确保 Routine Job 在 Quota 内一定有可用的资源。为了充分提升集群资源使用率，我们会将 Batch Job Quota 设置为无限，让它尽量去占用 Routine Job 的闲置资源，从而实现超卖。这方面内容后面会在再次详述。</p>
<h2 id="Schedule"><a href="#Schedule" class="headerlink" title="Schedule"></a>Schedule</h2><p>调度是资源管理系统的核心功能，它直接决定了系统的“好坏”。在 Borg 中，Job 被提交后，Borgmaster 会将其放入一个 Pending Queue。Scheduler 异步地扫描队列，将 Task 调度到有充足资源的机器上。通常情况下，调度过程分为两个步骤：Filter 和 Score。Filter，或是 Feasibility Checking，用于判断机器是否满足 Task 的约束和限制，比如 Schedule Preference、Affinity 或 Resource Limit。Filter 结束后，就需要 Score 符合要求的机器，或称为 Weight。上述两个步骤完成后，Scheduler 就会挑选相应数量的机器调度给 Task 运行。实际上，选择合适的调度策略尤为重要。</p>
<p>这里可以拿一个生产集群举例。在初期，我们的调度系统采用的 Score 策略类似 Borg E-PVM，它的作用是将 Task 尽量均匀的调度到整个集群上。从正面效果上讲，这种策略分散了 Task 负载，并在一定程度上缩小了故障域。但从反面看，它也引发了资源碎片化的问题。由于我们底层环境是异构的，机器配置并不统一，并且 Task 配置和物理配置并无对应关系。这就造成一些配置过大的 Task 无法运行，由此在一定程度上降低了资源的分配率和使用率。为了应付此类问题，我们自研了新的 Score 策略，称之为 “Best Fillup”。它的原理是在调度 Task 时选择可用资源最少的机器，也就是尽量填满。不过这种策略的缺点显而易见：单台机器的负载会升高，从而增加 Bursty Load 的风险；不利于 Batch Job 运行；故障域会增加。</p>
<p>这篇论文，作者采用了一种被称为 hybrid 的方式，据说比第一种策略增加 3-5% 的效率。具体实现方式还有待后续研究。</p>
<h2 id="Utilization"><a href="#Utilization" class="headerlink" title="Utilization"></a>Utilization</h2><p>资源管理系统的首要目标是提高资源使用率，Borg 亦是如此。不过由于过多的前置条件，诸如 Job 放置约束、负载尖峰、多样的机器配置和 Batch Job，导致不能仅选择 “average utilization” 作为策略指标。在 Borg 中，使用 <strong>Cell Compaction</strong> 作为评判基准。简述之就是：能承载给定负载的最小 Cell。</p>
<p>Borg 提供了一些提高 utilization 的思路和实践方法，有些是我们在生产中已经采用的，有些则非常值得我们学习和借鉴。</p>
<h3 id="Cell-Sharing"><a href="#Cell-Sharing" class="headerlink" title="Cell Sharing"></a>Cell Sharing</h3><p>Borg 发现，将各种优先级的 Task，比如 prod 和 non-prod 运行在共享的 Cell 中可以大幅度的提升资源利用率。</p>
<p><img src="/images/15414743848812.jpg" alt=""></p>
<p>上面（a）图表明，采用 Task 隔离的部署方式会增加对机器的需求。图（b）是对额外机器需求的分布函数。图（a）和图（b）都清楚的表明了将 prod job 和 non-prod job 分开部署会消耗更多的物理资源。Borg 的经验是大约会新增 20-30% 左右。</p>
<p>个中原理也很好理解：prod job 通常会为应对负载尖峰申请较大资源，实际上这部分资源在多数时间里是闲置的。Borg 会定时回收这部分资源，并将之分配给 non-prod job 使用。在 Kubernetes 中，对应的概念是 request limit 和 limit。我们在生产中，一般设置 Prod job 的 Request limit 等于 limit，这样它就具有了最高的 Guaranteed Qos。该 QoS 使得 pod 在机器负载高时不至于被驱逐和 OOM。non-prod job 则不设置 request limit 和 limit，这使得它具有 BestEffort 级别的 QoS。kubelet 会在资源负载高时优先驱逐此类 Pod。这样也达到了和 Borg 类似的效果。</p>
<h3 id="Large-cells"><a href="#Large-cells" class="headerlink" title="Large cells"></a>Large cells</h3><p>Borg 通过实验数据表明，小容量的 cell 通常比大容量的更占用物理资源。<br><img src="/images/15414759002584.jpg" alt=""></p>
<p>这点对我们有和很重要的指导意义。通常情况下，我们会在设计集群时对容量问题感到犹豫不决。显而易见，小集群可以带来更高的隔离性、更小的故障域以及潜在风险。但随之带来的则是管理和架构复杂度的增加，以及更多的故障点。大集群的优缺点正好相反。在资源利用率这个指标上，我们凭直觉认为是大集群更优，但苦于无坚实的理论依据。Borg 的研究表明，大集群有利于增加资源利用率，这点对我们的决策很有帮助。</p>
<h3 id="Fine-grained-resource-requests"><a href="#Fine-grained-resource-requests" class="headerlink" title="Fine-grained resource requests"></a>Fine-grained resource requests</h3><p>Borg 对资源细粒度分配的方法，目前已是主流，在此我就不再赘述。</p>
<h3 id="Resource-reclamation"><a href="#Resource-reclamation" class="headerlink" title="Resource reclamation"></a>Resource reclamation</h3><p>笔者感觉这部分内容帮助最大。熟悉 Kubernetes 的读者，应该对类似的概念很熟悉，也就是所谓的 request limit。job 在提交时需要指定 resource limit，它能确保内部的 task 有足够资源可以运行。有些用户会为 task 申请过大的资源，以应对可能的请求或计算的突增。但实际上，部分资源在多数时间内是闲置的。与其资源浪费，不如利用起来。这需要系统有较精确的预测机制，可以评估 task 对实际资源的需求，并将闲置资源回收以分配给低 priority 的任务，比如 batch job。上述过程在 Borg 中被称为 <strong>resource reclamation</strong>，对使用资源的评估则被称为 <strong>reservation</strong>。Borgmaster 会定期从 Borglet 收集 resource consumption，并执行 <strong>reservation</strong>。在初始阶段，reservation 等于 resource limit。随着 task 的运行，reservation 就变为了资源的实际使用量，外加 safety margin。</p>
<p>在 Borg 调度时，Scheduler 使用 resource limit 为 prod task 过滤和选择主机，这个过程并不依赖 reclaimed resource。从这个角度看，并不支持对 prod task 的资源超卖。但 non-prod task 则不同，它是占用已有 task 的 resource reservation。所以 non-prod task 会被调度到拥有 reclaimed resource 的机器上。</p>
<p>这种做法当然也是有一定风险的。若资源评估出现偏差，机器上的可用资源可能会被耗尽。在这种情况下，Borg 会杀死或者降级 non-prod task，prod task 则不会受到半分任何影响。</p>
<p><img src="/images/15414862899318.jpg" alt=""></p>
<p>上图证实了这种策略的有效性。参照 Week 1 和 4 的 baseline，Week 2 和 3 在调整了 estimation algorithm 后，实际资源的 usage 与 reservation 的 gap 在显著缩小。在 Borg 的一个 median cell 中，有 20% 的负载是运行在 reclaimed resource 上。</p>
<p>相较于 Borg，Kubernetes 虽然有 resource limit 和 capacity 的概念，但却缺少动态 reclaim 机制。这会使得系统对低 priority task 的资源缺少行之有效的评估机制，从而引发系统负载问题。个人感觉这个功能对资源调度和提升资源使用率影响巨大，这部分内容也是笔者的工作重心</p>
<h2 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h2><p>这部分内容虽十分重要，但对于我们的生产集群优先级不是很高，在此先略过。有兴趣的读者可以自行研究。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="">Large-scale cluster management at Google with Borg</a></li>
<li><a href="http://www.firmament.io/blog/scheduler-architectures.html" target="_blank" rel="noopener">The evolution of cluster scheduler architectures</a></li>
<li><a href="https://github.com/kubernetes-sigs/poseidon" target="_blank" rel="noopener">poseidon</a></li>
<li><a href="https://docs.google.com/document/d/1VNoaw1GoRK-yop_Oqzn7wZhxMxvN3pdNjuaICjXLarA/edit?usp=sharing" target="_blank" rel="noopener">Poseidon design</a></li>
<li><a href="https://github.com/camsas/firmament" target="_blank" rel="noopener">firemament</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/04/17/google-borg/" data-id="ck89nwpbe0005o8kby4jyhadc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-istio-cni" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/07/istio-cni/" class="article-date">
  <time datetime="2019-04-07T04:20:00.000Z" itemprop="datePublished">2019-04-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/07/istio-cni/">Istio 学习笔记：Istio CNI 插件</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imroc.io/" target="_blank" rel="noopener">陈鹏</a></p>
<h2 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h2><p>当前实现将用户 pod 流量转发到 proxy 的默认方式是使用 privileged 权限的 istio-init 这个 init container 来做的（运行脚本写入 iptables），Istio CNI 插件的主要设计目标是消除这个 privileged 权限的 init container，换成利用 k8s CNI 机制来实现相同功能的替代方案</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul>
<li>Istio CNI Plugin 不是 istio 提出类似 k8s CNI 的插件扩展机制，而是 k8s CNI 的一个具体实现</li>
<li>k8s CNI 插件是一条链，在创建和销毁pod的时候会调用链上所有插件来安装和卸载容器的网络，istio CNI Plugin 即为 CNI 插件的一个实现，相当于在创建销毁pod这些hook点来针对istio的pod做网络配置：写入iptables，让该 pod 所在的 network namespace 的网络流量转发到 proxy 进程</li>
<li>当然也就要求集群启用 CNI，kubelet 启动参数: <code>--network-plugin=cni</code> （该参数只有两个可选项：<code>kubenet</code>, <code>cni</code>）</li>
</ul>
<h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><ul>
<li>运行一个名为 istio-cni-node 的 daemonset 运行在每个节点，用于安装 istio CNI 插件</li>
<li>该 CNI 插件负责写入 iptables 规则，让用户 pod 所在 netns 的流量都转发到这个 pod 中 proxy 的进程</li>
<li>当启用 istio cni 后，sidecar 的自动注入或<code>istioctl kube-inject</code>将不再注入 initContainers (istio-init)</li>
</ul>
<h2 id="istio-cni-node-工作流程"><a href="#istio-cni-node-工作流程" class="headerlink" title="istio-cni-node 工作流程"></a>istio-cni-node 工作流程</h2><ul>
<li>复制 Istio CNI 插件二进制程序到CNI的bin目录（即kubelet启动参数<code>--cni-bin-dir</code>指定的路径，默认是<code>/opt/cni/bin</code>）</li>
<li>使用istio-cni-node自己的ServiceAccount信息为CNI插件生成kubeconfig，让插件能与apiserver通信(ServiceAccount信息会被自动挂载到<code>/var/run/secrets/kubernetes.io/serviceaccount</code>)</li>
<li>生成CNI插件的配置并将其插入CNI配置插件链末尾（CNI的配置文件路径是kubelet启动参数<code>--cni-conf-dir</code>所指定的目录，默认是<code>/etc/cni/net.d</code>）</li>
<li>watch CNI 配置(<code>cni-conf-dir</code>)，如果检测到被修改就重新改回来</li>
<li>watch istio-cni-node 自身的配置(configmap)，检测到有修改就重新执行CNI配置生成与下发流程（当前写这篇文章的时候是istio 1.1.1，还没实现此功能）</li>
</ul>
<h2 id="设计提案"><a href="#设计提案" class="headerlink" title="设计提案"></a>设计提案</h2><ul>
<li>Istio CNI Plugin 提案创建时间：2018-09-28</li>
<li>Istio CNI Plugin 提案文档存放在：Istio 的 Google Team Drive<ul>
<li>Istio TeamDrive 地址：<a href="https://drive.google.com/corp/drive/u/0/folders/0AIS5p3eW9BCtUk9PVA" target="_blank" rel="noopener">https://drive.google.com/corp/drive/u/0/folders/0AIS5p3eW9BCtUk9PVA</a></li>
<li>Istio CNI Plugin 提案文档路径：<code>Working Groups/Networking/Istio CNI Plugin</code></li>
<li>查看文件需要申请权限，申请方法：加入istio-team-drive-access这个google网上论坛group</li>
<li>istio-team-drive-access group 地址: <a href="https://groups.google.com/forum/#!forum/istio-team-drive-access" target="_blank" rel="noopener">https://groups.google.com/forum/#!forum/istio-team-drive-access</a></li>
</ul>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>Install Istio with the Istio CNI plugin: <a href="https://istio.io/docs/setup/kubernetes/additional-setup/cni/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/additional-setup/cni/</a></li>
<li>istio-cni 项目地址：<a href="https://github.com/istio/cni" target="_blank" rel="noopener">https://github.com/istio/cni</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/04/07/istio-cni/" data-id="ck89nwpbk000ao8kbzfkjch91" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-istio-analysis-3" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/01/istio-analysis-3/" class="article-date">
  <time datetime="2019-04-01T07:30:00.000Z" itemprop="datePublished">2019-04-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/04/01/istio-analysis-3/">istio 庖丁解牛(三) galley</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imfox.io/" target="_blank" rel="noopener">钟华</a></p>
<p>今天我们来解析istio控制面组件Galley. Galley Pod是一个单容器单进程组件, 没有sidecar, 结构独立, 职责明确.</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g1maoldl74j31850u049x.jpg" referrerpolicy="no-referrer"></p>
<p><a href="https://ws4.sinaimg.cn/large/006tKfTcgy1g187dn7s1tj315m0u0x6t.jpg" target="_blank" referrerpolicy="no-referrer">查看高清原图</a></p>
<p>前不久istio 1.1 版本正式发布, 其中istio的配置管理机制有较大的改进, 以下是<a href="https://istio.io/about/notes/1.1/" target="_blank" rel="noopener">1.1 release note</a> 中部分说明:</p>
<blockquote>
<p>Added <a href="https://istio.io/docs/concepts/what-is-istio/#galley" target="_blank" rel="noopener">Galley</a> as the primary configuration ingestion and distribution mechanism within Istio. It provides a robust model to validate, transform, and distribute configuration states to Istio components insulating the Istio components from Kubernetes details. Galley uses the <a href="https://github.com/istio/api/tree/release-1.1/mcp" target="_blank" rel="noopener">Mesh Configuration Protocol (MCP)</a> to interact with components</p>
</blockquote>
<p>Galley 原来仅负责进行配置验证, 1.1 后升级为整个控制面的配置管理中心, 除了继续提供配置验证功能外, Galley还负责配置的管理和分发, Galley 使用 <strong>网格配置协议</strong>(Mesh Configuration Protocol) 和其他组件进行配置的交互.</p>
<p>今天对Galley的剖析大概有以下方面:</p>
<ul>
<li>Galley 演进的背景</li>
<li>Galley 配置验证功能</li>
<li>MCP 协议</li>
<li>Galley 配置管理实现浅析</li>
</ul>
<hr>
<h2 id="Galley-演进的背景"><a href="#Galley-演进的背景" class="headerlink" title="Galley 演进的背景"></a>Galley 演进的背景</h2><p>在 k8s 场景下, 「配置(Configuration)」一词主要指yaml编写的Resource Definition, 如service、pod, 以及扩展的CRD( Custom Resource Definition), 如 istio的 VirtualService、DestinationRule 等.</p>
<p><strong>本文中「配置」一词可以等同于 k8s Resource Definition + istio CRD</strong></p>
<p>声明式 API 是 Kubernetes 项目编排能力“赖以生存”的核心所在, 而「配置」是声明式 API的承载方式.</p>
<blockquote>
<p>Istio 项目的设计与实现，其实都依托于 Kubernetes 的声明式 API 和它所提供的各种编排能力。可以说，Istio 是在 Kubernetes 项目使用上的一位“集大成者”</p>
<p>Istio 项目有多火热，就说明 Kubernetes 这套“声明式 API”有多成功</p>
</blockquote>
<p>k8s 内置了几十个Resources, istio 创造了50多个CRD, 其复杂度可见一斑, 所以有人说面向k8s编程近似于面向yaml编程.</p>
<p>早期的Galley 仅仅负责对「配置」进行运行时验证, istio 控制面各个组件各自去list/watch 各自关注的「配置」, 以下是istio早期的Configuration flow:</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g1mbphtde5j31d20swae2.jpg" referrerpolicy="no-referrer"></p>
<p>越来越多且复杂的「配置」给istio 用户带来了诸多不便, 主要体现在:</p>
<ul>
<li>「配置」的缺乏统一管理, 组件各自订阅, 缺乏统一回滚机制, 配置问题难以定位</li>
<li>「配置」可复用度低, 比如在1.1之前, 每个mixer adpater 就需要定义个新的CRD.</li>
<li>另外「配置」的隔离, ACL 控制, 一致性, 抽象程度, 序列化等等问题都还不太令人满意.</li>
</ul>
<p>随着istio功能的演进, 可预见的istio CRD数量还会继续增加, 社区计划将Galley 强化为istio 「配置」控制层, Galley 除了继续提供「配置」验证功能外, 还将提供配置管理流水线, 包括输入, 转换, 分发, 以及适合istio控制面的「配置」分发协议(MCP).</p>
<p>本文对Galley的分析基于istio tag 1.1.1 (commit 2b13318)</p>
<hr>
<h2 id="Galley-配置验证功能"><a href="#Galley-配置验证功能" class="headerlink" title="Galley 配置验证功能"></a>Galley 配置验证功能</h2><p>在<a href="https://imfox.io/2019/03/19/istio-analysis-2/" target="_blank" rel="noopener">istio 庖丁解牛(二) sidecar injector</a>中我分析了istio-sidecar-injector 如何利用 MutatingWebhook 来实现sidecar注入, Galley 使用了k8s提供的另一个Admission Webhooks: ValidatingWebhook, 来做配置的验证:</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g1mcwsf5ggj30sz0ecjt4.jpg" referrerpolicy="no-referrer"></p>
<p>istio 需要一个关于ValidatingWebhook的配置项, 用于告诉k8s api server, 哪些CRD应该发往哪个服务的哪个接口去做验证, 该配置名为istio-galley, 简化的内容如下:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">%kubectl</span> <span class="string">get</span> <span class="string">ValidatingWebhookConfiguration</span> <span class="string">istio-galley</span> <span class="bullet">-oyaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">admissionregistration.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ValidatingWebhookConfiguration</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">istio-galley</span></span><br><span class="line"><span class="attr">webhooks:</span></span><br><span class="line"><span class="attr">- clientConfig:</span></span><br><span class="line">  <span class="string">......</span></span><br><span class="line"><span class="attr">    service:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">istio-galley</span></span><br><span class="line"><span class="attr">      namespace:</span> <span class="string">istio-system</span></span><br><span class="line"><span class="attr">      path:</span> <span class="string">/admitpilot</span></span><br><span class="line"><span class="attr">  failurePolicy:</span> <span class="string">Fail</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pilot.validation.istio.io</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line">  <span class="string">...pilot关注的CRD...</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">gateways</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">virtualservices</span></span><br><span class="line">  <span class="string">......</span></span><br><span class="line"><span class="attr">- clientConfig:</span></span><br><span class="line">  <span class="string">......</span></span><br><span class="line"><span class="attr">    service:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">istio-galley</span></span><br><span class="line"><span class="attr">      namespace:</span> <span class="string">istio-system</span></span><br><span class="line"><span class="attr">      path:</span> <span class="string">/admitmixer</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">mixer.validation.istio.io</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line">  <span class="string">...mixer关注的CRD...</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">rules</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">metrics</span></span><br><span class="line">  <span class="string">......</span></span><br></pre></td></tr></table></figure>
<p>可以看到, 该配置将pilot和mixer关注的CRD, 分别发到了服务istio-galley的<code>/admitpilot</code>和<code>/admitmixer</code>, 在Galley 源码中可以很容易找到这2个path Handler的入口:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h.HandleFunc(<span class="string">"/admitpilot"</span>, wh.serveAdmitPilot)</span><br><span class="line">h.HandleFunc(<span class="string">"/admitmixer"</span>, wh.serveAdmitMixer)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="MCP协议"><a href="#MCP协议" class="headerlink" title="MCP协议"></a>MCP协议</h2><p>MCP 提供了一套配置订阅和分发的API, 在MCP中, 可以抽象为以下模型:</p>
<ul>
<li>source: 「配置」的提供端, 在Istio中Galley 即是source</li>
<li>sink: 「配置」的消费端, 在isito中典型的sink包括Pilot和Mixer组件</li>
<li>resource: source和sink关注的资源体, 也就是isito中的「配置」</li>
</ul>
<p>当sink和source之间建立了对某些resource的订阅和分发关系后, source 会将指定resource的变化信息推送给sink, sink端可以选择接受或者不接受resource更新(比如格式错误的情况), 并对应返回ACK/NACK 给source端.</p>
<p>MCP 提供了gRPC 的实现, 实现代码参见: <a href="https://github.com/istio/api/tree/master/mcp/v1alpha1" target="_blank" rel="noopener">https://github.com/istio/api/tree/master/mcp/v1alpha1</a>,</p>
<p>其中包括2个services: <code>ResourceSource</code> 和 <code>ResourceSink</code>,  通常情况下,  source 会作为 gRPC的server 端, 提供<code>ResourceSource</code>服务,  sink 作为 gRPC的客户端, sink主动发起请求连接source; 不过有的场景下, source 会作为gRPC的client端, sink作为gRPC的server端提供<code>ResourceSink</code>服务, source主动发起请求连接sink.</p>
<p>以上2个服务, 内部功能逻辑都是一致的, 都是sink需要订阅source管理的resource, 区别仅仅是哪端主动发起的连接请求.</p>
<p>具体到istio的场景中:</p>
<ul>
<li>在单k8s集群的istio mesh中, Galley默认实现了<code>ResourceSource</code> service, Pilot和Mixer会作为该service的client主动连接Galley进行配置订阅.</li>
<li>Galley 可以配置去主动连接远程的其他sink, 比如说在多k8s集群的mesh中, 主集群中的Galley可以为多个集群的Pilot/Mixer提供配置管理, 跨集群的Pilot/Mixer无法主动连接主集群Galley, 这时候Galley就可以作为gRPC的client 主动发起连接, 跨集群的Pilot/Mixer作为gRPC server 实现<code>ResourceSink</code>服务,</li>
</ul>
<p>两种模式的示意图如下:</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g1n7omb7vrj30uk0u0452.jpg" referrerpolicy="no-referrer"></p>
<hr>
<h2 id="Galley-配置管理实现浅析"><a href="#Galley-配置管理实现浅析" class="headerlink" title="Galley 配置管理实现浅析"></a>Galley 配置管理实现浅析</h2><p>galley 进程对外暴露了若干服务, 最重要的就是基于gRPC的mcp服务, 以及http的验证服务, 除此之外还提供了 prometheus exporter接口以及Profiling接口:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> serverArgs.EnableServer &#123; <span class="comment">// 配置管理服务</span></span><br><span class="line">	<span class="keyword">go</span> server.RunServer(serverArgs, livenessProbeController, readinessProbeController)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> validationArgs.EnableValidation &#123; <span class="comment">// 验证服务</span></span><br><span class="line">	<span class="keyword">go</span> validation.RunValidation(validationArgs, kubeConfig, livenessProbeController, readinessProbeController)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 提供 prometheus exporter</span></span><br><span class="line"><span class="keyword">go</span> server.StartSelfMonitoring(galleyStop, monitoringPort)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> enableProfiling &#123;</span><br><span class="line">    <span class="comment">// 使用包net/http/pprof</span></span><br><span class="line">    <span class="comment">// 通过http server提供runtime profiling数据</span></span><br><span class="line">	<span class="keyword">go</span> server.StartProfiling(galleyStop, pprofPort)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 开始探针更新</span></span><br><span class="line"><span class="keyword">go</span> server.StartProbeCheck(livenessProbeController, readinessProbeController, galleyStop)</span><br></pre></td></tr></table></figure>
<p>接下来主要分析下「配置」管理服务的实现:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go server.RunServer(serverArgs, livenessProbeController, readinessProbeController)</span><br></pre></td></tr></table></figure>
<p>下面是Galley 配置服务结构示意图:</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g1mzi3oe9xj31r10u0qgp.jpg" referrerpolicy="no-referrer"></p>
<p><a href="https://ws2.sinaimg.cn/large/006tKfTcgy1g1n8o76s8yj31r10u0trx.jpg" target="_blank" referrerpolicy="no-referrer">查看高清原图</a></p>
<p>从上图可以看到, Galley 配置服务主要包括 Processor 和 负责mcp通信的grpc Server.</p>
<p>其中 Processor 又由以下部分组成:</p>
<ul>
<li>Source:  代表Galley管理的配置的来源</li>
<li>Handler:  对「配置」事件的处理器</li>
<li>State: Galley管理的「配置」在内存中状态</li>
</ul>
<hr>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>interface Source 代表istio关注的配置的来源,  其<code>Start</code>方法需要实现对特定资源的变化监听.</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Source to be implemented by a source configuration provider.</span></span><br><span class="line"><span class="keyword">type</span> Source <span class="keyword">interface</span> &#123;</span><br><span class="line">	<span class="comment">// Start the source interface, provided the EventHandler. The initial state of the underlying</span></span><br><span class="line">	<span class="comment">// config store should be reflected as a series of Added events, followed by a FullSync event.</span></span><br><span class="line">	Start(handler resource.EventHandler) error</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Stop the source interface. Upon return from this method, the channel should not be accumulating any</span></span><br><span class="line">	<span class="comment">// more events.</span></span><br><span class="line">	Stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在Galley中, 有多个Source的实现, 主要包括</p>
<ul>
<li><p><code>source/fs.source</code></p>
</li>
<li><p><code>source/kube/builtin.source</code></p>
</li>
<li><code>source/kube/dynamic.source</code></li>
<li><code>source/kube.aggregate</code></li>
</ul>
<p>其中<code>source/fs</code>代表从文件系统中获取配置, 这种形式常用于开发和测试过程中, 不需要创建实际的k8s CRD, 只需要CRD文件即可, 同时<code>source/fs</code>也是实现了更新watch(使用<a href="https://github.com/howeyc/fsnotify" target="_blank" rel="noopener">https://github.com/howeyc/fsnotify</a>)</p>
<p><code>source/kube/builtin.source</code>处理k8s 内置的配置来源, 包括<code>Service</code>, <code>Node</code>, <code>Pod</code>, <code>Endpoints</code>等, <code>source/kube/dynamic.source</code>处理其他的istio 关注的CRD, <code>source/kube.aggregate</code>是多个Source 的聚合, 其本身也实现了Source interface:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> aggregate <span class="keyword">struct</span> &#123;</span><br><span class="line">	mu      sync.Mutex</span><br><span class="line">	sources []runtime.Source</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *aggregate)</span> <span class="title">Start</span><span class="params">(handler resource.EventHandler)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	......</span><br><span class="line">	<span class="keyword">for</span> _, source := <span class="keyword">range</span> s.sources &#123;</span><br><span class="line">		<span class="keyword">if</span> err := source.Start(syncHandler); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	......</span><br></pre></td></tr></table></figure>
<p><code>source/kube/builtin.source</code>、<code>source/kube/dynamic.source</code>本身都包含一个k8s SharedIndexInformer:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// source is a simplified client interface for listening/getting Kubernetes resources in an unstructured way.</span></span><br><span class="line"><span class="keyword">type</span> source <span class="keyword">struct</span> &#123;</span><br><span class="line">	......</span><br><span class="line">	<span class="comment">// SharedIndexInformer for watching/caching resources</span></span><br><span class="line">	informer cache.SharedIndexInformer</span><br><span class="line"></span><br><span class="line">	handler resource.EventHandler</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>二者的<code>Start</code>方法的实现, 正是用到了k8s典型的 Informer+list/watch 模式, 获取关注「配置」的变化事件, 在此不再赘述.</p>
<p>Source 获得「配置」更新事件后, 会将其推送到Processor 的events chan 中, events 长度为1024,  通过<code>go p.process()</code>, <code>Proccesor</code>的<code>handler</code>会对事件进行异步处理.</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *Processor)</span> <span class="title">Start</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	......</span><br><span class="line">    events := <span class="built_in">make</span>(<span class="keyword">chan</span> resource.Event, <span class="number">1024</span>)</span><br><span class="line">	err := p.source.Start(<span class="function"><span class="keyword">func</span><span class="params">(e resource.Event)</span></span> &#123;</span><br><span class="line">		events &lt;- e</span><br><span class="line">	&#125;)</span><br><span class="line">    ......</span><br><span class="line">	p.events = events</span><br><span class="line"></span><br><span class="line">	<span class="keyword">go</span> p.process()</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *Processor)</span> <span class="title">process</span><span class="params">()</span></span> &#123;</span><br><span class="line">loop:</span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		<span class="keyword">select</span> &#123;</span><br><span class="line">		<span class="comment">// Incoming events are received through p.events</span></span><br><span class="line">		<span class="keyword">case</span> e := &lt;-p.events:</span><br><span class="line">			p.processEvent(e)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">case</span> &lt;-p.state.strategy.Publish:</span><br><span class="line">			scope.Debug(<span class="string">"Processor.process: publish"</span>)</span><br><span class="line">			p.state.publish()</span><br><span class="line"></span><br><span class="line">		<span class="comment">// p.done signals the graceful Shutdown of the processor.</span></span><br><span class="line">		<span class="keyword">case</span> &lt;-p.done:</span><br><span class="line">			scope.Debug(<span class="string">"Processor.process: done"</span>)</span><br><span class="line">			<span class="keyword">break</span> loop</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> p.postProcessHook != <span class="literal">nil</span> &#123;</span><br><span class="line">			p.postProcessHook()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Handler-和-State"><a href="#Handler-和-State" class="headerlink" title="Handler 和 State"></a>Handler 和 State</h3><p>interface Handler 代表对「配置」变化事件的处理器:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Handler handles an incoming resource event.</span></span><br><span class="line"><span class="keyword">type</span> Handler <span class="keyword">interface</span> &#123;</span><br><span class="line">	Handle(e resource.Event)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在istio中有多个Handler的实现, 典型的有:</p>
<ul>
<li>Dispatcher</li>
<li>State</li>
</ul>
<p>Dispatcher 是多个Handler的集合:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type Dispatcher struct &#123;</span><br><span class="line">	handlers map[resource.Collection][]Handler</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>State 是对Galley的内存中的状态, 包括了Galley 当前持有「配置」的schema、发布策略以及内容快照等:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// State is the in-memory state of Galley.</span></span><br><span class="line"><span class="keyword">type</span> State <span class="keyword">struct</span> &#123;</span><br><span class="line">	name   <span class="keyword">string</span></span><br><span class="line">	schema *resource.Schema</span><br><span class="line"></span><br><span class="line">	distribute  <span class="keyword">bool</span></span><br><span class="line">	strategy    *publish.Strategy</span><br><span class="line">	distributor publish.Distributor</span><br><span class="line"></span><br><span class="line">	config *Config</span><br><span class="line"></span><br><span class="line">	<span class="comment">// version counter is a nonce that generates unique ids for each updated view of State.</span></span><br><span class="line">	versionCounter <span class="keyword">int64</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// entries for per-message-type State.</span></span><br><span class="line">	entriesLock sync.Mutex</span><br><span class="line">	entries     <span class="keyword">map</span>[resource.Collection]*resourceTypeState</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Virtual version numbers for Gateways &amp; VirtualServices for Ingress projected ones</span></span><br><span class="line">	ingressGWVersion   <span class="keyword">int64</span></span><br><span class="line">	ingressVSVersion   <span class="keyword">int64</span></span><br><span class="line">	lastIngressVersion <span class="keyword">int64</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// pendingEvents counts the number of events awaiting publishing.</span></span><br><span class="line">	pendingEvents <span class="keyword">int64</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// lastSnapshotTime records the last time a snapshot was published.</span></span><br><span class="line">	lastSnapshotTime time.Time</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同时State 也实现了interface <code>Handler</code>, 最终「配置」资源将会作为快照存储到State的<code>distributor</code>中, <code>distributor</code>实际的实现是mcp包中的<code>Cache</code>, 实际会调用mcp中的<code>Cache#SetSnapshot</code>.</p>
<hr>
<h3 id="Distributor-、Watcher-和-Cache"><a href="#Distributor-、Watcher-和-Cache" class="headerlink" title="Distributor 、Watcher 和 Cache"></a>Distributor 、Watcher 和 Cache</h3><p>在mcp包中, 有2个interface 值得特别关注: Distributor 和 Watcher</p>
<p>interface Distributor 定义了「配置」快照存储需要实现的接口, State 最终会调用<code>SetSnapshot</code>将配置存储到快照中.</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Distributor interface allows processor to distribute snapshots of configuration.</span></span><br><span class="line"><span class="keyword">type</span> Distributor <span class="keyword">interface</span> &#123;</span><br><span class="line">	SetSnapshot(name <span class="keyword">string</span>, snapshot sn.Snapshot)</span><br><span class="line"></span><br><span class="line">	ClearSnapshot(name <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>interface Watcher 功能有点类似k8s的 list/watch, Watch方法会注册 mcp sink 的watch 请求和处理函数:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Watcher requests watches for configuration resources by node, last</span></span><br><span class="line"><span class="comment">// applied version, and type. The watch should send the responses when</span></span><br><span class="line"><span class="comment">// they are ready. The watch can be canceled by the consumer.</span></span><br><span class="line"><span class="keyword">type</span> Watcher <span class="keyword">interface</span> &#123;</span><br><span class="line">	<span class="comment">// Watch returns a new open watch for a non-empty request.</span></span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	<span class="comment">// Cancel is an optional function to release resources in the</span></span><br><span class="line">	<span class="comment">// producer. It can be called idempotently to cancel and release resources.</span></span><br><span class="line">	Watch(*Request, PushResponseFunc) CancelWatchFunc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>struct <code>mcp/snapshot.Cache</code> 同时实现了Distributor 和 Watcher interface:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">type Cache struct &#123;</span><br><span class="line">	mu         sync.RWMutex</span><br><span class="line">	snapshots  map[string]Snapshot</span><br><span class="line">	status     map[string]*StatusInfo</span><br><span class="line">	watchCount int64</span><br><span class="line"></span><br><span class="line">	groupIndex GroupIndexFn</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>mcp 服务端在接口 <code>StreamAggregatedResources</code>和<code>EstablishResourceStream</code>中, 会调用Watch方法, 注册sink连接的watch请求:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sr := &amp;source.Request&#123;</span><br><span class="line">	SinkNode:    req.SinkNode,</span><br><span class="line">	Collection:  collection,</span><br><span class="line">	VersionInfo: req.VersionInfo,</span><br><span class="line">&#125;</span><br><span class="line">w.cancel = con.watcher.Watch(sr, con.queueResponse)</span><br></pre></td></tr></table></figure>
<p> <code>mcp/snapshot.Cache</code> 实现了interface Distributor 的<code>SetSnapshot</code>方法, 该方法在State状态变化后会被调用, 该方法会遍历之前watch注册的responseWatch, 并将WatchResponse传递给各个处理方法.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// SetSnapshot updates a snapshot for a group.</span><br><span class="line">func (c *Cache) SetSnapshot(group string, snapshot Snapshot) &#123;</span><br><span class="line">	c.mu.Lock()</span><br><span class="line">	defer c.mu.Unlock()</span><br><span class="line"></span><br><span class="line">	// update the existing entry</span><br><span class="line">	c.snapshots[group] = snapshot</span><br><span class="line"></span><br><span class="line">	// trigger existing watches for which version changed</span><br><span class="line">	if info, ok := c.status[group]; ok &#123;</span><br><span class="line">		info.mu.Lock()</span><br><span class="line">		defer info.mu.Unlock()</span><br><span class="line"></span><br><span class="line">		for id, watch := range info.watches &#123;</span><br><span class="line">			version := snapshot.Version(watch.request.Collection)</span><br><span class="line">			if version != watch.request.VersionInfo &#123;</span><br><span class="line">				scope.Infof(&quot;SetSnapshot(): respond to watch %d for %v @ version %q&quot;,</span><br><span class="line">					id, watch.request.Collection, version)</span><br><span class="line"></span><br><span class="line">				response := &amp;source.WatchResponse&#123;</span><br><span class="line">					Collection: watch.request.Collection,</span><br><span class="line">					Version:    version,</span><br><span class="line">					Resources:  snapshot.Resources(watch.request.Collection),</span><br><span class="line">					Request:    watch.request,</span><br><span class="line">				&#125;</span><br><span class="line">				watch.pushResponse(response)</span><br><span class="line"></span><br><span class="line">				// discard the responseWatch</span><br><span class="line">				delete(info.watches, id)</span><br><span class="line"></span><br><span class="line">				scope.Debugf(&quot;SetSnapshot(): watch %d for %v @ version %q complete&quot;,</span><br><span class="line">					id, watch.request.Collection, version)</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>提供给Watch的处理函数<code>queueResponse</code>会将WatchResponse放入连接的响应队列, 最终会推送给mcp sink端.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// Queue the response for sending in the dispatch loop. The caller may provide</span><br><span class="line">// a nil response to indicate that the watch should be closed.</span><br><span class="line">func (con *connection) queueResponse(resp *WatchResponse) &#123;</span><br><span class="line">	if resp == nil &#123;</span><br><span class="line">		con.queue.Close()</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		con.queue.Enqueue(resp.Collection, resp)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<p>最后上一张Galley mcp 服务相关模型UML:</p>
<p><img src="https://imfox.io/assets/images/istio-a/galley_uml.png" alt=""></p>
<p><a href="https://imfox.io/assets/images/istio-a/galley_uml.png" target="_blank">查看高清原图</a></p>
<p>Galley 源代码展示了面向抽象(interface)编程的好处, Source 是对「配置」数据源的抽象, Distributor 是「配置」快照存储的抽象, Watcher 是对「配置」订阅端的抽象. 抽象的具体实现可以组合起来使用. 另外Galley组件之间也充分解耦, 组件之间的数据通过chan/watcher等流转.</p>
<p>关于早期 istio 配置管理的演进计划, 可以参考2018年5月 CNCF KubeCon talk <a href="https://www.youtube.com/watch?v=x1Tyw8dFKjI&amp;index=2&amp;t=0s&amp;list=LLQ2StCCdx81xHxHxBO0foGA" target="_blank" rel="noopener">Introduction to Istio Configuration - Joy Zhang</a> (需.翻.墙),  1.1 版本中Galley 也还未完全实现该文中的roadmap, 如 configuration pipeline 等. 未来Galley 还会继续演进.</p>
<blockquote>
<p>版权归作者所有, 欢迎转载, 转载请注明出处</p>
</blockquote>
<hr>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://www.youtube.com/watch?v=x1Tyw8dFKjI&amp;index=2&amp;t=0s&amp;list=LLQ2StCCdx81xHxHxBO0foGA" target="_blank" rel="noopener">Introduction to Istio Configuration </a></li>
<li><a href="https://docs.google.com/document/d/1o2-V4TLJ8fJACXdlsnxKxDv2Luryo48bAhR8ShxE5-k/edit#heading=h.qex63c29z2to" target="_blank" rel="noopener">google doc Mesh Configuration Protocol (MCP)</a></li>
<li><a href="https://github.com/istio/api/tree/master/mcp" target="_blank" rel="noopener">github Mesh Configuration Protocol (MCP)</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/04/01/istio-analysis-3/" data-id="ck89nwpbj0009o8kbw2f73vyv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-istio-analysis-2" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/19/istio-analysis-2/" class="article-date">
  <time datetime="2019-03-19T07:30:00.000Z" itemprop="datePublished">2019-03-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/19/istio-analysis-2/">istio 庖丁解牛(二) sidecar injector</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imfox.io/" target="_blank" rel="noopener">钟华</a></p>
<p>今天我们分析下istio-sidecar-injector 组件:</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g187i5bzkpj315q0u07dq.jpg" referrerpolicy="no-referrer"></p>
<p><a href="https://ws4.sinaimg.cn/large/006tKfTcgy1g187dn7s1tj315m0u0x6t.jpg" referrerpolicy="no-referrer" target="_blank">查看高清原图</a></p>
<p>用户空间的Pod要想加入mesh, 首先需要注入sidecar 容器, istio 提供了2种方式实现注入:</p>
<ul>
<li>自动注入: 利用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">Kubernetes Dynamic Admission Webhooks</a> 对 新建的pod 进行注入: initContainer + sidecar</li>
<li>手动注入: 使用命令<code>istioctl kube-inject</code></li>
</ul>
<p>「注入」本质上就是修改Pod的资源定义, 添加相应的sidecar容器定义, 内容包括2个新容器:</p>
<ul>
<li>名为<code>istio-init</code>的initContainer: 通过配置iptables来劫持Pod中的流量</li>
<li>名为<code>istio-proxy</code>的sidecar容器: 两个进程pilot-agent和envoy, pilot-agent 进行初始化并启动envoy</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g187flw0dmj30wq0grn0b.jpg" referrerpolicy="no-referrer"></p>
<hr>
<h2 id="1-Dynamic-Admission-Control"><a href="#1-Dynamic-Admission-Control" class="headerlink" title="1. Dynamic Admission Control"></a>1. Dynamic Admission Control</h2><p>kubernetes 的准入控制(Admission Control)有2种:</p>
<ul>
<li>Built in Admission Control: 这些Admission模块可以选择性地编译进api server, 因此需要修改和重启kube-apiserver</li>
<li>Dynamic Admission Control: 可以部署在kube-apiserver之外, 同时无需修改或重启kube-apiserver.</li>
</ul>
<p>其中, Dynamic Admission Control 包含2种形式:</p>
<ul>
<li>Admission Webhooks: 该controller 提供http server, 被动接受kube-apiserver分发的准入请求.</li>
<li><p>Initializers: 该controller主动list and watch 关注的资源对象, 对watch到的未初始化对象进行相应的改造.</p>
<p>其中, Admission Webhooks 又包含2种准入控制:</p>
</li>
<li><p>ValidatingAdmissionWebhook</p>
</li>
<li>MutatingAdmissionWebhook</li>
</ul>
<p>istio 使用了MutatingAdmissionWebhook来实现对用户Pod的注入,  首先需要保证以下条件满足:</p>
<ul>
<li>确保 kube-apiserver 启动参数 开启了 MutatingAdmissionWebhook</li>
<li>给namespace 增加 label: <code>kubectl label namespace default istio-injection=enabled</code></li>
<li>同时还要保证 kube-apiserver 的 aggregator layer 开启: <code>--enable-aggregator-routing=true</code> 且证书和api server连通性正确设置.</li>
</ul>
<p>另外还需要一个配置对象, 来告诉kube-apiserver istio关心的资源对象类型, 以及webhook的服务地址. 如果使用helm安装istio, 配置对象已经添加好了, 查阅MutatingWebhookConfiguration:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">% kubectl get mutatingWebhookConfiguration -oyaml</span><br><span class="line">- apiVersion: admissionregistration.k8s.io/v1beta1</span><br><span class="line">  kind: MutatingWebhookConfiguration</span><br><span class="line">  metadata:</span><br><span class="line">    name: istio-sidecar-injector</span><br><span class="line">  webhooks:</span><br><span class="line">    - clientConfig:</span><br><span class="line">      service:</span><br><span class="line">        name: istio-sidecar-injector</span><br><span class="line">        namespace: istio-system</span><br><span class="line">        path: /inject</span><br><span class="line">    name: sidecar-injector.istio.io</span><br><span class="line">    namespaceSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        istio-injection: enabled</span><br><span class="line">    rules:</span><br><span class="line">    - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">      apiVersions:</span><br><span class="line">      - v1</span><br><span class="line">      operations:</span><br><span class="line">      - CREATE</span><br><span class="line">      resources:</span><br><span class="line">      - pods</span><br></pre></td></tr></table></figure>
<p>该配置告诉kube-apiserver: 命名空间istio-system 中的服务 <code>istio-sidecar-injector</code>(默认443端口), 通过路由<code>/inject</code>, 处理<code>v1/pods</code>的CREATE, 同时pod需要满足命名空间<code>istio-injection: enabled</code>, 当有符合条件的pod被创建时, kube-apiserver就会对该服务发起调用, 服务返回的内容正是添加了sidecar注入的pod定义.</p>
<hr>
<h2 id="2-Sidecar-注入内容分析"><a href="#2-Sidecar-注入内容分析" class="headerlink" title="2. Sidecar 注入内容分析"></a>2. Sidecar 注入内容分析</h2><p>查看Pod <code>istio-sidecar-injector</code>的yaml定义:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">%kubectl</span> <span class="bullet">-n</span> <span class="string">istio-system</span> <span class="string">get</span> <span class="string">pod</span> <span class="string">istio-sidecar-injector-5f7894f54f-w7f9v</span> <span class="bullet">-oyaml</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> <span class="string">/etc/istio/inject</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">inject-config</span></span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - configMap:</span></span><br><span class="line"><span class="attr">      items:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">config</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">config</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">istio-sidecar-injector</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">inject-config</span></span><br></pre></td></tr></table></figure>
<p>可以看到该Pod利用<a href="https://kubernetes.io/docs/concepts/storage/volumes/#projected" target="_blank" rel="noopener">projected volume</a>将<code>istio-sidecar-injector</code>这个config map 的config挂到了自己容器路径<code>/etc/istio/inject/config</code>, 该config map 内容正是注入用户空间pod所需的模板.</p>
<p>如果使用helm安装istio, 该 configMap 模板源码位于: <a href="https://github.com/istio/istio/blob/master/install/kubernetes/helm/istio/templates/sidecar-injector-configmap.yaml" target="_blank" rel="noopener">https://github.com/istio/istio/blob/master/install/kubernetes/helm/istio/templates/sidecar-injector-configmap.yaml</a>.</p>
<p>该config map 是在安装istio时添加的, kubernetes 会自动维护 projected volume的更新, 因此 容器 <code>sidecar-injector</code>只需要从本地文件直接读取所需配置.</p>
<p>高级用户可以按需修改这个模板内容.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=&apos;&#123;.data.config&#125;&apos;</span><br></pre></td></tr></table></figure>
<p>查看该configMap, <code>data.config</code>包含以下内容(简化):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">policy: enabled // 是否开启自动注入</span><br><span class="line">template: |-    // 使用go template 定义的pod patch</span><br><span class="line">  initContainers:</span><br><span class="line">  [[ if ne (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) &quot;NONE&quot; ]]</span><br><span class="line">  - name: istio-init</span><br><span class="line">    image: &quot;docker.io/istio/proxy_init:1.1.0&quot;</span><br><span class="line">    ......</span><br><span class="line">    securityContext:</span><br><span class="line">      capabilities:</span><br><span class="line">        add:</span><br><span class="line">        - NET_ADMIN</span><br><span class="line">    ......</span><br><span class="line">  containers:</span><br><span class="line">  - name: istio-proxy</span><br><span class="line">    args:</span><br><span class="line">    - proxy</span><br><span class="line">    - sidecar</span><br><span class="line">    ......</span><br><span class="line">    image: [[ annotation .ObjectMeta `sidecar.istio.io/proxyImage`  &quot;docker.io/istio/proxyv2:1.1.0&quot;  ]]</span><br><span class="line">    ......</span><br><span class="line">    readinessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: /healthz/ready</span><br><span class="line">        port: [[ annotation .ObjectMeta `status.sidecar.istio.io/port`  0  ]]</span><br><span class="line">    ......</span><br><span class="line">    securityContext:</span><br><span class="line">      capabilities:</span><br><span class="line">        add:</span><br><span class="line">        - NET_ADMIN</span><br><span class="line">      runAsGroup: 1337</span><br><span class="line">  ......</span><br><span class="line">    volumeMounts:</span><br><span class="line">    ......</span><br><span class="line">    - mountPath: /etc/istio/proxy</span><br><span class="line">      name: istio-envoy</span><br><span class="line">    - mountPath: /etc/certs/</span><br><span class="line">      name: istio-certs</span><br><span class="line">      readOnly: true</span><br><span class="line">      ......</span><br><span class="line">  volumes:</span><br><span class="line">  ......</span><br><span class="line">  - emptyDir:</span><br><span class="line">      medium: Memory</span><br><span class="line">    name: istio-envoy</span><br><span class="line">  - name: istio-certs</span><br><span class="line">    secret:</span><br><span class="line">      optional: true</span><br><span class="line">      [[ if eq .Spec.ServiceAccountName &quot;&quot; -]]</span><br><span class="line">      secretName: istio.default</span><br><span class="line">      [[ else -]]</span><br><span class="line">      secretName: [[ printf &quot;istio.%s&quot; .Spec.ServiceAccountName ]]</span><br><span class="line">      ......</span><br></pre></td></tr></table></figure>
<p>对istio-init生成的部分参数分析:</p>
<ul>
<li><code>-u 1337</code> 排除用户ID为1337，即Envoy自身的流量</li>
<li>解析用户容器<code>.Spec.Containers</code>, 获得容器的端口列表, 传入<code>-b</code>参数(入站端口控制)</li>
<li>指定要从重定向到 Envoy 中排除（可选）的入站端口列表, 默认写入<code>-d 15020</code>, 此端口是sidecar的status server</li>
<li>赋予该容器<code>NET_ADMIN</code> 能力, 允许容器istio-init进行网络管理操作</li>
</ul>
<p>对istio-proxy 生成的部分参数分析:</p>
<ul>
<li>启动参数<code>proxy sidecar xxx</code> 用以定义该节点的代理类型(NodeType)</li>
<li>默认的status server 端口<code>--statusPort=15020</code></li>
<li>解析用户容器<code>.Spec.Containers</code>, 获取用户容器的application Ports, 然后设置到sidecar的启动参数<code>--applicationPorts</code>中, 该参数会最终传递给envoy, 用以确定哪些端口流量属于该业务容器.</li>
<li>设置<code>/healthz/ready</code> 作为该代理的readinessProbe</li>
<li>同样赋予该容器<code>NET_ADMIN</code>能力</li>
</ul>
<p>另外<code>istio-sidecar-injector</code>还给容器<code>istio-proxy</code>挂了2个volumes:</p>
<ul>
<li><p>名为<code>istio-envoy</code>的emptydir volume, 挂载到容器目录<code>/etc/istio/proxy</code>, 作为envoy的配置文件目录</p>
</li>
<li><p>名为<code>istio-certs</code>的secret volume, 默认secret名为<code>istio.default</code>,  挂载到容器目录<code>/etc/certs/</code>, 存放相关的证书, 包括服务端证书, 和可能的mtls客户端证书</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">% kubectl exec productpage-v1-6597cb5df9-xlndw -c istio-proxy -- ls /etc/certs/</span><br><span class="line">cert-chain.pem</span><br><span class="line">key.pem</span><br><span class="line">root-cert.pem</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>后续文章探究sidecar <code>istio-proxy</code>会对其进一步分析.</p>
<hr>
<h2 id="3-istio-sidecar-injector-webhook-源码分析"><a href="#3-istio-sidecar-injector-webhook-源码分析" class="headerlink" title="3. istio-sidecar-injector-webhook 源码分析"></a>3. istio-sidecar-injector-webhook 源码分析</h2><ul>
<li>镜像Dockerfile: <code>istio/pilot/docker/Dockerfile.sidecar_injector</code></li>
<li>启动命令: <code>/sidecar-injector</code></li>
<li>命令源码: <code>istio/pilot/cmd/sidecar-injector</code></li>
</ul>
<p>容器中命令/sidecar-injector启动参数如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- args:</span><br><span class="line">  - --caCertFile=/etc/istio/certs/root-cert.pem</span><br><span class="line">  - --tlsCertFile=/etc/istio/certs/cert-chain.pem</span><br><span class="line">  - --tlsKeyFile=/etc/istio/certs/key.pem</span><br><span class="line">  - --injectConfig=/etc/istio/inject/config</span><br><span class="line">  - --meshConfig=/etc/istio/config/mesh</span><br><span class="line">  - --healthCheckInterval=2s</span><br><span class="line">  - --healthCheckFile=/health</span><br></pre></td></tr></table></figure>
<p><code>sidecar-injector</code> 的核心数据模型是 <code>Webhook</code>struct, 注入配置sidecarConfig包括注入模板以及注入开关和规则:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">type Webhook struct &#123;</span><br><span class="line">	mu                     sync.RWMutex</span><br><span class="line">	sidecarConfig          *Config // 注入配置: 模板,开关,规则</span><br><span class="line">	sidecarTemplateVersion string</span><br><span class="line">	meshConfig             *meshconfig.MeshConfig</span><br><span class="line"></span><br><span class="line">	healthCheckInterval time.Duration</span><br><span class="line">	healthCheckFile     string</span><br><span class="line"></span><br><span class="line">	server     *http.Server</span><br><span class="line">	meshFile   string</span><br><span class="line">	configFile string            // 注入内容路径, 从启动参数injectConfig中获取</span><br><span class="line">	watcher    *fsnotify.Watcher // 基于文件系统的notifications</span><br><span class="line">	certFile   string</span><br><span class="line">	keyFile    string</span><br><span class="line">	cert       *tls.Certificate</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Config struct &#123;</span><br><span class="line">	Policy InjectionPolicy `json:&quot;policy&quot;`</span><br><span class="line">	Template string `json:&quot;template&quot;`</span><br><span class="line">	NeverInjectSelector []metav1.LabelSelector `json:&quot;neverInjectSelector&quot;`</span><br><span class="line">	AlwaysInjectSelector []metav1.LabelSelector `json:&quot;alwaysInjectSelector&quot;`</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>sidecar-injector</code> 的root cmd 会创建一个<code>Webhook</code>, 该struct包含一个http server, 并将路由<code>/inject</code>注册到处理器函数<code>serveInject</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">RunE: func(c *cobra.Command, _ []string) error &#123;</span><br><span class="line">    ......</span><br><span class="line">    wh, err := inject.NewWebhook(parameters)</span><br><span class="line">    ......</span><br><span class="line">    go wh.Run(stop)</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func NewWebhook(p WebhookParameters) (*Webhook, error) &#123;</span><br><span class="line">    ......</span><br><span class="line">	watcher, err := fsnotify.NewWatcher()</span><br><span class="line">	// watch the parent directory of the target files so we can catch</span><br><span class="line">	// symlink updates of k8s ConfigMaps volumes.</span><br><span class="line">	for _, file := range []string&#123;p.ConfigFile, p.MeshFile, p.CertFile, p.KeyFile&#125; &#123;</span><br><span class="line">		watchDir, _ := filepath.Split(file)</span><br><span class="line">		if err := watcher.Watch(watchDir); err != nil &#123;</span><br><span class="line">			return nil, fmt.Errorf(&quot;could not watch %v: %v&quot;, file, err)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	......</span><br><span class="line">	h := http.NewServeMux()</span><br><span class="line">	h.HandleFunc(&quot;/inject&quot;, wh.serveInject)</span><br><span class="line">	wh.server.Handler = h</span><br><span class="line">	......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>Webhook#Run</code>方法会启动该http server, 并负责响应配置文件的更新:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">func (wh *Webhook) Run(stop &lt;-chan struct&#123;&#125;) &#123;</span><br><span class="line">	go func() &#123;</span><br><span class="line">		wh.server.ListenAndServeTLS(&quot;&quot;, &quot;&quot;)</span><br><span class="line">		......</span><br><span class="line">	&#125;()</span><br><span class="line">	......</span><br><span class="line">	var timerC &lt;-chan time.Time</span><br><span class="line">	for &#123;</span><br><span class="line">		select &#123;</span><br><span class="line">		case &lt;-timerC:</span><br><span class="line">			timerC = nil</span><br><span class="line">			sidecarConfig, meshConfig, err := loadConfig(wh.configFile, wh.meshFile)</span><br><span class="line">			......</span><br><span class="line">		case event := &lt;-wh.watcher.Event:</span><br><span class="line">			// use a timer to debounce configuration updates</span><br><span class="line">			if (event.IsModify() || event.IsCreate()) &amp;&amp; timerC == nil &#123;</span><br><span class="line">				timerC = time.After(watchDebounceDelay)</span><br><span class="line">			&#125;</span><br><span class="line">		case ......</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>Webhook#Run</code>首先会启动处理注入请求的http server, 下面的for循环主要是处理2个配置文件的更新操作, select 里使用了一个timer(并不是ticker), 咋一看像是简单的定时更新配置文件, 其实不然. 配置文件更新事件由<code>wh.watcher</code>进行接收, 然后才会启动timer, 这里用到了第三方库<a href="https://github.com/howeyc/fsnotify" target="_blank" rel="noopener">https://github.com/howeyc/fsnotify</a>, 这是一个基于文件系统的notification. 这里使用timer限制在一个周期(watchDebounceDelay)里面最多重新加载一次配置文件, 避免在配置文件频繁变化的情况下多次触发不必要的loadConfig</p>
<blockquote>
<p>use a timer to debounce configuration updates</p>
</blockquote>
<p><code>Webhook.serveInject</code> 会调用<code>Webhook#inject</code>, 最终的模板处理函数是<code>injectionData</code>.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/03/19/istio-analysis-2/" data-id="ck89nwpbh0008o8kb6zestg9v" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-istio-analysis-1" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/11/istio-analysis-1/" class="article-date">
  <time datetime="2019-03-11T07:30:00.000Z" itemprop="datePublished">2019-03-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/11/istio-analysis-1/">istio 庖丁解牛(一) 组件概览</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imfox.me/" target="_blank" rel="noopener">钟华</a></p>
<p>Istio 作为 Service Mesh 领域的集大成者, 提供了流控, 安全, 遥测等模型, 其功能复杂, 模块众多,  有较高的学习和使用门槛,  本文会对istio 1.1 的各组件进行分析, 希望能帮助读者了解istio各组件的职责、以及相互的协作关系.</p>
<meta name="referrer" content="no-referrer">

<h2 id="1-istio-组件构成"><a href="#1-istio-组件构成" class="headerlink" title="1. istio 组件构成"></a>1. istio 组件构成</h2><p>以下是istio 1.1 官方架构图:</p>
<p><img src="https://preliminary.istio.io/docs/concepts/what-is-istio/arch.svg" width="80%"></p>
<p>虽然Istio 支持多个平台, 但将其与 Kubernetes 结合使用，其优势会更大, Istio 对Kubernetes 平台支持也是最完善的, 本文将基于Istio + Kubernetes 进行展开.</p>
<p>如果安装了grafana, prometheus, kiali, jaeger等组件的情况下, 一个完整的控制面组件包括以下pod:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">% kubectl -n istio-system get pod</span><br><span class="line">NAME                                          READY     STATUS</span><br><span class="line">grafana-5f54556df5-s4xr4                      1/1       Running</span><br><span class="line">istio-citadel-775c6cfd6b-8h5gt                1/1       Running</span><br><span class="line">istio-galley-675d75c954-kjcsg                 1/1       Running</span><br><span class="line">istio-ingressgateway-6f7b477cdd-d8zpv         1/1       Running</span><br><span class="line">istio-pilot-7dfdb48fd8-92xgt                  2/2       Running</span><br><span class="line">istio-policy-544967d75b-p6qkk                 2/2       Running</span><br><span class="line">istio-sidecar-injector-5f7894f54f-w7f9v       1/1       Running</span><br><span class="line">istio-telemetry-777876dc5d-msclx              2/2       Running</span><br><span class="line">istio-tracing-5fbc94c494-558fp                1/1       Running</span><br><span class="line">kiali-7c6f4c9874-vzb4t                        1/1       Running</span><br><span class="line">prometheus-66b7689b97-w9glt                   1/1       Running</span><br></pre></td></tr></table></figure>
<p>将istio系统组件细化到进程级别, 大概是这个样子:</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g187gshs79j315m0u0qct.jpg" referrerpolicy="no-referrer"></p>
<p><a href="https://ws4.sinaimg.cn/large/006tKfTcgy1g187dn7s1tj315m0u0x6t.jpg" referrerpolicy="no-referrer" target="_blank">查看高清原图</a></p>
<p>Service Mesh 的Sidecar 模式要求对数据面的用户Pod进行代理的注入, 注入的代理容器会去处理服务治理领域的各种「脏活累活」, 使得用户容器可以专心处理业务逻辑.</p>
<p>从上图可以看出, Istio 控制面本身就是一个复杂的微服务系统, 该系统包含多个组件Pod, 每个组件 各司其职, 既有单容器Pod, 也有多容器Pod, 既有单进程容器, 也有多进程容器,   每个组件会调用不同的命令, 各组件之间会通过RPC进行协作, 共同完成对数据面用户服务的管控.</p>
<hr>
<h2 id="2-Istio-源码-镜像和命令"><a href="#2-Istio-源码-镜像和命令" class="headerlink" title="2. Istio 源码, 镜像和命令"></a>2. Istio 源码, 镜像和命令</h2><p>Isito 项目代码主要由以下2个git 仓库组成:</p>
<table>
<thead>
<tr>
<th>仓库地址</th>
<th>语言</th>
<th>模块</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/istio/istio" target="_blank" rel="noopener">https://github.com/istio/istio</a></td>
<td>Go</td>
<td>包含istio控制面的大部分组件: pilot, mixer, citadel, galley, sidecar-injector等,</td>
</tr>
<tr>
<td><a href="https://github.com/istio/proxy" target="_blank" rel="noopener">https://github.com/istio/proxy</a></td>
<td>C++</td>
<td>包含 istio 使用的边车代理, 这个边车代理包含envoy和mixer client两块功能</td>
</tr>
</tbody>
</table>
<h3 id="2-1-istio-istio"><a href="#2-1-istio-istio" class="headerlink" title="2.1 istio/istio"></a>2.1 istio/istio</h3><p><a href="https://github.com/istio/istio" target="_blank" rel="noopener">https://github.com/istio/istio</a> 包含的主要的镜像和命令:</p>
<table>
<thead>
<tr>
<th>容器名</th>
<th>镜像名</th>
<th>启动命令</th>
<th>源码入口</th>
</tr>
</thead>
<tbody>
<tr>
<td>Istio_init</td>
<td>istio/proxy_init</td>
<td>istio-iptables.sh</td>
<td>istio/tools/deb/istio-iptables.sh</td>
</tr>
<tr>
<td>istio-proxy</td>
<td>istio/proxyv2</td>
<td>pilot-agent</td>
<td>istio/pilot/cmd/pilot-agent</td>
</tr>
<tr>
<td>sidecar-injector-webhook</td>
<td>istio/sidecar_injector</td>
<td>sidecar-injector</td>
<td>istio/pilot/cmd/sidecar-injector</td>
</tr>
<tr>
<td>discovery</td>
<td>istio/pilot</td>
<td>pilot-discovery</td>
<td>istio/pilot/cmd/pilot-discovery</td>
</tr>
<tr>
<td>galley</td>
<td>istio/galley</td>
<td>galley</td>
<td>istio/galley/cmd/galley</td>
</tr>
<tr>
<td>mixer</td>
<td>istio/mixer</td>
<td>mixs</td>
<td>istio/mixer/cmd/mixs</td>
</tr>
<tr>
<td>citadel</td>
<td>istio/citadel</td>
<td>istio_ca</td>
<td>istio/security/cmd/istio_ca</td>
</tr>
</tbody>
</table>
<p>另外还有2个命令不在上图中使用:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>源码入口</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>mixc</td>
<td>istio/mixer/cmd/mixc</td>
<td>用于和Mixer server 交互的客户端</td>
</tr>
<tr>
<td>node_agent</td>
<td>istio/security/cmd/node_agent</td>
<td>用于node上安装安全代理, 这在Mesh Expansion特性中会用到, 即k8s和vm打通.</td>
</tr>
</tbody>
</table>
<h3 id="2-2-istio-proxy"><a href="#2-2-istio-proxy" class="headerlink" title="2.2 istio/proxy"></a>2.2 istio/proxy</h3><p><a href="https://github.com/istio/proxy" target="_blank" rel="noopener">https://github.com/istio/proxy</a>  该项目本身不会产出镜像, 它可以编译出一个<code>name = &quot;Envoy&quot;</code>的二进制程序, 该二进制程序会被ADD到istio的边车容器镜像<code>istio/proxyv2</code>中.</p>
<p>istio proxy 项目使用的编译方式是Google出品的bazel,  bazel可以直接在编译中引入第三方库，加载第三方源码.</p>
<p>这个项目包含了对Envoy源码的引用，还在此基础上进行了扩展，这些扩展是通过Envoy filter（过滤器）的形式来提供，这样做的目的是让边车代理将策略执行决策委托给Mixer，因此可以理解istio proxy 这个项目有2大功能模块:</p>
<ol>
<li>Envoy: 使用到Envoy的全部功能</li>
<li>mixer client: 测量和遥测相关的客户端实现, 基于Envoy做扩展，通过RPC和Mixer server 进行交互,  实现策略管控和遥测</li>
</ol>
<p>后续我将对以上各个模块、命令以及它们之间的协作进行探究.</p>
<hr>
<h2 id="3-Istio-Pod-概述"><a href="#3-Istio-Pod-概述" class="headerlink" title="3. Istio Pod 概述"></a>3. Istio Pod 概述</h2><h3 id="3-1-数据面用户Pod"><a href="#3-1-数据面用户Pod" class="headerlink" title="3.1 数据面用户Pod"></a>3.1 数据面用户Pod</h3><p>数据面用户Pod注入的内容包括:</p>
<ol>
<li><p>initContainer <code>istio-init</code>:  通过配置iptables来劫持Pod中的流量, 转发给envoy</p>
</li>
<li><p>sidecar container <code>istio-proxy</code>:  包含2个进程, 父进程pliot-agent 初始化并管控envoy, 子进程envoy除了包含原生envoy的功能外, 还加入了mixer client的逻辑.</p>
<p>主要端口:</p>
<ul>
<li><code>--statusPort</code>  status server 端口, 默认为0, 表示不启动, istio启动时通常传递为15020, 由pliot-agent监听</li>
<li><code>--proxyAdminPort</code> 代理管理端口, 默认 15000, 由子进程envoy监听.</li>
</ul>
</li>
</ol>
<h3 id="3-2-istio-sidecar-injector"><a href="#3-2-istio-sidecar-injector" class="headerlink" title="3.2 istio-sidecar-injector"></a>3.2 istio-sidecar-injector</h3><p>包含一个单容器,  <code>sidecar-injector-webhook</code>: 启动一个http server, 接受kube api server 的Admission Webhook 请求, 对用户pod进行sidecar注入.</p>
<p>进程为<code>sidecar-injector</code>, 主要监听端口:</p>
<ul>
<li><code>--port</code> Webhook服务端口,  默认443, 通过k8s service<code>istio-sidecar-injector</code> 对外提供服务.</li>
</ul>
<h3 id="3-3-istio-galley"><a href="#3-3-istio-galley" class="headerlink" title="3.3 istio-galley"></a>3.3 istio-galley</h3><p>包含一个单容器 <code>galley</code>: 提供 istio 中的配置管理服务, 验证Istio的CRD 资源的合法性.</p>
<p>进程为<code>galley server ......</code>, 主要监听端口:</p>
<ul>
<li><code>--server-address</code> galley gRPC 地址, 默认是tcp://0.0.0.0:9901</li>
<li><p><code>--validation-port</code>  https端口, 提供验证crd合法性服务的端口, 默认443.</p>
</li>
<li><p><code>--monitoringPort</code> http 端口, self-monitoring 端口, 默认 15014</p>
</li>
</ul>
<p>以上端口通过k8s service<code>istio-galley</code>对外提供服务</p>
<h3 id="3-4-istio-pilot"><a href="#3-4-istio-pilot" class="headerlink" title="3.4 istio-pilot"></a>3.4 istio-pilot</h3><p>pilot组件核心Pod, 对接平台适配层, 抽象服务注册信息、流量控制模型等, 封装统一的 API，供 Envoy 调用获取.</p>
<p>包含以下容器:</p>
<ol>
<li><p>sidecar container <code>istio-proxy</code></p>
</li>
<li><p>container <code>discovery</code>:  进程为<code>pilot-discovery discovery ......</code></p>
<p>主要监听端口:</p>
<ul>
<li>15010: 通过grpc 提供的 xds 获取接口</li>
<li><p>15011: 通过https 提供的 xds 获取接口</p>
</li>
<li><p>8080:  通过http 提供的 xds 获取接口, 兼容v1版本, 另外 http readiness 探针 <code>/ready</code>也在该端口</p>
</li>
<li><code>--monitoringPort</code> http self-monitoring 端口, 默认 15014</li>
</ul>
<p>以上端口通过k8s service<code>istio-pilot</code>对外提供服务</p>
</li>
</ol>
<h3 id="3-5-istio-telemetry-和istio-policy"><a href="#3-5-istio-telemetry-和istio-policy" class="headerlink" title="3.5 istio-telemetry 和istio-policy"></a>3.5 istio-telemetry 和istio-policy</h3><p>mixer 组件包含2个pod, istio-telemetry 和 istio-policy, istio-telemetry负责遥测功能,  istio-policy 负责策略控制, 它们分别包含2个容器:</p>
<ol>
<li><p>sidecar container<code>istio-proxy</code></p>
</li>
<li><p><code>mixer</code>: 进程为 <code>mixs server ……</code></p>
<p>主要监听端口:</p>
<ul>
<li>9091: grpc-mixer</li>
<li><p>15004: grpc-mixer-mtls</p>
</li>
<li><p><code>--monitoring-port</code>: http self-monitoring 端口, 默认 15014, liveness 探针<code>/version</code></p>
</li>
</ul>
</li>
</ol>
<h3 id="3-7-istio-citadel"><a href="#3-7-istio-citadel" class="headerlink" title="3.7 istio-citadel"></a>3.7 istio-citadel</h3><p>负责安全和证书管理的Pod, 包含一个单容器 <code>citadel</code></p>
<p>启动命令<code>/usr/local/bin/istio_ca --self-signed-ca ......</code> 主要监听端口:</p>
<ul>
<li><p><code>--grpc-port</code> citadel grpc 端口, 默认8060</p>
</li>
<li><p><code>--monitoring-port</code>: http self-monitoring 端口, 默认 15014, liveness 探针<code>/version</code></p>
</li>
</ul>
<p>以上端口通过k8s service<code>istio-citadel</code>对外提供服务</p>
<hr>
<p>后续将对各组件逐一进行分析.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/03/11/istio-analysis-1/" data-id="ck89nwpbg0007o8kb997amg78" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-servicemesh-istio" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/31/servicemesh-istio/" class="article-date">
  <time datetime="2019-01-31T07:30:00.000Z" itemprop="datePublished">2019-01-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/31/servicemesh-istio/">Istio 服务网格领域的新王者</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://github.com/zhongfox" target="_blank" rel="noopener">钟华</a></p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/istio/title.png" alt="istio"></p>
<p>今天分享的内容主要包括以下4个话题:</p>
<ul>
<li>1 Service Mesh: 下一代微服务</li>
<li>2 Istio: 第二代 Service Mesh</li>
<li>3 Istio 数据面</li>
<li>4 Istio 控制面</li>
</ul>
<p>首先我会和大家一起过一下 Service Mesh的发展历程, 并看看Istio 为 Service Mesh 带来了什么, 这部分相对比较轻松. 接下来我将和大家分析一下Istio的主要架构, 重点是数据面和控制面的实现, 包括sidecar的注入, 流量拦截, xDS介绍, Istio流量模型, 分布式跟踪, Mixer 的适配器模型等等, 中间也会穿插着 istio的现场使用demo.</p>
<hr>
<h1 id="1-Service-Mesh-下一代微服务"><a href="#1-Service-Mesh-下一代微服务" class="headerlink" title="1. Service Mesh: 下一代微服务"></a>1. Service Mesh: 下一代微服务</h1><ul>
<li>应用通信模式演进</li>
<li>Service Mesh(服务网格)的出现</li>
<li>第二代 Service Mesh</li>
<li>Service Mesh 的定义</li>
<li>Service Mesh 产品简史</li>
<li>国内Service Mesh 发展情况</li>
</ul>
<hr>
<h2 id="1-1-应用通信模式演进-网络流控进入操作系统"><a href="#1-1-应用通信模式演进-网络流控进入操作系统" class="headerlink" title="1.1 应用通信模式演进: 网络流控进入操作系统"></a>1.1 应用通信模式演进: 网络流控进入操作系统</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.1.png"></p>
<p>在计算机网络发展的初期, 开发人员需要在自己的代码中处理服务器之间的网络连接问题, 包括流量控制, 缓存队列, 数据加密等. 在这段时间内底层网络逻辑和业务逻辑是混杂在一起.</p>
<p>随着技术的发展，TCP/IP 等网络标准的出现解决了流量控制等问题。尽管网络逻辑代码依然存在，但已经从应用程序里抽离出来，成为操作系统网络层的一部分, 形成了经典的网络分层模式.</p>
<hr>
<h2 id="1-2-应用通信模式演进-微服务架构的出现"><a href="#1-2-应用通信模式演进-微服务架构的出现" class="headerlink" title="1.2 应用通信模式演进: 微服务架构的出现"></a>1.2 应用通信模式演进: 微服务架构的出现</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.2.png"></p>
<p>微服务架构是更为复杂的分布式系统，它给运维带来了更多挑战, 这些挑战主要包括资源的有效管理和服务之间的治理, 如:</p>
<ul>
<li>服务注册, 服务发现</li>
<li>服务伸缩</li>
<li>健康检查</li>
<li>快速部署</li>
<li>服务容错: 断路器, 限流, 隔离舱, 熔断保护, 服务降级等等</li>
<li>认证和授权</li>
<li>灰度发布方案</li>
<li>服务调用可观测性, 指标收集</li>
<li>配置管理</li>
</ul>
<p>在微服务架构的实现中，为提升效率和降低门槛，应用开发者会基于微服务框架来实现微服务。微服务框架一定程度上为使用者屏蔽了底层网络的复杂性及分布式场景下的不确定性。通过API/SDK的方式提供服务注册发现、服务RPC通信、服务配置管理、服务负载均衡、路由限流、容错、服务监控及治理、服务发布及升级等通用能力, 比较典型的产品有:</p>
<ul>
<li>分布式RPC通信框架: COBRA, WebServices, Thrift, GRPC 等</li>
<li>服务治理特定领域的类库和解决方案: Hystrix, Zookeeper, Zipkin, Sentinel 等</li>
<li>对多种方案进行整合的微服务框架: SpringCloud、Finagle、Dubbox 等</li>
</ul>
<p>实施微服务的成本往往会超出企业的预期(内容多, 门槛高), 花在服务治理上的时间成本甚至可能高过进行产品研发的时间. 另外上述的方案会限制可用的工具、运行时和编程语言。微服务软件库一般专注于某个平台, 这使得异构系统难以兼容, 存在重复的工作, 系统缺乏可移植性.</p>
<p>Docker 和Kubernetes 技术的流行, 为Pass资源的分配管理和服务的部署提供了新的解决方案, 但是微服务领域的其他服务治理问题仍然存在.</p>
<hr>
<h2 id="1-3-Sidecar-模式的兴起"><a href="#1-3-Sidecar-模式的兴起" class="headerlink" title="1.3 Sidecar 模式的兴起"></a>1.3 Sidecar 模式的兴起</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.3.png"></p>
<p>Sidecar(有时会叫做agent) 在原有的客户端和服务端之间加多了一个代理, 为应用程序提供的额外的功能, 如服务发现, 路由代理, 认证授权, 链路跟踪 等等.</p>
<p>业界使用Sidecar 的一些先例:</p>
<ul>
<li>2013 年，Airbnb 开发了Synapse 和 Nerve，是sidecar的一种开源实现</li>
<li>2014 年, Netflix 发布了Prana，它也是一个sidecar，可以让非 JVM 应用接入他们的 NetflixOSS 生态系统</li>
</ul>
<hr>
<h2 id="1-4-Service-Mesh-服务网格-的出现"><a href="#1-4-Service-Mesh-服务网格-的出现" class="headerlink" title="1.4 Service Mesh(服务网格)的出现"></a>1.4 Service Mesh(服务网格)的出现</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.4.png"></p>
<p>直观地看, Sidecar 到 Service Mesh 是一个规模的升级, 不过Service Mesh更强调的是:</p>
<ul>
<li>不再将Sidecar(代理)视为单独的组件，而是强调由这些代理连接而形成的网络</li>
<li>基础设施, 对应用程序透明</li>
</ul>
<hr>
<h2 id="1-5-Service-Mesh-定义"><a href="#1-5-Service-Mesh-定义" class="headerlink" title="1.5 Service Mesh 定义"></a>1.5 Service Mesh 定义</h2><p>以下是Linkerd的CEO <a href="https://twitter.com/wm" target="_blank" rel="noopener">Willian Morgan</a>给出的Service Mesh的定义:</p>
<blockquote>
<p>A Service Mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the Service Mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.</p>
</blockquote>
<p>服务网格（Service Mesh）是致力于解决服务间通讯的<strong>基础设施层</strong>。它负责在现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，Service Mesh 通常是通过一组<strong>轻量级网络代理</strong>（Sidecar proxy），与应用程序代码部署在一起来实现，且<strong>对应用程序透明</strong>。</p>
<hr>
<h2 id="1-6-第二代-Service-Mesh"><a href="#1-6-第二代-Service-Mesh" class="headerlink" title="1.6 第二代 Service Mesh"></a>1.6 第二代 Service Mesh</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.6.png"></p>
<p>控制面板对每一个代理实例了如指掌，通过控制面板可以实现代理的访问控制和度量指标收集, 提升了服务网格的可观测性和管控能力, Istio 正是这类系统最为突出的代表.</p>
<hr>
<h2 id="1-7-Service-Mesh-产品简史"><a href="#1-7-Service-Mesh-产品简史" class="headerlink" title="1.7 Service Mesh 产品简史"></a>1.7 Service Mesh 产品简史</h2><p><img src="https://zhongfox.github.io/assets/images/istio/1.7.png"></p>
<ul>
<li><p>2016 年 1 月 15 日，前 Twitter 的基础设施工程师 <a href="https://twitter.com/wm" target="_blank" rel="noopener">William Morgan</a> 和 Oliver Gould，在 GitHub 上发布了 Linkerd 0.0.7 版本，采用Scala编写, 他们同时组建了一个创业小公司 Buoyant，这是业界公认的第一个Service Mesh </p>
</li>
<li><p>2016 年，<a href="https://twitter.com/mattklein123" target="_blank" rel="noopener">Matt Klein</a>在 Lyft 默默地进行 Envoy 的开发。Envoy 诞生的时间其实要比 Linkerd 更早一些，只是在 Lyft 内部不为人所知</p>
</li>
<li><p>2016 年 9 月 29 日在 SF Microservices 上，“Service Mesh”这个词汇第一次在公开场合被使用。这标志着“Service Mesh”这个词，从 Buoyant 公司走向社区.</p>
</li>
<li><p>2016 年 9 月 13 日，Matt Klein 宣布 Envoy 在 GitHub 开源，直接发布 1.0.0 版本。</p>
</li>
<li><p>2016 年下半年，Linkerd 陆续发布了 0.8 和 0.9 版本，开始支持 HTTP/2 和 gRPC，1.0 发布在即；同时，借助 Service Mesh 在社区的认可度，Linkerd 在年底开始申请加入 CNCF</p>
</li>
<li><p>2017 年 1 月 23 日，Linkerd 加入 CNCF。</p>
</li>
<li><p>2017 年 3 月 7 日，Linkerd 宣布完成千亿次产品请求</p>
</li>
<li><p>2017 年 4 月 25 日，Linkerd 1.0 版本发布</p>
</li>
<li><p>2017 年 7 月 11 日，Linkerd 发布版本 1.1.1，宣布和 Istio 项目集成</p>
</li>
<li><p>2017 年 9 月, nginx突然宣布要搞出一个Servicemesh来, Nginmesh: <a href="https://github.com/nginxinc/nginmesh" target="_blank" rel="noopener">https://github.com/nginxinc/nginmesh</a>, 可以作为istio的数据面, 不过这个项目目前处于不活跃开发(This project is no longer under active development)</p>
</li>
<li><p>2017 年 12 月 5 日，Conduit 0.1.0 版本发布</p>
</li>
</ul>
<p>Envoy 和 Linkerd 都是在数据面上的实现, 属于同一个层面的竞争, 前者是用 C++ 语言实现的，在性能和资源消耗上要比采用 Scala 语言实现的 Linkerd 小，这一点对于延迟敏感型和资源敏的服务尤为重要.</p>
<p>Envoy 对 作为 Istio 的标准数据面实现, 其最主要的贡献是提供了一套<a href="https://github.com/envoyproxy/data-plane-api/blob/master/API_OVERVIEW.md" target="_blank" rel="noopener">标准数据面API</a>, 将服务信息和流量规则下发到数据面的sidecar中, 另外Envoy还支持热重启. Istio早期采用了Envoy v1 API，目前的版本中则使用V2 API，V1已被废弃.</p>
<p>通过采用该标准API，Istio将控制面和数据面进行了解耦，为多种数据面sidecar实现提供了可能性。事实上基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现.</p>
<p>将控制面和数据面解耦是Istio后来居上，风头超过Service mesh鼻祖Linkerd的一招妙棋。Istio站在了控制面的高度上，而Linkerd则成为了可选的一种sidecar实现.</p>
<p>Conduit 的整体架构和 Istio 一致，借鉴了 Istio 数据平面 + 控制平面的设计，而且选择了 Rust 编程语言来实现数据平面，以达成 Conduit 宣称的更轻、更快和超低资源占用.</p>
<center>(参考: 敖小剑 <a href="https://skyao.io/publication/201801-service-mesh-2017-summary/" target="_blank" rel="noopener">Service Mesh年度总结：群雄逐鹿烽烟起</a>)</center>

<hr>
<h2 id="1-8-似曾相识的竞争格局"><a href="#1-8-似曾相识的竞争格局" class="headerlink" title="1.8 似曾相识的竞争格局"></a>1.8 似曾相识的竞争格局</h2><table>
<thead>
<tr>
<th></th>
<th>Kubernetes</th>
<th>Istio</th>
</tr>
</thead>
<tbody>
<tr>
<td>领域</td>
<td>容器编排</td>
<td>服务网格</td>
</tr>
<tr>
<td>主要竞品</td>
<td>Swarm, Mesos</td>
<td>Linkerd, Conduit</td>
</tr>
<tr>
<td>主要盟友</td>
<td>RedHat, CoreOS</td>
<td>IBM, Lyft</td>
</tr>
<tr>
<td>主要竞争对手</td>
<td>Docker 公司</td>
<td>Buoyant 公司</td>
</tr>
<tr>
<td>标准化</td>
<td>OCI: runtime spec, image spec</td>
<td>XDS</td>
</tr>
<tr>
<td>插件化</td>
<td>CNI, CRI</td>
<td>Istio CNI, Mixer Adapter</td>
</tr>
<tr>
<td>结果</td>
<td>Kubernetes 成为容器编排事实标准</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>google 主导的Kubernetes 在容器编排领域取得了完胜, 目前在服务网格领域的打法如出一辙, 社区对Istio前景也比较看好.</p>
<p>Istio CNI 计划在1.1 作为实验特性, 用户可以通过扩展方式定制sidecar的网络.</p>
<hr>
<h2 id="1-9-国内Service-Mesh-发展情况"><a href="#1-9-国内Service-Mesh-发展情况" class="headerlink" title="1.9 国内Service Mesh 发展情况"></a>1.9 国内Service Mesh 发展情况</h2><ul>
<li><p>蚂蚁金服开源SOFAMesh: <a href="https://github.com/alipay/sofa-mesh" target="_blank" rel="noopener">https://github.com/alipay/sofa-mesh</a></p>
<ul>
<li>从istio fork</li>
<li>使用Golang语言开发全新的Sidecar，替代Envoy</li>
<li>为了避免Mixer带来的性能瓶颈，合并Mixer部分功能进入Sidecar</li>
<li>Pilot和Citadel模块进行了大幅的扩展和增强</li>
<li>扩展RPC协议: SOFARPC/HSF/Dubbo</li>
</ul>
</li>
<li><p>华为:</p>
<ul>
<li>go-chassis: <a href="https://github.com/go-chassis/go-chassis" target="_blank" rel="noopener">https://github.com/go-chassis/go-chassis</a> golang 微服务框架, 支持istio平台</li>
<li>mesher: <a href="https://github.com/go-mesh/mesher" target="_blank" rel="noopener">https://github.com/go-mesh/mesher</a> mesh 数据面解决方案</li>
<li>国内首家提供Service Mesh公共服务的云厂商</li>
<li>目前(2019年1月)公有云Istio 产品线上已经支持申请公测, 产品形态比较完善</li>
</ul>
</li>
<li><p>腾讯云 TSF:</p>
<ul>
<li>基于 Istio、envoy 进行改造</li>
<li>支持 Kubernetes、虚拟机以及裸金属的服务</li>
<li>对 Istio 的能力进行了扩展和增强, 对 Consul 的完整适配</li>
<li>对于其他二进制协议进行扩展支持</li>
</ul>
</li>
<li><p>唯品会</p>
<ul>
<li>OSP (Open Service Platform)</li>
</ul>
</li>
<li><p>新浪:</p>
<ul>
<li>Motan: 是一套基于java开发的RPC框架,  Weibo Mesh 是基于Motan</li>
</ul>
</li>
</ul>
<hr>
<h1 id="2-Istio-第二代-Service-Mesh"><a href="#2-Istio-第二代-Service-Mesh" class="headerlink" title="2. Istio: 第二代 Service Mesh"></a>2. Istio: 第二代 Service Mesh</h1><p>Istio来自希腊语，英文意思是「sail」, 意为「启航」</p>
<ul>
<li>2.1 Istio 架构</li>
<li>2.2 核心功能</li>
<li>2.3 Istio 演示: BookInfo</li>
</ul>
<hr>
<h2 id="2-1-Istio-架构"><a href="#2-1-Istio-架构" class="headerlink" title="2.1 Istio 架构"></a>2.1 Istio 架构</h2><p><img width="90%" src="https://preliminary.istio.io/docs/concepts/what-is-istio/arch.svg"></p>
<center>Istio Architecture（图片来自<a href="https://istio.io/docs/concepts/what-is-istio/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<ul>
<li><p>数据面</p>
<ul>
<li>Sidecar</li>
</ul>
</li>
<li><p>控制面</p>
<ul>
<li>Pilot：服务发现、流量管理</li>
<li>Mixer：访问控制、遥测</li>
<li>Citadel：终端用户认证、流量加密</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-2-核心功能"><a href="#2-2-核心功能" class="headerlink" title="2.2 核心功能"></a>2.2 核心功能</h3><ul>
<li>流量管理</li>
<li>安全</li>
<li>可观察性</li>
<li>多平台支持</li>
<li>集成和定制</li>
</ul>
<p>下面是我对Istio架构总结的思维导图:</p>
<p><img src="https://zhongfox.github.io/assets/images/istio/naotu2.png"></p>
<hr>
<h2 id="2-3-Istio-演示-BookInfo"><a href="#2-3-Istio-演示-BookInfo" class="headerlink" title="2.3 Istio 演示: BookInfo"></a>2.3 Istio 演示: BookInfo</h2><p>以下是Istio官网经典的 BookInfo Demo, 这是一个多语言组成的异构微服务系统:</p>
<p><img width="80%" src="https://istio.io/docs/examples/bookinfo/withistio.svg"></p>
<center>Bookinfo Application（图片来自<a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<p>下面我将现场给大家进行演示, 从demo安装开始, 并体验一下istio的流控功能:</p>
<h4 id="使用helm管理istio"><a href="#使用helm管理istio" class="headerlink" title="使用helm管理istio"></a>使用helm管理istio</h4><p>下载istio release: <a href="https://istio.io/docs/setup/kubernetes/download-release/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/download-release/</a></p>
<h5 id="安装istio"><a href="#安装istio" class="headerlink" title="安装istio:"></a>安装istio:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml</span><br><span class="line">helm install install/kubernetes/helm/istio --name istio --namespace istio-system</span><br></pre></td></tr></table></figure>
<p>注意事项, 若要开启sidecar自动注入功能, 需要:</p>
<ul>
<li>确保 kube-apiserver 启动参数 开启了ValidatingAdmissionWebhook 和 MutatingAdmissionWebhook</li>
<li>给namespace 增加 label: <code>kubectl label namespace default istio-injection=enabled</code></li>
<li>同时还要保证 kube-apiserver 的 aggregator layer 开启: <code>--enable-aggregator-routing=true</code> 且证书和api server连通性正确设置.</li>
</ul>
<h5 id="如需卸载istio"><a href="#如需卸载istio" class="headerlink" title="如需卸载istio:"></a>如需卸载istio:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm delete --purge istio</span><br><span class="line">kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system</span><br></pre></td></tr></table></figure>
<p>更多安装选择请参考: <a href="https://istio.io/docs/setup/kubernetes/helm-install/" target="_blank" rel="noopener">https://istio.io/docs/setup/kubernetes/helm-install/</a></p>
<h4 id="安装Bookinfo-Demo"><a href="#安装Bookinfo-Demo" class="headerlink" title="安装Bookinfo Demo:"></a>安装Bookinfo Demo:</h4><p>Bookinfo 是一个多语言异构的微服务demo, 其中 productpage 微服务会调用 details 和 reviews 两个微服务, reviews 会调用ratings 微服务, reviews 微服务有 3 个版本. 关于此项目更多细节请参考: <a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">https://istio.io/docs/examples/bookinfo/</a></p>
<h5 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用:"></a>部署应用:</h5><p><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</code></p>
<p>这将创建 productpage, details, ratings, reviews 对应的deployments 和 service, 其中reviews 有三个deployments, 代表三个不同的版本.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> % kubectl get pod</span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE</span><br><span class="line">details-v1-6865b9b99d-mnxbt    2/2       Running   0          1m</span><br><span class="line">productpage-v1-f8c8fb8-zjbhh   2/2       Running   0          59s</span><br><span class="line">ratings-v1-77f657f55d-95rcz    2/2       Running   0          1m</span><br><span class="line">reviews-v1-6b7f6db5c5-zqvkn    2/2       Running   0          59s</span><br><span class="line">reviews-v2-7ff5966b99-lw72l    2/2       Running   0          59s</span><br><span class="line">reviews-v3-5df889bcff-w9v7g    2/2       Running   0          59s</span><br><span class="line"></span><br><span class="line"> % kubectl get svc</span><br><span class="line">NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">details       ClusterIP   172.18.255.240   &lt;none&gt;        9080/TCP   1m</span><br><span class="line">productpage   ClusterIP   172.18.255.137   &lt;none&gt;        9080/TCP   1m</span><br><span class="line">ratings       ClusterIP   172.18.255.41    &lt;none&gt;        9080/TCP   1m</span><br><span class="line">reviews       ClusterIP   172.18.255.140   &lt;none&gt;        9080/TCP   1m</span><br></pre></td></tr></table></figure>
<p>对入口流量进行配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span><br></pre></td></tr></table></figure>
<p>该操作会创建bookinfo-gateway 的Gateway, 并将流量发送到productpage服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get gateway</span><br><span class="line">NAME               AGE</span><br><span class="line">bookinfo-gateway   1m</span><br></pre></td></tr></table></figure>
<p>此时通过bookinfo-gateway 对应的LB或者nodeport 访问/productpage 页面, 可以看到三个版本的reviews服务在随机切换</p>
<h4 id="基于权重的路由"><a href="#基于权重的路由" class="headerlink" title="基于权重的路由"></a>基于权重的路由</h4><p>通过CRD DestinationRule创建3 个reviews 子版本:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/destination-rule-reviews.yaml</span><br></pre></td></tr></table></figure>
<p>通过CRD VirtualService 调整个 reviews 服务子版本的流量比例, 设置 v1 和 v3 各占 50%</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml</span><br></pre></td></tr></table></figure>
<p>刷新页面, 可以看到无法再看到reviews v2的内容, 页面在v1和v3之间切换.</p>
<h4 id="基于内容路由"><a href="#基于内容路由" class="headerlink" title="基于内容路由"></a>基于内容路由</h4><p>修改reviews CRD, 将jason 登录的用户版本路由到v2, 其他用户路由到版本v3.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-jason-v2-v3.yaml</span><br></pre></td></tr></table></figure>
<p>刷新页面, 使用jason登录的用户, 将看到v2 黑色星星版本, 其他用户将看到v3 红色星星版本.</p>
<p>更多BookInfo 示例, 请参阅: <a href="https://istio.io/docs/examples/bookinfo/" target="_blank" rel="noopener">https://istio.io/docs/examples/bookinfo/</a>, 若要删除应用: 执行脚本 <code>./samples/bookinfo/platform/kube/cleanup.sh</code></p>
<hr>
<h1 id="3-Istio-数据面"><a href="#3-Istio-数据面" class="headerlink" title="3. Istio 数据面"></a>3. Istio 数据面</h1><ul>
<li>3.1 数据面组件</li>
<li>3.2 sidecar 流量劫持原理</li>
<li>3.3 数据面标准API: xDS</li>
<li>3.4 分布式跟踪</li>
</ul>
<h2 id="3-1-数据面组件"><a href="#3-1-数据面组件" class="headerlink" title="3.1 数据面组件"></a>3.1 数据面组件</h2><p>Istio 注入sidecar实现:</p>
<ul>
<li>自动注入: 利用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">Kubernetes Dynamic Admission Webhooks</a> 对 新建的pod 进行注入: init container + sidecar</li>
<li>手动注入: 使用<code>istioctl kube-inject</code></li>
</ul>
<p>注入Pod内容:</p>
<ul>
<li>istio-init: 通过配置iptables来劫持Pod中的流量</li>
<li>istio-proxy: 两个进程pilot-agent和envoy, pilot-agent 进行初始化并启动envoy</li>
</ul>
<h4 id="Sidecar-自动注入实现"><a href="#Sidecar-自动注入实现" class="headerlink" title="Sidecar 自动注入实现"></a>Sidecar 自动注入实现</h4><p>Istio 利用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">Kubernetes Dynamic Admission Webhooks</a> 对pod 进行sidecar注入</p>
<p>查看istio 对这2个Webhooks 的配置 ValidatingWebhookConfiguration 和 MutatingWebhookConfiguration:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% kubectl get ValidatingWebhookConfiguration -oyaml</span><br><span class="line">% kubectl get mutatingWebhookConfiguration -oyaml</span><br></pre></td></tr></table></figure>
<p>可以看出:</p>
<ul>
<li>命名空间<code>istio-system</code> 中的服务 <code>istio-galley</code>, 通过路由<code>/admitpilot</code>, 处理config.istio.io部分, rbac.istio.io, authentication.istio.io, networking.istio.io等资源的Validating 工作</li>
<li>命名空间istio-system 中的服务 <code>istio-galley</code>, 通过路由<code>/admitmixer</code>, 处理其他config.istio.io资源的Validating 工作</li>
<li>命名空间istio-system 中的服务 <code>istio-sidecar-injector</code>, 通过路由<code>/inject</code>, 处理其他<code>v1/pods</code>的CREATE, 同时需要满足命名空间<code>istio-injection: enabled</code></li>
</ul>
<h4 id="istio-init"><a href="#istio-init" class="headerlink" title="istio-init"></a>istio-init</h4><p>数据面的每个Pod会被注入一个名为<code>istio-init</code> 的initContainer, initContrainer是K8S提供的机制，用于在Pod中执行一些初始化任务.在Initialcontainer执行完毕并退出后，才会启动Pod中的其它container.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">initContainers:</span><br><span class="line">- image: docker.io/istio/proxy_init:1.0.5</span><br><span class="line">  args:</span><br><span class="line">  - -p</span><br><span class="line">  - &quot;15001&quot;</span><br><span class="line">  - -u</span><br><span class="line">  - &quot;1337&quot;</span><br><span class="line">  - -m</span><br><span class="line">  - REDIRECT</span><br><span class="line">  - -i</span><br><span class="line">  - &apos;*&apos;</span><br><span class="line">  - -x</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  - -b</span><br><span class="line">  - &quot;9080&quot;</span><br><span class="line">  - -d</span><br><span class="line">  - &quot;&quot;</span><br></pre></td></tr></table></figure>
<p>istio-init ENTRYPOINT 和 args 组合的启动命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/bin/istio-iptables.sh -p 15001 -u 1337 -m REDIRECT -i &apos;*&apos; -x &quot;&quot; -b 9080 -d &quot;&quot;</span><br></pre></td></tr></table></figure>
<p>istio-iptables.sh 源码地址为 <a href="https://github.com/istio/istio/blob/master/tools/deb/istio-iptables.sh" target="_blank" rel="noopener">https://github.com/istio/istio/blob/master/tools/deb/istio-iptables.sh</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ istio-iptables.sh -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h]</span><br><span class="line">  -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001）</span><br><span class="line">  -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337）</span><br><span class="line">  -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值）</span><br><span class="line">  -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE)</span><br><span class="line">  -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS）</span><br><span class="line">  -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS）</span><br><span class="line">  -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR）</span><br><span class="line">  -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。</span><br><span class="line"></span><br><span class="line">环境变量位于 $ISTIO_SIDECAR_CONFIG（默认在：/var/lib/istio/envoy/sidecar.env）</span><br></pre></td></tr></table></figure>
<p>istio-init 通过配置iptable来劫持Pod中的流量:</p>
<ul>
<li>参数<code>-p 15001</code>: Pod中的数据流量被iptable拦截，并发向15001端口, 该端口将由 envoy 监听</li>
<li>参数<code>-u 1337</code>: 用于排除用户ID为1337，即Envoy自身的流量，以避免Iptable把Envoy发出的数据又重定向到Envoy, UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户, 见Sidecar <code>istio-proxy</code> 配置参数<code>securityContext.runAsUser</code></li>
<li>参数<code>-b 9080</code> <code>-d &quot;&quot;</code>: 入站端口控制, 将所有访问 9080 端口（即应用容器的端口）的流量重定向到 Envoy 代理</li>
<li>参数<code>-i &#39;*&#39;</code> <code>-x &quot;&quot;</code>: 出站IP控制, 将所有出站流量都重定向到 Envoy 代理</li>
</ul>
<p>Init 容器初始化完毕后就会自动终止，但是 Init 容器初始化结果(iptables)会保留到应用容器和 Sidecar 容器中.</p>
<h4 id="istio-proxy"><a href="#istio-proxy" class="headerlink" title="istio-proxy"></a>istio-proxy</h4><p>istio-proxy 以 sidecar 的形式注入到应用容器所在的pod中, 简化的注入yaml:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">- image: docker.io/istio/proxyv2:1.0.5</span><br><span class="line">  name: istio-proxy</span><br><span class="line">  args:</span><br><span class="line">  - proxy</span><br><span class="line">  - sidecar</span><br><span class="line">  - --configPath</span><br><span class="line">  - /etc/istio/proxy</span><br><span class="line">  - --binaryPath</span><br><span class="line">  - /usr/local/bin/envoy</span><br><span class="line">  - --serviceCluster</span><br><span class="line">  - ratings</span><br><span class="line">  - --drainDuration</span><br><span class="line">  - 45s</span><br><span class="line">  - --parentShutdownDuration</span><br><span class="line">  - 1m0s</span><br><span class="line">  - --discoveryAddress</span><br><span class="line">  - istio-pilot.istio-system:15007</span><br><span class="line">  - --discoveryRefreshDelay</span><br><span class="line">  - 1s</span><br><span class="line">  - --zipkinAddress</span><br><span class="line">  - zipkin.istio-system:9411</span><br><span class="line">  - --connectTimeout</span><br><span class="line">  - 10s</span><br><span class="line">  - --proxyAdminPort</span><br><span class="line">  - &quot;15000&quot;</span><br><span class="line">  - --controlPlaneAuthPolicy</span><br><span class="line">  - NONE</span><br><span class="line">  env:</span><br><span class="line">    ......</span><br><span class="line">  ports:</span><br><span class="line">  - containerPort: 15090</span><br><span class="line">    name: http-envoy-prom</span><br><span class="line">    protocol: TCP</span><br><span class="line">  securityContext:</span><br><span class="line">    runAsUser: 1337</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>istio-proxy容器中有两个进程pilot-agent和envoy:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~ % kubectl exec productpage-v1-f8c8fb8-wgmzk -c istio-proxy -- ps -ef</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">istio-p+     1     0  0 Jan03 ?        00:00:27 /usr/local/bin/pilot-agent proxy sidecar --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15007 --discoveryRefreshDelay 1s --zipkinAddress zipkin.istio-system:9411 --connectTimeout 10s --proxyAdminPort 15000 --controlPlaneAuthPolicy NONE</span><br><span class="line">istio-p+    21     1  0 Jan03 ?        01:26:24 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage --service-node sidecar~172.18.3.12~productpage-v1-f8c8fb8-wgmzk.default~default.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warn --v2-config-only</span><br></pre></td></tr></table></figure>
<p>可以看到:</p>
<ul>
<li><code>/usr/local/bin/pilot-agent</code> 是 <code>/usr/local/bin/envoy</code> 的父进程, Pilot-agent进程根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件(<code>/etc/istio/proxy/envoy-rev0.json</code>)，并负责启动Envoy进程</li>
<li>pilot-agent 的启动参数里包括: discoveryAddress(pilot服务地址), Envoy 二进制文件的位置, 服务集群名, 监控指标上报地址, Envoy 的管理端口, 热重启时间等</li>
</ul>
<p>Envoy配置初始化流程:</p>
<ol>
<li>Pilot-agent根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件envoy-rev0.json，该文件告诉Envoy从xDS server中获取动态配置信息，并配置了xDS server的地址信息，即控制面的Pilot</li>
<li>Pilot-agent使用envoy-rev0.json启动Envoy进程</li>
<li>Envoy根据初始配置获得Pilot地址，采用xDS接口从Pilot获取到Listener，Cluster，Route等d动态配置信息</li>
<li>Envoy根据获取到的动态配置启动Listener，并根据Listener的配置，结合Route和Cluster对拦截到的流量进行处理</li>
</ol>
<p>查看envoy 初始配置文件:</p>
<p><code>kubectl exec productpage-v1-f8c8fb8-wgmzk -c istio-proxy -- cat /etc/istio/proxy/envoy-rev0.json</code></p>
<hr>
<h2 id="3-2-sidecar-流量劫持原理"><a href="#3-2-sidecar-流量劫持原理" class="headerlink" title="3.2 sidecar 流量劫持原理"></a>3.2 sidecar 流量劫持原理</h2><p>sidecar 既要作为服务消费者端的正向代理，又要作为服务提供者端的反向代理, 具体拦截过程如下:</p>
<ul>
<li><p>Pod 所在的network namespace内, 除了envoy发出的流量外, iptables规则会对进入和发出的流量都进行拦截，通过nat redirect重定向到Envoy监听的15001端口.</p>
</li>
<li><p>envoy 会根据从Pilot拿到的 XDS 规则, 对流量进行转发.</p>
</li>
<li><p>envoy 的 listener 0.0.0.0:15001 接收进出 Pod 的所有流量，然后将请求移交给对应的virtual listener</p>
</li>
<li><p>对于本pod的服务, 有一个http listener <code>podIP+端口</code> 接受inbound 流量</p>
</li>
<li><p>每个service+非http端口, 监听器配对的 Outbound 非 HTTP 流量</p>
</li>
<li><p>每个service+http端口, 有一个http listener: <code>0.0.0.0+端口</code> 接受outbound流量</p>
</li>
</ul>
<p>整个拦截转发过程对业务容器是透明的, 业务容器仍然使用 Service 域名和端口进行通信, service 域名仍然会转换为service IP, 但service IP 在sidecar 中会被直接转换为 pod IP, 从容器中出去的流量已经使用了pod IP会直接转发到对应的Pod, 对比传统kubernetes 服务机制, service IP 转换为Pod IP 在node上进行, 由 kube-proxy维护的iptables实现.</p>
<hr>
<h2 id="3-3-数据面标准API-xDS"><a href="#3-3-数据面标准API-xDS" class="headerlink" title="3.3 数据面标准API: xDS"></a>3.3 数据面标准API: xDS</h2><p>xDS是一类发现服务的总称，包含LDS，RDS，CDS，EDS以及 SDS。Envoy通过xDS API可以动态获取Listener(监听器)， Route(路由)，Cluster(集群)，Endpoint(集群成员)以 及Secret(证书)配置</p>
<p>xDS API 涉及的概念:</p>
<ul>
<li>Host</li>
<li>Downstream</li>
<li>Upstream</li>
<li>Listener</li>
<li>Cluster</li>
</ul>
<p>Envoy 配置热更新: 配置的动态变更，而不需要重启 Envoy:</p>
<ol>
<li>新老进程采用基本的RPC协议使用Unix Domain Socket通讯.</li>
<li>新进程启动并完成所有初始化工作后，向老进程请求监听套接字的副本.</li>
<li>新进程接管套接字后，通知老进程关闭套接字.</li>
<li>通知老进程终止自己.</li>
</ol>
<h4 id="xDS-调试"><a href="#xDS-调试" class="headerlink" title="xDS 调试"></a>xDS 调试</h4><p>Pilot在9093端口提供了下述调试接口:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># What is sent to envoy</span><br><span class="line"># Listeners and routes</span><br><span class="line">curl $PILOT/debug/adsz</span><br><span class="line"></span><br><span class="line"># Endpoints</span><br><span class="line">curl $PILOT/debug/edsz</span><br><span class="line"></span><br><span class="line"># Clusters</span><br><span class="line">curl $PILOT/debug/cdsz</span><br></pre></td></tr></table></figure>
<p>Sidecar Envoy 也提供了管理接口，缺省为localhost的15000端口，可以获取listener，cluster以及完整的配置数据</p>
<p>可以通过以下命令查看支持的调试接口:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec productpage-v1-f8c8fb8-zjbhh -c istio-proxy curl http://127.0.0.1:15000/help</span><br></pre></td></tr></table></figure>
<p>或者forward到本地就行调试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl port-forward productpage-v1-f8c8fb8-zjbhh 15000</span><br></pre></td></tr></table></figure>
<p>相关的调试接口:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">http://127.0.0.1:15000</span><br><span class="line">http://127.0.0.1:15000/help</span><br><span class="line">http://127.0.0.1:15000/config_dump</span><br><span class="line">http://127.0.0.1:15000/listeners</span><br><span class="line">http://127.0.0.1:15000/clusters</span><br></pre></td></tr></table></figure>
<p>使用istioctl 查看代理配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">istioctl pc &#123;xDS类型&#125;  &#123;POD_NAME&#125; &#123;过滤条件&#125; &#123;-o json/yaml&#125;</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">istioctl pc routes productpage-v1-f8c8fb8-zjbhh --name 9080 -o json</span><br></pre></td></tr></table></figure>
<p>xDS 类型包括: listener, route, cluster, endpoint</p>
<h4 id="对xDS-进行分析-productpage-访问-reviews-服务"><a href="#对xDS-进行分析-productpage-访问-reviews-服务" class="headerlink" title="对xDS 进行分析: productpage 访问 reviews 服务"></a>对xDS 进行分析: productpage 访问 reviews 服务</h4><p>查看 product 的所有listener:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc listener  productpage-v1-f8c8fb8-zjbhh</span><br><span class="line">ADDRESS            PORT      TYPE</span><br><span class="line">172.18.255.178     15011     TCP</span><br><span class="line">172.18.255.194     44134     TCP</span><br><span class="line">172.18.255.110     443       TCP</span><br><span class="line">172.18.255.190     50000     TCP</span><br><span class="line">172.18.255.203     853       TCP</span><br><span class="line">172.18.255.2       443       TCP</span><br><span class="line">172.18.255.239     16686     TCP</span><br><span class="line">0.0.0.0            80        TCP</span><br><span class="line">172.18.255.215     3306      TCP</span><br><span class="line">172.18.255.203     31400     TCP</span><br><span class="line">172.18.255.111     443       TCP</span><br><span class="line">172.18.255.203     8060      TCP</span><br><span class="line">172.18.255.203     443       TCP</span><br><span class="line">172.18.255.40      443       TCP</span><br><span class="line">172.18.255.1       443       TCP</span><br><span class="line">172.18.255.53      53        TCP</span><br><span class="line">172.18.255.203     15011     TCP</span><br><span class="line">172.18.255.105     14268     TCP</span><br><span class="line">172.18.255.125     42422     TCP</span><br><span class="line">172.18.255.105     14267     TCP</span><br><span class="line">172.18.255.52      80        TCP</span><br><span class="line">0.0.0.0            15010     HTTP</span><br><span class="line">0.0.0.0            9411      HTTP</span><br><span class="line">0.0.0.0            8060      HTTP</span><br><span class="line">0.0.0.0            9080      HTTP</span><br><span class="line">0.0.0.0            15004     HTTP</span><br><span class="line">0.0.0.0            20001     HTTP</span><br><span class="line">0.0.0.0            9093      HTTP</span><br><span class="line">0.0.0.0            8080      HTTP</span><br><span class="line">0.0.0.0            15030     HTTP</span><br><span class="line">0.0.0.0            9091      HTTP</span><br><span class="line">0.0.0.0            9090      HTTP</span><br><span class="line">0.0.0.0            15031     HTTP</span><br><span class="line">0.0.0.0            3000      HTTP</span><br><span class="line">0.0.0.0            15001     TCP</span><br><span class="line">172.18.3.50        9080      HTTP 这是当前pod ip 暴露的服务地址, 会路由到回环地址, 各个pod 会不一样</span><br></pre></td></tr></table></figure>
<p>envoy 流量入口的listener:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc listener  productpage-v1-f8c8fb8-zjbhh --address 0.0.0.0 --port 15001 -o json</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;virtual&quot;,</span><br><span class="line">        &quot;address&quot;: &#123;</span><br><span class="line">            &quot;socketAddress&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;0.0.0.0&quot;,</span><br><span class="line">                &quot;portValue&quot;: 15001</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;filterChains&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;filters&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;name&quot;: &quot;envoy.tcp_proxy&quot;,</span><br><span class="line">                        &quot;config&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;BlackHoleCluster&quot;,</span><br><span class="line">                            &quot;stat_prefix&quot;: &quot;BlackHoleCluster&quot;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;useOriginalDst&quot;: true # 这意味着它将请求交给最符合请求原始目标的监听器。如果找不到任何匹配的虚拟监听器，它会将请求发送给返回 404 的 BlackHoleCluster</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>以下是reviews的所有pod IP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> % kubectl get ep reviews</span><br><span class="line">NAME      ENDPOINTS                                            AGE</span><br><span class="line">reviews   172.18.2.35:9080,172.18.3.48:9080,172.18.3.49:9080   1d</span><br></pre></td></tr></table></figure>
<p>对于目的地址是以上ip的http访问, 这些 ip 并没有对应的listener, 因此会通过端口9080 匹配到listener <code>0.0.0.0 9080</code></p>
<p>查看listener <code>0.0.0.0 9080</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc listener  productpage-v1-f8c8fb8-zjbhh --address 0.0.0.0 --port 9080 -ojson</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;0.0.0.0_9080&quot;,</span><br><span class="line">        &quot;address&quot;: &#123;</span><br><span class="line">            &quot;socketAddress&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;0.0.0.0&quot;,</span><br><span class="line">                &quot;portValue&quot;: 9080</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">                            &quot;rds&quot;: &#123;</span><br><span class="line">                                &quot;config_source&quot;: &#123;</span><br><span class="line">                                    &quot;ads&quot;: &#123;&#125;</span><br><span class="line">                                &#125;,</span><br><span class="line">                                &quot;route_config_name&quot;: &quot;9080&quot;</span><br><span class="line">                            &#125;,</span><br><span class="line">                            ......</span><br></pre></td></tr></table></figure>
<p>查看名为<code>9080</code> 的 route:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc routes  productpage-v1-f8c8fb8-zjbhh --name 9080 -o json</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;9080&quot;,</span><br><span class="line">        &quot;virtualHosts&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;details.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;details.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;details.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;details&quot;,</span><br><span class="line">                    &quot;details:9080&quot;,</span><br><span class="line">                    &quot;details.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;details.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;details.default.svc&quot;,</span><br><span class="line">                    &quot;details.default.svc:9080&quot;,</span><br><span class="line">                    &quot;details.default&quot;,</span><br><span class="line">                    &quot;details.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.240&quot;,</span><br><span class="line">                    &quot;172.18.255.240:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;match&quot;: &#123;</span><br><span class="line">                            &quot;prefix&quot;: &quot;/&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;route&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;outbound|9080||details.default.svc.cluster.local&quot;,</span><br><span class="line">                            &quot;timeout&quot;: &quot;0.000s&quot;,</span><br><span class="line">                            &quot;maxGrpcTimeout&quot;: &quot;0.000s&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        ......</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;productpage.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;productpage.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;productpage.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;productpage&quot;,</span><br><span class="line">                    &quot;productpage:9080&quot;,</span><br><span class="line">                    &quot;productpage.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;productpage.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;productpage.default.svc&quot;,</span><br><span class="line">                    &quot;productpage.default.svc:9080&quot;,</span><br><span class="line">                    &quot;productpage.default&quot;,</span><br><span class="line">                    &quot;productpage.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.137&quot;,</span><br><span class="line">                    &quot;172.18.255.137:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [ ...... ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;ratings.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;ratings.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;ratings.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;ratings&quot;,</span><br><span class="line">                    &quot;ratings:9080&quot;,</span><br><span class="line">                    &quot;ratings.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;ratings.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;ratings.default.svc&quot;,</span><br><span class="line">                    &quot;ratings.default.svc:9080&quot;,</span><br><span class="line">                    &quot;ratings.default&quot;,</span><br><span class="line">                    &quot;ratings.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.41&quot;,</span><br><span class="line">                    &quot;172.18.255.41:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [ ...... ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;name&quot;: &quot;reviews.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                &quot;domains&quot;: [</span><br><span class="line">                    &quot;reviews.default.svc.cluster.local&quot;,</span><br><span class="line">                    &quot;reviews.default.svc.cluster.local:9080&quot;,</span><br><span class="line">                    &quot;reviews&quot;,</span><br><span class="line">                    &quot;reviews:9080&quot;,</span><br><span class="line">                    &quot;reviews.default.svc.cluster&quot;,</span><br><span class="line">                    &quot;reviews.default.svc.cluster:9080&quot;,</span><br><span class="line">                    &quot;reviews.default.svc&quot;,</span><br><span class="line">                    &quot;reviews.default.svc:9080&quot;,</span><br><span class="line">                    &quot;reviews.default&quot;,</span><br><span class="line">                    &quot;reviews.default:9080&quot;,</span><br><span class="line">                    &quot;172.18.255.140&quot;,</span><br><span class="line">                    &quot;172.18.255.140:9080&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;routes&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;match&quot;: &#123;</span><br><span class="line">                            &quot;prefix&quot;: &quot;/&quot;,</span><br><span class="line">                            &quot;headers&quot;: [</span><br><span class="line">                                &#123;</span><br><span class="line">                                    &quot;name&quot;: &quot;end-user&quot;,</span><br><span class="line">                                    &quot;exactMatch&quot;: &quot;jason&quot;</span><br><span class="line">                                &#125;</span><br><span class="line">                            ]</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;route&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;outbound|9080|v2|reviews.default.svc.cluster.local&quot;,</span><br><span class="line">                            &quot;timeout&quot;: &quot;0.000s&quot;,</span><br><span class="line">                            &quot;maxGrpcTimeout&quot;: &quot;0.000s&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        ......</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;match&quot;: &#123;</span><br><span class="line">                            &quot;prefix&quot;: &quot;/&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;route&quot;: &#123;</span><br><span class="line">                            &quot;cluster&quot;: &quot;outbound|9080|v3|reviews.default.svc.cluster.local&quot;,</span><br><span class="line">                            &quot;timeout&quot;: &quot;0.000s&quot;,</span><br><span class="line">                            &quot;maxGrpcTimeout&quot;: &quot;0.000s&quot;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        .......</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;validateClusters&quot;: false</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>可以看到, 在9080 这个route 中, 包含所有这个端口的http 路由信息, 通过virtualHosts列表进行服务域名分发到各个cluster.</p>
<p>查看virtualHosts <code>reviews.default.svc.cluster.local:9080</code> 中的routes信息, 可以看到jason 路由到了cluster <code>outbound|9080|v2|reviews.default.svc.cluster.local</code></p>
<p>查看该cluster:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">% istioctl pc cluster productpage-v1-f8c8fb8-zjbhh --fqdn reviews.default.svc.cluster.local --subset v2 -o json</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;outbound|9080|v2|reviews.default.svc.cluster.local&quot;,</span><br><span class="line">        &quot;type&quot;: &quot;EDS&quot;,</span><br><span class="line">        &quot;edsClusterConfig&quot;: &#123;</span><br><span class="line">            &quot;edsConfig&quot;: &#123;</span><br><span class="line">                &quot;ads&quot;: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;serviceName&quot;: &quot;outbound|9080|v2|reviews.default.svc.cluster.local&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;connectTimeout&quot;: &quot;1.000s&quot;,</span><br><span class="line">        &quot;lbPolicy&quot;: &quot;RANDOM&quot;,</span><br><span class="line">        &quot;circuitBreakers&quot;: &#123;</span><br><span class="line">            &quot;thresholds&quot;: [</span><br><span class="line">                &#123;&#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>查看其对应的endpoint:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> % istioctl pc endpoint productpage-v1-f8c8fb8-zjbhh --cluster &apos;outbound|9080|v2|reviews.default.svc.cluster.local&apos;</span><br><span class="line">ENDPOINT             STATUS      CLUSTER</span><br><span class="line">172.18.2.35:9080     HEALTHY     outbound|9080|v2|reviews.default.svc.cluster.local</span><br></pre></td></tr></table></figure>
<p>该endpoint 即为 reviews 服务 V2 对应的 pod IP</p>
<h4 id="XDS服务接口的最终一致性考虑"><a href="#XDS服务接口的最终一致性考虑" class="headerlink" title="XDS服务接口的最终一致性考虑"></a>XDS服务接口的最终一致性考虑</h4><p>遵循 make before break 模型</p>
<hr>
<h2 id="3-4-分布式跟踪"><a href="#3-4-分布式跟踪" class="headerlink" title="3.4 分布式跟踪"></a>3.4 分布式跟踪</h2><p>以下是分布式全链路跟踪示意图:</p>
<p><img width="40%" style="float: left" src="https://zhongfox.github.io/assets/images/hunter/opentracing_1.png"><br><img width="60%" style="float: left;margin-top:2cm;" src="https://zhongfox.github.io/assets/images/hunter/opentracing_2.png"></p>
<center>一个典型的Trace案例（图片来自<a href="https://wu-sheng.gitbooks.io/opentracing-io/content/" target="_blank" rel="noopener">opentracing文档中文版</a>)</center>

<hr>
<p>Jaeger 是Uber 开源的全链路跟踪系统, 符合OpenTracing协议, OpenTracing 和 Jaeger 均是CNCF 成员项目, 以下是Jaeger 架构的示意图:</p>
<p><img src="https://www.jaegertracing.io/img/architecture.png"></p>
<center>Jaeger 架构示意图（图片来自<a href="https://www.jaegertracing.io/docs/1.6/architecture/" target="_blank" rel="noopener">Jaeger官方文档</a>)</center>

<p>分布式跟踪系统让开发者能够得到可视化的调用流程展示。这对复杂的微服务系统进行问题排查和性能优化时至关重要.</p>
<p>Envoy 原生支持http 链路跟踪:</p>
<ul>
<li>生成 Request ID：Envoy 会在需要的时候生成 UUID，并操作名为 [x-request-id] 的 HTTP Header。应用可以转发这个 Header 用于统一的记录和跟踪.</li>
<li>支持集成外部跟踪服务：Envoy 支持可插接的外部跟踪可视化服务。目前支持有:<ul>
<li>LightStep</li>
<li>Zipkin 或者 Zipkin 兼容的后端（比如说 Jaeger）</li>
<li>Datadog</li>
</ul>
</li>
<li>客户端跟踪 ID 连接：x-client-trace-id Header 可以用来把不信任的请求 ID 连接到受信的 x-request-id Header 上</li>
</ul>
<h4 id="跟踪上下文信息的传播"><a href="#跟踪上下文信息的传播" class="headerlink" title="跟踪上下文信息的传播"></a>跟踪上下文信息的传播</h4><ul>
<li>不管使用的是哪个跟踪服务，都应该传播 x-request-id，这样在被调用服务中启动相关性的记录</li>
<li>如果使用的是 Zipkin，Envoy 要传播的是 <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#config-http-conn-man-headers-b3" target="_blank" rel="noopener">B3 Header</a>。（x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, 以及 x-b3-flags. x-b3-sampled）</li>
<li>上下文跟踪并非零修改, 在调用下游服务时, 上游应用应该自行传播跟踪相关的 HTTP Header</li>
</ul>
<hr>
<h1 id="4-Istio-控制面"><a href="#4-Istio-控制面" class="headerlink" title="4. Istio 控制面"></a>4. Istio 控制面</h1><ul>
<li>4.1 Pilot 架构</li>
<li>4.2 流量管理模型</li>
<li>4.3 故障处理</li>
<li>4.4 Mixer 架构</li>
<li>4.5 Mixer适配器模型</li>
<li>4.6 Mixer 缓存机制</li>
</ul>
<hr>
<h2 id="4-1-Pilot-架构"><a href="#4-1-Pilot-架构" class="headerlink" title="4.1 Pilot 架构"></a>4.1 Pilot 架构</h2><p><img src="https://preliminary.istio.io/docs/concepts/traffic-management/PilotAdapters.svg"></p>
<center>Pilot Architecture（图片来自<a href="https://istio.io/docs/concepts/traffic-management/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<ul>
<li>Rules API: 对外封装统一的 API，供服务的开发者或者运维人员调用，可以用于流量控制。</li>
<li>Envoy API: 对内封装统一的 API，供 Envoy 调用以获取注册信息、流量控制信息等。</li>
<li>抽象模型层: 对服务的注册信息、流量控制规则等进行抽象，使其描述与平台无关。</li>
<li>平台适配层: 用于适配各个平台如 Kubernetes、Mesos、Cloud Foundry 等，把平台特定的注册信息、资源信息等转换成抽象模型层定义的平台无关的描述。例如，Pilot 中的 Kubernetes 适配器实现必要的控制器来 watch Kubernetes API server 中 pod 注册信息、ingress 资源以及用于存储流量管理规则的第三方资源的更改</li>
</ul>
<hr>
<h2 id="4-2-流量管理模型"><a href="#4-2-流量管理模型" class="headerlink" title="4.2 流量管理模型"></a>4.2 流量管理模型</h2><ul>
<li>VirtualService</li>
<li>DestinationRule</li>
<li>ServiceEntry</li>
<li>Gateway</li>
</ul>
<h4 id="VirtualService"><a href="#VirtualService" class="headerlink" title="VirtualService"></a>VirtualService</h4><p>VirtualService 中定义了一系列针对指定服务的流量路由规则。每个路由规则都是针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务, 或者目标服务的子集或版本, 匹配规则中还包含了对流量发起方的定义，这样一来，规则还可以针对特定客户上下文进行定制.</p>
<h4 id="Gateway"><a href="#Gateway" class="headerlink" title="Gateway"></a>Gateway</h4><p>Gateway 描述了一个负载均衡器，用于承载网格边缘的进入和发出连接。这一规范中描述了一系列开放端口，以及这些端口所使用的协议、负载均衡的 SNI 配置等内容</p>
<h4 id="ServiceEntry"><a href="#ServiceEntry" class="headerlink" title="ServiceEntry"></a>ServiceEntry</h4><p>Istio 服务网格内部会维护一个与平台无关的使用通用模型表示的服务注册表，当你的服务网格需要访问外部服务的时候，就需要使用 ServiceEntry 来添加服务注册, 这类服务可能是网格外的 API，或者是处于网格内部但却不存在于平台的服务注册表中的条目（例如需要和 Kubernetes 服务沟通的一组虚拟机服务）.</p>
<h4 id="EnvoyFilter"><a href="#EnvoyFilter" class="headerlink" title="EnvoyFilter"></a>EnvoyFilter</h4><p>EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置.</p>
<h4 id="Kubernetes-Ingress-vs-Istio-Gateway"><a href="#Kubernetes-Ingress-vs-Istio-Gateway" class="headerlink" title="Kubernetes Ingress vs Istio Gateway"></a>Kubernetes Ingress vs Istio Gateway</h4><p><img src="https://zhongfox.github.io/assets/images/istio/gateway.png"></p>
<ul>
<li>合并了L4-6和L7的规范, 对传统技术栈用户的应用迁入不方便</li>
<li>表现力不足:<ul>
<li>只能对 service、port、HTTP 路径等有限字段匹配来路由流量</li>
<li>端口只支持默认80/443</li>
</ul>
</li>
</ul>
<p>Istio Gateway:·</p>
<ul>
<li>定义了四层到六层的负载均衡属性 (通常是SecOps或NetOps关注的内容)<ul>
<li>端口</li>
<li>端口所使用的协议(HTTP, HTTPS, GRPC, HTTP2, MONGO, TCP, TLS)</li>
<li>Hosts</li>
<li>TLS SNI header 路由支持</li>
<li>TLS 配置支持(http 自动301, 证书等)</li>
<li>ip / unix domain socket</li>
</ul>
</li>
</ul>
<h4 id="Kubernetes-Istio-Envoy-xDS-模型对比"><a href="#Kubernetes-Istio-Envoy-xDS-模型对比" class="headerlink" title="Kubernetes, Istio, Envoy xDS 模型对比"></a>Kubernetes, Istio, Envoy xDS 模型对比</h4><p>以下是对Kubernetes, Istio, Envoy xDS 模型的不严格对比</p>
<table>
<thead>
<tr>
<th></th>
<th>Kubernetes</th>
<th>Istio</th>
<th>Envoy xDS</th>
</tr>
</thead>
<tbody>
<tr>
<td>入口流量</td>
<td>Ingress</td>
<td>GateWay</td>
<td>Listener</td>
</tr>
<tr>
<td>服务定义</td>
<td>Service</td>
<td>-</td>
<td>Cluster+Listener</td>
</tr>
<tr>
<td>外部服务定义</td>
<td>-</td>
<td>ServiceEntry</td>
<td>Cluster+Listener</td>
</tr>
<tr>
<td>版本定义</td>
<td>-</td>
<td>DestinationRule</td>
<td>Cluster+Listener</td>
</tr>
<tr>
<td>版本路由</td>
<td>-</td>
<td>VirtualService</td>
<td>Route</td>
</tr>
<tr>
<td>实例</td>
<td>Endpoint</td>
<td>-</td>
<td>Endpoint</td>
</tr>
</tbody>
</table>
<h4 id="Kubernetes-和-Istio-服务寻址的区别"><a href="#Kubernetes-和-Istio-服务寻址的区别" class="headerlink" title="Kubernetes 和 Istio 服务寻址的区别:"></a>Kubernetes 和 Istio 服务寻址的区别:</h4><p><strong>Kubernetes</strong>:</p>
<ol>
<li>kube-dns: service domain -&gt; service ip</li>
<li>kube-proxy(node iptables): service ip -&gt; pod ip</li>
</ol>
<p><strong>Istio</strong>:</p>
<ol>
<li>kube-dns: service domain -&gt; service ip</li>
<li>sidecar envoy: service ip -&gt; pod ip</li>
</ol>
<hr>
<h2 id="4-3-故障处理"><a href="#4-3-故障处理" class="headerlink" title="4.3 故障处理"></a>4.3 故障处理</h2><p>随着微服务的拆分粒度增强, 服务调用会增多, 更复杂, 扇入 扇出,  调用失败的风险增加, 以下是常见的服务容错处理方式:</p>
<table>
<thead>
<tr>
<th></th>
<th>控制端</th>
<th>目的</th>
<th>实现</th>
<th>Istio</th>
</tr>
</thead>
<tbody>
<tr>
<td>超时</td>
<td>client</td>
<td>保护client</td>
<td>请求等待超时/请求运行超时</td>
<td>timeout</td>
</tr>
<tr>
<td>重试</td>
<td>client</td>
<td>容忍server临时错误, 保证业务整体可用性</td>
<td>重试次数/重试的超时时间</td>
<td>retries.attempts, retries.perTryTimeout</td>
</tr>
<tr>
<td>熔断</td>
<td>client</td>
<td>降低性能差的服务或实例的影响</td>
<td>通常会结合超时+重试, 动态进行服务状态决策(Open/Closed/Half-Open)</td>
<td>trafficPolicy.outlierDetection</td>
</tr>
<tr>
<td>降级</td>
<td>client</td>
<td>保证业务主要功能可用</td>
<td>主逻辑失败采用备用逻辑的过程(镜像服务分级, 调用备用服务, 或者返回mock数据)</td>
<td>暂不支持, 需要业务代码按需实现</td>
</tr>
<tr>
<td>隔离</td>
<td>client</td>
<td>防止异常server占用过多client资源</td>
<td>隔离对不同服务调用的资源依赖: 线程池隔离/信号量隔离</td>
<td>暂不支持</td>
</tr>
<tr>
<td>幂等</td>
<td>server</td>
<td>容忍client重试, 保证数据一致性</td>
<td>唯一ID/加锁/事务等手段</td>
<td>暂不支持, 需要业务代码按需实现</td>
</tr>
<tr>
<td>限流</td>
<td>server</td>
<td>保护server</td>
<td>常用算法: 计数器, 漏桶, 令牌桶</td>
<td>trafficPolicy.connectionPool</td>
</tr>
</tbody>
</table>
<p>Istio 没有无降级处理支持: Istio可以提高网格中服务的可靠性和可用性。但是，应用程序仍然需要处理故障（错误）并采取适当的回退操作。例如，当负载均衡池中的所有实例都失败时，Envoy 将返回 HTTP 503。应用程序有责任实现必要的逻辑，对这种来自上游服务的 HTTP 503 错误做出合适的响应。</p>
<hr>
<h2 id="4-4-Mixer-架构"><a href="#4-4-Mixer-架构" class="headerlink" title="4.4 Mixer 架构"></a>4.4 Mixer 架构</h2><p><img src="https://istio.io/docs/concepts/policies-and-telemetry/topology-without-cache.svg"></p>
<center>Mixer Topology（图片来自<a href="https://istio.io/docs/concepts/policies-and-telemetry/" target="_blank" rel="noopener">Isio官网文档</a>)</center>

<p>Istio 的四大功能点连接, 安全, 控制, 观察, 其中「控制」和「观察」的功能主要都是由Mixer组件来提供, Mixer 在Istio中角色:</p>
<ul>
<li>功能上: 负责策略控制和遥测收集</li>
<li>架构上:提供插件模型，可以扩展和定制</li>
</ul>
<hr>
<h2 id="4-5-Mixer-Adapter-模型"><a href="#4-5-Mixer-Adapter-模型" class="headerlink" title="4.5 Mixer Adapter 模型"></a>4.5 Mixer Adapter 模型</h2><ul>
<li>Attribute</li>
<li>Template</li>
<li>Adapter</li>
<li>Instance</li>
<li>Handler</li>
<li>Rule</li>
</ul>
<h4 id="Attribute"><a href="#Attribute" class="headerlink" title="Attribute"></a>Attribute</h4><p>Attribute 是策略和遥测功能中有关请求和环境的基本数据, 是用于描述特定服务请求或请求环境的属性的一小段数据。例如，属性可以指定特定请求的大小、操作的响应代码、请求来自的 IP 地址等.</p>
<ul>
<li>Istio 中的主要属性生产者是 Envoy，但专用的 Mixer 适配器也可以生成属性</li>
<li>属性词汇表见: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/attribute-vocabulary/" target="_blank" rel="noopener">Attribute Vocabulary</a></li>
<li>数据流向: envoy -&gt; mixer</li>
</ul>
<h4 id="Template"><a href="#Template" class="headerlink" title="Template"></a>Template</h4><p>Template 是对 adapter 的数据格式和处理接口的抽象, Template定义了:</p>
<ul>
<li>当处理请求时发送给adapter 的数据格式</li>
<li>adapter 必须实现的gRPC service 接口</li>
</ul>
<p>每个Template 通过 <code>template.proto</code> 进行定义:</p>
<ul>
<li>名为<code>Template</code> 的一个message</li>
<li>Name: 通过template所在的package name自动生成</li>
<li>template_variety: 可选Check, Report, Quota or AttributeGenerator, 决定了adapter必须实现的方法. 同时决定了在mixer的什么阶段要生成template对应的instance:<ul>
<li>Check: 在Mixer’s Check API call时创建并发送instance</li>
<li>Report: 在Mixer’s Report API call时创建并发送instance</li>
<li>Quota: 在Mixer’s Check  API call时创建并发送instance(查询配额时)</li>
<li>AttributeGenerator: for both Check, Report Mixer API calls</li>
</ul>
</li>
</ul>
<p>Istio 内置的Templates: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/templates/" target="_blank" rel="noopener">https://istio.io/docs/reference/config/policy-and-telemetry/templates/</a></p>
<h4 id="Adapter"><a href="#Adapter" class="headerlink" title="Adapter"></a>Adapter</h4><p>封装了 Mixer 和特定外部基础设施后端进行交互的必要接口，例如 Prometheus 或者 Stackdriver</p>
<ul>
<li>定义了需要处理的模板(在yaml中配置template)</li>
<li>定义了处理某个Template数据格式的GRPC接口</li>
<li>定义 Adapter需要的配置格式(Params)</li>
<li>可以同时处理多个数据(instance)</li>
</ul>
<p>Istio 内置的Adapter: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/adapters/" target="_blank" rel="noopener">https://istio.io/docs/reference/config/policy-and-telemetry/adapters/</a></p>
<h4 id="Instance"><a href="#Instance" class="headerlink" title="Instance"></a>Instance</h4><p>代表符合某个Template定义的数据格式的具体实现, 该具体实现由用户配置的 CRD,  CRD 定义了将Attributes 转换为具体instance 的规则, 支持属性表达式</p>
<ul>
<li>Instance CRD 是Template 中定义的数据格式 + 属性转换器</li>
<li>内置的Instance 类型(其实就是内置 Template): <a href="https://istio.io/docs/reference/config/policy-and-telemetry/templates/" target="_blank" rel="noopener">Templates</a></li>
<li>属性表达式见: <a href="https://istio.io/docs/reference/config/policy-and-telemetry/expression-language/" target="_blank" rel="noopener">Expression Language</a></li>
<li>数据流向: mixer -&gt; adapter 实例</li>
</ul>
<h4 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h4><p>用户配置的 CRD, 为具体Adapter提供一个具体配置, 对应Adapter的可运行实例</p>
<h4 id="Rule"><a href="#Rule" class="headerlink" title="Rule"></a>Rule</h4><p>用户配置的 CRD, 配置一组规则，这些规则描述了何时调用特定(通过Handler对应的)适配器及哪些Instance</p>
<hr>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><blockquote>
<p>计算机科学中的所有问题，都可以用另一个层来解决，除了层数太多的问题</p>
</blockquote>
<p>Kubernetes 本身已经很复杂, Istio 为了更高层控制的抽象, 又增加了很多概念. 复杂度堪比kubernetes.</p>
<p>可以看出istio 设计精良, 在处理微服务的复杂场景有很多优秀之处, 不过目前istio目前的短板还是很明显, 高度的抽象带来了很多性能的损耗, 社区现在也有很多优化的方向, 像蚂蚁金服开源的SofaMesh 主要是去精简层, 试图在sidecar里去做很多mixer 的事情, 减少sidecar和mixer的同步请求依赖,  而一些其他的sidecar 网络方案, 更多的是考虑去优化层, 优化sidecar 这一层的性能开销.</p>
<p>在Istio 1.0 之前, 主要还是以功能的实现为主, 不过后面随着社区的积极投入, 相信Istio的性能会有长足的提升.</p>
<p>笔者之前从事过多年的服务治理相关的工作, 过程中切身体会到微服务治理的痛点, 所以也比较关注 service mesh的发展, 个人对istio也非常看好, 刚好今年我们中心容器产品今年也有这方面的计划, 期待我们能在这个方向进行一些产品和技术的深耕.</p>
<hr>
<p><img src="https://zhongfox.github.io/assets/images/istio/last.png"></p>
<hr>
<p>参考资料:</p>
<ul>
<li><a href="http://www.servicemesher.com/" target="_blank" rel="noopener">servicemesher 中文社区</a></li>
<li><a href="https://thenewstack.io/why-you-should-care-about-istio-gateways/" target="_blank" rel="noopener">Why You Should Care About Istio Gateways</a></li>
<li><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank" rel="noopener">Pattern: Service Mesh</a></li>
<li><a href="https://github.com/istio/istio/wiki/Mixer-Out-Of-Process-Adapter-Dev-Guide" target="_blank" rel="noopener">Mixer Out Of Process Adapter Dev Guide</a></li>
<li><a href="https://github.com/istio/istio/wiki/Mixer-Out-Of-Process-Adapter-Walkthrough" target="_blank" rel="noopener">Mixer Out of Process Adapter Walkthrough</a></li>
<li><a href="http://www.servicemesher.com/blog/envoy-xds-protocol" target="_blank" rel="noopener">Envoy 中的 xDS REST 和 gRPC 协议详解</a></li>
<li><a href="https://preliminary.istio.io/blog/2018/delayering-istio/delayering-istio/" target="_blank" rel="noopener">Delayering Istio with AppSwitch</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/01/31/servicemesh-istio/" data-id="ck89nwpc2000no8kbkwuzr8ys" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-k8s-traffic-copy" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/10/k8s-traffic-copy/" class="article-date">
  <time datetime="2019-01-10T02:17:37.000Z" itemprop="datePublished">2019-01-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/10/k8s-traffic-copy/">Kubernetes 流量复制方案</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者：田小康</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>测试环境没有真实的数据, 会导致很多测试工作难以展开, 尤其是一些测试任务需要使用生产环境来做时, 会极大影响现网的稳定性。</p>
<p>我们需要一个流量复制方案, 将现网流量复制到预发布/测试环境</p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/traffic-copy-diagram.png" alt="流量复制示意"></p>
<h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><ul>
<li>将线上请求拷贝一份到预发布/测试环境</li>
<li>不影响现网请求</li>
<li>可配置流量复制比例, 毕竟测试环境资源有限</li>
<li>零代码改动</li>
</ul>
<h1 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h1><p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/k8s-traffic-copy-diagram.png" alt="Kubernetes 流量复制方案"></p>
<ul>
<li>承载入口流量的 Pod 新增一个 <code>Nginx 容器</code> 接管流量</li>
<li><a href="http://nginx.org/en/docs/http/ngx_http_mirror_module.html" target="_blank" rel="noopener">Nginx Mirror</a> 模块会将流量复制一份并 proxy 到指定 URL (测试环境)</li>
<li><code>Nginx mirror</code> 复制流量不会影响正常请求处理流程, 镜像请求的 Resp 会被 Nginx 丢弃</li>
<li><code>K8s Service</code> 按照 <code>Label Selector</code> 去选择请求分发的 Pod, 意味着不同Pod, 只要有相同 <code>Label</code>, 就可以协同处理请求</li>
<li>通过控制有 <code>Mirror 功能的 Pod</code> 和 <code>正常的 Pod</code> 的比例, 便可以配置流量复制的比例</li>
</ul>
<p>我们的部署环境为 <a href="https://cloud.tencent.com/product/tke" target="_blank" rel="noopener">腾讯云容器服务</a>, 不过所述方案是普适于 <code>Kubernetes</code> 环境的.</p>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>PS: 下文假定读者了解</p>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/" target="_blank" rel="noopener">Kubernetes</a> 以及 YAML</li>
<li><a href="https://helm.sh/" target="_blank" rel="noopener">Helm</a></li>
<li><a href="https://www.nginx.com/" target="_blank" rel="noopener">Nginx</a></li>
</ul>
<h3 id="Nginx-镜像"><a href="#Nginx-镜像" class="headerlink" title="Nginx 镜像"></a>Nginx 镜像</h3><p>使用 Nginx 官方镜像便已经预装了 Mirror 插件</p>
<p>即: <code>docker pull nginx</code></p>
<p><code>yum install nginx</code> 安装的版本貌似没有 Mirror 插件的哦, 需要自己装</p>
<h3 id="Nginx-ConfigMap"><a href="#Nginx-ConfigMap" class="headerlink" title="Nginx ConfigMap"></a>Nginx ConfigMap</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">entrance-nginx-config</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">nginx.conf:</span> <span class="string">|-</span></span><br><span class="line">    <span class="string">worker_processes</span> <span class="string">auto;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">error_log</span> <span class="string">/data/athena/logs/entrance/nginx-error.log;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">events</span> <span class="string">&#123;</span></span><br><span class="line">      <span class="string">worker_connections</span>  <span class="number">1024</span><span class="string">;</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">http</span> <span class="string">&#123;</span></span><br><span class="line">      <span class="string">default_type</span>  <span class="string">application/octet-stream;</span></span><br><span class="line">      <span class="string">sendfile</span>        <span class="string">on;</span></span><br><span class="line">      <span class="string">keepalive_timeout</span>  <span class="number">65</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line">      <span class="string">server</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-access.log;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">listen</span>       <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;;</span></span><br><span class="line">        <span class="string">server_name</span>  <span class="string">entrance;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">root</span>   <span class="string">html;</span></span><br><span class="line">          <span class="string">index</span>  <span class="string">index.html</span> <span class="string">index.htm;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/entrance/</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">mirror</span> <span class="string">/mirror;</span></span><br><span class="line">          <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-entrance-access.log;</span></span><br><span class="line">          <span class="string">proxy_pass</span> <span class="attr">http://localhost:&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePortMirror</span> <span class="string">&#125;&#125;/;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/mirror</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">internal;</span></span><br><span class="line">          <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-mirror-access.log;</span></span><br><span class="line">          <span class="string">proxy_pass</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.mirrorProxyPass</span> <span class="string">&#125;&#125;;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  <span class="string">/50x.html;</span></span><br><span class="line">        <span class="string">location</span> <span class="string">=</span> <span class="string">/50x.html</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">root</span>   <span class="string">html;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line">      <span class="string">&#125;</span></span><br><span class="line">    <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>其中重点部分如下:</p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/nginx-config.png" alt=""></p>
<h3 id="业务方容器-Nginx-Mirror"><a href="#业务方容器-Nginx-Mirror" class="headerlink" title="业务方容器 + Nginx Mirror"></a>业务方容器 + Nginx Mirror</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.entrance.mirrorEnable</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">entrance-mirror</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.mirrorReplicaCount</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">            - weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">              podAffinityTerm:</span></span><br><span class="line"><span class="attr">                labelSelector:</span></span><br><span class="line"><span class="attr">                  matchExpressions:</span></span><br><span class="line"><span class="attr">                    - key:</span> <span class="string">"name"</span></span><br><span class="line"><span class="attr">                      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                      values:</span></span><br><span class="line"><span class="bullet">                        -</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">                topologyKey:</span> <span class="string">"kubernetes.io/hostname"</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">init-kafka</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"centos-dev"</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.delay</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'sleep 480s; until nslookup athena-cp-kafka; do echo "waiting for athena-cp-kafka"; sleep 2; done;'</span><span class="string">]</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">else</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'until nslookup athena-cp-kafka; do echo "waiting for athena-cp-kafka"; sleep 2; done;'</span><span class="string">]</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.entrance.image.repository &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.entrance.image.tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_KAFKA_BOOTSTRAP</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.kafka.kafkaBootstrap &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_KAFKA_SCHEMA_REGISTRY_URL</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.kafka.kafkaSchemaRegistryUrl &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_PG_CONN</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.pg.pgConn &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_COS_CONN</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.cos.cosConn &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_DEPLOY_TYPE</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.deployType &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_SYS_ID</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsSysId &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_SYS_SECRET</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsSysSecret &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_BASE_URL</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsBaseUrl &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_RESOURCE_FLOW_PERIOD_SEC</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsResourceFlowPeriodSec &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_CLUSTER</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.cluster &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_POD_NAME</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_HOST_IP</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">status.hostIP</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_POD_IP</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['/bin/bash',</span> <span class="string">'/data/service/go_workspace/script/start-entrance.sh'</span><span class="string">,</span> <span class="string">'-host 0.0.0.0:<span class="template-variable">&#123;&#123; .Values.entrance.service.nodePortMirror &#125;&#125;</span>'</span><span class="string">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/data/athena/</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">3000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">800</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span></span><br><span class="line"><span class="bullet">              -</span> <span class="string">bash</span></span><br><span class="line"><span class="bullet">              -</span> <span class="string">/data/service/go_workspace/script/health-check/check-entrance.sh</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">120</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.nginx.image.repository &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.nginx.image.tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance-mirror</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">          - containerPort:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">          - mountPath:</span> <span class="string">/data/athena/</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">            readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">          - mountPath:</span> <span class="string">/etc/nginx/nginx.conf</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">nginx-config</span></span><br><span class="line"><span class="attr">            subPath:</span> <span class="string">nginx.conf</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">1000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">500</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          tcpSocket:</span></span><br><span class="line"><span class="attr">            port:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">        entrance:</span> <span class="string">"true"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">"/data/athena/"</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nginx-config</span></span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">entrance-nginx-config</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      imagePullSecrets:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.imagePullSecrets &#125;&#125;</span>"</span></span><br><span class="line"><span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br></pre></td></tr></table></figure>
<p>上面为真实在业务中使用的 Deployment 配置, 有些地方可以参考:</p>
<ul>
<li><code>valueFrom.fieldRef.fieldPath</code> 可以取到容器运行时的一些字段, 如 <code>NodeIP</code>, <code>PodIP</code> 这些可以用于全链路监控</li>
<li><code>ConfigMap</code> 直接 Mount 到文件系统, 覆盖默认配置的例子</li>
<li><code>affinity.podAntiAffinity</code> 亲和性调度, 使 Pod 在主机间均匀分布</li>
<li>使用了 <code>tcpSocket</code> 和 <code>exec.command</code> 两种健康检查方式</li>
</ul>
<h3 id="Helm-Values"><a href="#Helm-Values" class="headerlink" title="Helm Values"></a>Helm Values</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># entrance, Athena 上报入口模块</span></span><br><span class="line"><span class="attr">entrance:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  replicaCount:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  mirrorEnable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  mirrorReplicaCount:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  mirrorProxyPass:</span> <span class="string">"http://10.16.0.147/entrance/"</span></span><br><span class="line"><span class="attr">  image:</span></span><br><span class="line"><span class="attr">    repository:</span> <span class="string">athena-go</span></span><br><span class="line"><span class="attr">    tag:</span> <span class="string">v1901091026</span></span><br><span class="line"><span class="attr">  service:</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30081</span></span><br><span class="line"><span class="attr">    nodePortMirror:</span> <span class="number">30082</span></span><br></pre></td></tr></table></figure>
<p>如上, <code>replicaCount: 3</code> + <code>mirrorReplicaCount: 1</code> = 4 个容器, 有 1/4 流量复制到 <code>http://10.16.0.147/entrance/</code></p>
<h3 id="内网负载均衡"><a href="#内网负载均衡" class="headerlink" title="内网负载均衡"></a>内网负载均衡</h3><p>流量复制到测试环境时, 尽量使用内网负载均衡, 为了成本, 安全及性能方面的考虑</p>
<p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/lb-inner.png" alt="LB-inner-config"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过下面几个步骤, 便可以实现流量复制啦</p>
<ul>
<li>建一个内网负载均衡, 暴漏测试环境的 <code>服务入口 Service</code></li>
<li><code>服务入口 Service</code> 需要有可以更换端口号的能力 (例如命令行参数/环境变量)</li>
<li>线上环境, 新增一个 Deployment, Label 和之前的 <code>服务入口 Service</code> 一样, 只是端口号分配一个新的</li>
<li>为新增的 Deployment 增加一个 Nginx 容器, 配置 nginx.conf</li>
<li>调节有 <code>Nginx Mirror</code> 的 Pod 和 正常的 <code>Pod</code> 比例, 便可以实现<code>按比例流量复制</code></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2019/01/10/k8s-traffic-copy/" data-id="ck89nwpbp000co8kbxj8uzzgk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">三月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/03/27/build-cloud-native-large-scale-distributed-monitoring-system-1/">打造云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段</a>
          </li>
        
          <li>
            <a href="/2020/01/13/kubernetes-overflow-and-drop/">Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</a>
          </li>
        
          <li>
            <a href="/2019/12/15/no-route-to-host/">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a>
          </li>
        
          <li>
            <a href="/2019/11/26/service-topology/">k8s v1.17 新特性预告: 拓扑感知服务路由</a>
          </li>
        
          <li>
            <a href="/2019/08/12/troubleshooting-with-kubernetes-network/">Kubernetes 网络疑难杂症排查分享</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 腾讯云容器团队<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>