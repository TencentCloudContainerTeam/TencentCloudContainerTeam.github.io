<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Kubernetes 疑难杂症排查分享：神秘的溢出与丢包 | 腾讯云容器团队</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="作者: 陈鹏  上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。  问题描述有用户反馈大量图片加载不出来。 图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个">
<meta name="keywords" content="container kubernetes tencentcloud">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes 疑难杂症排查分享：神秘的溢出与丢包">
<meta property="og:url" content="https://TencentCloudContainerTeam.github.io/2020/01/13/kubernetes-overflow-and-drop/index.html">
<meta property="og:site_name" content="腾讯云容器团队">
<meta property="og:description" content="作者: 陈鹏  上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。  问题描述有用户反馈大量图片加载不出来。 图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个">
<meta property="og:locale" content="zh-cn">
<meta property="og:image" content="https://imroc.io/assets/blog/troubleshooting-k8s-network/backlog.png">
<meta property="og:updated_time" content="2020-04-25T15:58:47.792Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kubernetes 疑难杂症排查分享：神秘的溢出与丢包">
<meta name="twitter:description" content="作者: 陈鹏  上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。  问题描述有用户反馈大量图片加载不出来。 图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个">
<meta name="twitter:image" content="https://imroc.io/assets/blog/troubleshooting-k8s-network/backlog.png">
  
    <link rel="alternate" href="/atom.xml" title="腾讯云容器团队" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">腾讯云容器团队</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://TencentCloudContainerTeam.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-kubernetes-overflow-and-drop" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/13/kubernetes-overflow-and-drop/" class="article-date">
  <time datetime="2020-01-13T06:40:00.000Z" itemprop="datePublished">2020-01-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Kubernetes 疑难杂症排查分享：神秘的溢出与丢包
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者: <a href="https://imroc.io/" target="_blank" rel="noopener">陈鹏</a></p>
<blockquote>
<p>上一篇 <a href="https://tencentcloudcontainerteam.github.io/2019/12/15/no-route-to-host/">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a> 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。</p>
</blockquote>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>有用户反馈大量图片加载不出来。</p>
<p>图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client –&gt; k8s ingress –&gt; nginx –&gt; nfs。</p>
<h2 id="猜测"><a href="#猜测" class="headerlink" title="猜测"></a>猜测</h2><p>猜测: ingress 图片下载路径对应的后端服务出问题了。</p>
<p>验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！</p>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拿到 pod 中 nginx 的容器 id</span></span><br><span class="line">$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 <span class="string">"^Containers:"</span> | grep -Eo <span class="string">'docker://.*$'</span> | head -n 1 | sed <span class="string">'s/docker:\/\/\(.*\)$/\1/'</span></span><br><span class="line">49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过容器 id 拿到 nginx 进程 pid</span></span><br><span class="line">$ docker inspect -f &#123;&#123;.State.Pid&#125;&#125; 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e</span><br><span class="line">3985</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入 nginx 进程所在的 netns</span></span><br><span class="line">$ nsenter -n -t 3985</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器 netns 中的网卡信息，确认下</span></span><br><span class="line">$ ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.26.0.8/26 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<p>使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 -nnnn -ttt port 24568</span><br></pre></td></tr></table></figure>
<p>在其它节点准备使用 nc 指定源端口为 24568 向容器发包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -u 24568 172.16.1.21 80</span><br></pre></td></tr></table></figure>
<p>观察抓包结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">00:00:00.000000 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000206334 ecr 0,nop,wscale 9], length 0</span><br><span class="line">00:00:01.032218 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000207366 ecr 0,nop,wscale 9], length 0</span><br><span class="line">00:00:02.011962 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000209378 ecr 0,nop,wscale 9], length 0</span><br><span class="line">00:00:04.127943 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000213506 ecr 0,nop,wscale 9], length 0</span><br><span class="line">00:00:08.192056 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000221698 ecr 0,nop,wscale 9], length 0</span><br><span class="line">00:00:16.127983 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000237826 ecr 0,nop,wscale 9], length 0</span><br><span class="line">00:00:33.791988 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000271618 ecr 0,nop,wscale 9], length 0</span><br></pre></td></tr></table></figure>
<p>SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。</p>
<p>排除是 iptables 规则问题，在容器 netns 中使用 <code>netstat -s</code> 检查下是否有丢包统计:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -s | grep -E <span class="string">'overflow|drop'</span></span><br><span class="line">    12178939 <span class="built_in">times</span> the listen queue of a socket overflowed</span><br><span class="line">    12247395 SYNs to LISTEN sockets dropped</span><br></pre></td></tr></table></figure>
<p>果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。</p>
<h2 id="syn-queue-与-accept-queue"><a href="#syn-queue-与-accept-queue" class="headerlink" title="syn queue 与 accept queue"></a>syn queue 与 accept queue</h2><p>Linux 进程监听端口时，内核会给它对应的 socket 分配两个队列：</p>
<ul>
<li>syn queue: 半连接队列。server 收到 SYN 后，连接会先进入 <code>SYN_RCVD</code> 状态，并放入 syn queue，此队列的包对应还没有完全建立好的连接（TCP 三次握手还没完成）。</li>
<li>accept queue: 全连接队列。当 TCP 三次握手完成之后，连接会进入 <code>ESTABELISHED</code> 状态并从 syn queue 移到 accept queue，等待被进程调用 <code>accept()</code> 系统调用 “拿走”。</li>
</ul>
<blockquote>
<p>注意：这两个队列的连接都还没有真正被应用层接收到，当进程调用 <code>accept()</code> 后，连接才会被应用层处理，具体到我们这个问题的场景就是 nginx 处理 HTTP 请求。</p>
</blockquote>
<p>为了更好理解，可以看下这张 TCP 连接建立过程的示意图：</p>
<p><img src="https://imroc.io/assets/blog/troubleshooting-k8s-network/backlog.png" alt=""></p>
<h2 id="listen-与-accept"><a href="#listen-与-accept" class="headerlink" title="listen 与 accept"></a>listen 与 accept</h2><p>不管使用什么语言和框架，在写 server 端应用时，它们的底层在监听端口时最终都会调用 <code>listen()</code> 系统调用，处理新请求时都会先调用 <code>accept()</code> 系统调用来获取新的连接，然后再处理请求，只是有各自不同的封装而已，以 go 语言为例：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调用 listen 监听端口</span></span><br><span class="line">l, err := net.Listen(<span class="string">"tcp"</span>, <span class="string">":80"</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	<span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">	<span class="comment">// 不断调用 accept 获取新连接，如果 accept queue 为空就一直阻塞</span></span><br><span class="line">	conn, err := l.Accept()</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		log.Println(<span class="string">"accept error:"</span>, err)</span><br><span class="line">		<span class="keyword">continue</span></span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 每来一个新连接意味着一个新请求，启动协程处理请求</span></span><br><span class="line">	<span class="keyword">go</span> handle(conn)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Linux-的-backlog"><a href="#Linux-的-backlog" class="headerlink" title="Linux 的 backlog"></a>Linux 的 backlog</h2><p>内核既然给监听端口的 socket 分配了 syn queue 与 accept queue 两个队列，那它们有大小限制吗？可以无限往里面塞数据吗？当然不行！ 资源是有限的，尤其是在内核态，所以需要限制一下这两个队列的大小。那么它们的大小是如何确定的呢？我们先来看下 listen 这个系统调用:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int listen(int sockfd, int backlog)</span><br></pre></td></tr></table></figure>
<p>可以看到，能够传入一个整数类型的 <code>backlog</code> 参数，我们再通过 <code>man listen</code> 看下解释：</p>
<p><code>The behavior of the backlog argument on TCP sockets changed with Linux 2.2.  Now it specifies the queue length for completely established sockets waiting to  be  accepted,  instead  of  the  number  of  incomplete  connection requests.   The  maximum  length  of  the queue for incomplete sockets can be set using /proc/sys/net/ipv4/tcp_max_syn_backlog.  When syncookies are enabled there is no logical maximum length and this setting is ignored.  See tcp(7) for more information.</code></p>
<p><code>If the backlog argument is greater than the value in /proc/sys/net/core/somaxconn, then it is silently truncated to that value; the default value in this file is 128.  In kernels before 2.4.25, this limit  was  a  hard  coded value, SOMAXCONN, with the value 128.</code></p>
<p>继续深挖了一下源码，结合这里的解释提炼一下：</p>
<ul>
<li>listen 的 backlog 参数同时指定了 socket 的 syn queue 与 accept queue 大小。</li>
<li><p>accept queue 最大不能超过 <code>net.core.somaxconn</code> 的值，即: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max accept queue size = min(backlog, net.core.somaxconn)</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果启用了 syncookies (net.ipv4.tcp_syncookies=1)，当 syn queue 满了，server 还是可以继续接收 <code>SYN</code> 包并回复 <code>SYN+ACK</code> 给 client，只是不会存入 syn queue 了。因为会利用一套巧妙的 syncookies 算法机制生成隐藏信息写入响应的 <code>SYN+ACK</code> 包中，等 client 回 <code>ACK</code> 时，server 再利用 syncookies 算法校验报文，校验通过后三次握手就顺利完成了。所以如果启用了 syncookies，syn queue 的逻辑大小是没有限制的，</p>
</li>
<li>syncookies 通常都是启用了的，所以一般不用担心 syn queue 满了导致丢包。syncookies 是为了防止 SYN Flood 攻击 (一种常见的 DDoS 方式)，攻击原理就是 client 不断发 SYN 包但不回最后的 ACK，填满 server 的 syn queue 从而无法建立新连接，导致 server 拒绝服务。</li>
<li>如果 syncookies 没有启用，syn queue 的大小就有限制，除了跟 accept queue 一样受 <code>net.core.somaxconn</code> 大小限制之外，还会受到 <code>net.ipv4.tcp_max_syn_backlog</code> 的限制，即:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max syn queue size = min(backlog, net.core.somaxconn, net.ipv4.tcp_max_syn_backlog)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>4.3 及其之前版本的内核，syn queue 的大小计算方式跟现在新版内核这里还不一样，详细请参考 commit <a href="https://github.com/torvalds/linux/commit/ef547f2ac16bd9d77a780a0e7c70857e69e8f23f#diff-56ecfd3cd70d57cde321f395f0d8d743L43" target="_blank" rel="noopener">ef547f2ac16b</a></p>
<h2 id="队列溢出"><a href="#队列溢出" class="headerlink" title="队列溢出"></a>队列溢出</h2><p>毫无疑问，在队列大小有限制的情况下，如果队列满了，再有新连接过来肯定就有问题。</p>
<p>翻下 linux 源码，看下处理 SYN 包的部分，在 <code>net/ipv4/tcp_input.c</code> 的 <code>tcp_conn_request</code> 函数:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ((net-&gt;ipv4.sysctl_tcp_syncookies == <span class="number">2</span> ||</span><br><span class="line">     inet_csk_reqsk_queue_is_full(sk)) &amp;&amp; !isn) &#123;</span><br><span class="line">	want_cookie = tcp_syn_flood_action(sk, rsk_ops-&gt;slab_name);</span><br><span class="line">	<span class="keyword">if</span> (!want_cookie)</span><br><span class="line">		<span class="keyword">goto</span> drop;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (sk_acceptq_is_full(sk)) &#123;</span><br><span class="line">	NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);</span><br><span class="line">	<span class="keyword">goto</span> drop;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>goto drop</code> 最终会走到 <code>tcp_listendrop</code> 函数，实际上就是将 <code>ListenDrops</code> 计数器 +1:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">tcp_listendrop</span><span class="params">(<span class="keyword">const</span> struct sock *sk)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	atomic_inc(&amp;((struct sock *)sk)-&gt;sk_drops);</span><br><span class="line">	__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENDROPS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>大致可以看出来，对于 SYN 包：</p>
<ul>
<li>如果 syn queue 满了并且没有开启 syncookies 就丢包，并将 <code>ListenDrops</code> 计数器 +1。</li>
<li>如果 accept queue 满了也会丢包，并将 <code>ListenOverflows</code> 和 <code>ListenDrops</code> 计数器 +1。</li>
</ul>
<p>而我们前面排查问题通过 <code>netstat -s</code> 看到的丢包统计，其实就是对应的 <code>ListenOverflows</code> 和 <code>ListenDrops</code> 这两个计数器。</p>
<p>除了用 <code>netstat -s</code>，还可以使用 <code>nstat -az</code> 直接看系统内各个计数器的值:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ nstat -az | grep -E <span class="string">'TcpExtListenOverflows|TcpExtListenDrops'</span></span><br><span class="line">TcpExtListenOverflows           12178939              0.0</span><br><span class="line">TcpExtListenDrops               12247395              0.0</span><br></pre></td></tr></table></figure>
<p>另外，对于低版本内核，当 accept queue 满了，并不会完全丢弃 SYN 包，而是对 SYN 限速。把内核源码切到 3.10 版本，看 <code>net/ipv4/tcp_ipv4.c</code> 中 <code>tcp_v4_conn_request</code> 函数:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Accept backlog is full. If we have already queued enough</span></span><br><span class="line"><span class="comment"> * of warm entries in syn queue, drop request. It is better than</span></span><br><span class="line"><span class="comment"> * clogging syn queue with openreqs with exponentially increasing</span></span><br><span class="line"><span class="comment"> * timeout.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">if</span> (sk_acceptq_is_full(sk) &amp;&amp; inet_csk_reqsk_queue_young(sk) &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);</span><br><span class="line">        <span class="keyword">goto</span> drop;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 <code>inet_csk_reqsk_queue_young(sk) &gt; 1</code> 的条件实际就是用于限速，仿佛在对 client 说: 哥们，你慢点！我的 accept queue 都满了，即便咱们握手成功，连接也可能放不进去呀。</p>
<h2 id="回到问题上来"><a href="#回到问题上来" class="headerlink" title="回到问题上来"></a>回到问题上来</h2><p>总结之前观察到两个现象：</p>
<ul>
<li>容器内抓包发现收到 client 的 SYN，但 nginx 没回包。</li>
<li>通过 <code>netstat -s</code> 发现有溢出和丢包的统计 (<code>ListenOverflows</code> 与 <code>ListenDrops</code>)。</li>
</ul>
<p>根据之前的分析，我们可以推测是 syn queue 或 accept queue 满了。</p>
<p>先检查下 syncookies 配置:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/net/ipv4/tcp_syncookies</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p>确认启用了 <code>syncookies</code>，所以 syn queue 大小没有限制，不会因为 syn queue 满而丢包，并且即便没开启 <code>syncookies</code>，syn queue 有大小限制，队列满了也不会使 <code>ListenOverflows</code> 计数器 +1。</p>
<p>从计数器结果来看，<code>ListenOverflows</code> 和 <code>ListenDrops</code> 的值差别不大，所以推测很有可能是 accept queue 满了，因为当 accept queue 满了会丢 SYN 包，并且同时将 <code>ListenOverflows</code> 与 <code>ListenDrops</code> 计数器分别 +1。</p>
<p>如何验证 accept queue 满了呢？可以在容器的 netns 中执行 <code>ss -lnt</code> 看下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ss -lnt</span><br><span class="line">State      Recv-Q Send-Q Local Address:Port                Peer Address:Port</span><br><span class="line">LISTEN     129    128                *:80                             *:*</span><br></pre></td></tr></table></figure>
<p>通过这条命令我们可以看到当前 netns 中监听 tcp 80 端口的 socket，<code>Send-Q</code> 为 128，<code>Recv-Q</code> 为 129。</p>
<p>什么意思呢？通过调研得知：</p>
<ul>
<li>对于 <code>LISTEN</code> 状态，<code>Send-Q</code> 表示 accept queue 的最大限制大小，<code>Recv-Q</code> 表示其实际大小。</li>
<li>对于 <code>ESTABELISHED</code> 状态，<code>Send-Q</code> 和 <code>Recv-Q</code> 分别表示发送和接收数据包的 buffer。</li>
</ul>
<p>所以，看这里输出结果可以得知 accept queue 满了，当 <code>Recv-Q</code> 的值比 <code>Send-Q</code> 大 1 时表明 accept queue 溢出了，如果再收到 SYN 包就会丢弃掉。</p>
<p>导致 accept queue 满的原因一般都是因为进程调用 <code>accept()</code> 太慢了，导致大量连接不能被及时 “拿走”。</p>
<p>那么什么情况下进程调用 <code>accept()</code> 会很慢呢？猜测可能是进程连接负载高，处理不过来。</p>
<p>而负载高不仅可能是 CPU 繁忙导致，还可能是 IO 慢导致，当文件 IO 慢时就会有很多 IO WAIT，在 IO WAIT 时虽然 CPU 不怎么干活，但也会占据 CPU 时间片，影响 CPU 干其它活。</p>
<p>最终进一步定位发现是 nginx pod 挂载的 nfs 服务对应的 nfs server 负载较高，导致 IO 延时较大，从而使 nginx 调用 <code>accept()</code> 变慢，accept queue 溢出，使得大量代理静态图片文件的请求被丢弃，也就导致很多图片加载不出来。</p>
<p>虽然根因不是 k8s 导致的问题，但也从中挖出一些在高并发场景下值得优化的点，请继续往下看。</p>
<h2 id="somaxconn-的默认值很小"><a href="#somaxconn-的默认值很小" class="headerlink" title="somaxconn 的默认值很小"></a>somaxconn 的默认值很小</h2><p>我们再看下之前 <code>ss -lnt</code> 的输出:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ss -lnt</span><br><span class="line">State      Recv-Q Send-Q Local Address:Port                Peer Address:Port</span><br><span class="line">LISTEN     129    128                *:80                             *:*</span><br></pre></td></tr></table></figure>
<p>仔细一看，<code>Send-Q</code> 表示 accept queue 最大的大小，才 128 ？也太小了吧！</p>
<p>根据前面的介绍我们知道，accept queue 的最大大小会受 <code>net.core.somaxconn</code> 内核参数的限制，我们看下 pod 所在节点上这个内核参数的大小:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/net/core/somaxconn</span><br><span class="line">32768</span><br></pre></td></tr></table></figure>
<p>是 32768，挺大的，为什么这里 accept queue 最大大小就只有 128 了呢？</p>
<p><code>net.core.somaxconn</code> 这个内核参数是 namespace 隔离了的，我们在容器 netns 中再确认了下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/net/core/somaxconn</span><br><span class="line">128</span><br></pre></td></tr></table></figure>
<p>为什么只有 128？看下 stackoverflow <a href="https://stackoverflow.com/questions/26177059/refresh-net-core-somaxcomm-or-any-sysctl-property-for-docker-containers/26197875#26197875" target="_blank" rel="noopener">这里</a> 的讨论: </p>
<p><code>The &quot;net/core&quot; subsys is registered per network namespace. And the initial value for somaxconn is set to 128.</code></p>
<p>原来新建的 netns 中 somaxconn 默认就为 128，在 <code>include/linux/socket.h</code> 中可以看到这个常量的定义:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Maximum queue length specifiable by listen.  */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SOMAXCONN	128</span></span><br></pre></td></tr></table></figure>
<p>很多人在使用 k8s 时都没太在意这个参数，为什么大家平常在较高并发下也没发现有问题呢？</p>
<p>因为通常进程 <code>accept()</code> 都是很快的，所以一般 accept queue 基本都没什么积压的数据，也就不会溢出导致丢包了。</p>
<p>对于并发量很高的应用，还是建议将 somaxconn 调高。虽然可以进入容器 netns 后使用 <code>sysctl -w net.core.somaxconn=1024</code> 或 <code>echo 1024 &gt; /proc/sys/net/core/somaxconn</code> 临时调整，但调整的意义不大，因为容器内的进程一般在启动的时候才会调用 <code>listen()</code>，然后 accept queue 的大小就被决定了，并且不再改变。</p>
<p>下面介绍几种调整方式:</p>
<h3 id="方式一-使用-k8s-sysctls-特性直接给-pod-指定内核参数"><a href="#方式一-使用-k8s-sysctls-特性直接给-pod-指定内核参数" class="headerlink" title="方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数"></a>方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数</h3><p>示例 yaml:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sysctl-example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  securityContext:</span></span><br><span class="line"><span class="attr">    sysctls:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">net.core.somaxconn</span></span><br><span class="line"><span class="attr">      value:</span> <span class="string">"8096"</span></span><br></pre></td></tr></table></figure>
<p>有些参数是 <code>unsafe</code> 类型的，不同环境不一样，我的环境里是可以直接设置 pod 的 <code>net.core.somaxconn</code> 这个 sysctl 的。如果你的环境不行，请参考官方文档 <a href="https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/#enabling-unsafe-sysctls" target="_blank" rel="noopener">Using sysctls in a Kubernetes Cluster</a> 启用 <code>unsafe</code> 类型的 sysctl。</p>
<blockquote>
<p>注：此特性在 k8s v1.12 beta，默认开启。</p>
</blockquote>
<h3 id="方式二-使用-initContainers-设置内核参数"><a href="#方式二-使用-initContainers-设置内核参数" class="headerlink" title="方式二: 使用 initContainers 设置内核参数"></a>方式二: 使用 initContainers 设置内核参数</h3><p>示例 yaml:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">sysctl-example-init</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  initContainers:</span></span><br><span class="line"><span class="attr">  - image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    command:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">sh</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">-c</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">echo</span> <span class="number">1024</span> <span class="string">&gt; /proc/sys/net/core/somaxconn</span></span><br><span class="line"><span class="string"></span><span class="attr">    imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">setsysctl</span></span><br><span class="line"><span class="attr">    securityContext:</span></span><br><span class="line"><span class="attr">      privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  Containers:</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注: init container 需要 privileged 权限。</p>
</blockquote>
<h3 id="方式三-安装-tuning-CNI-插件统一设置-sysctl"><a href="#方式三-安装-tuning-CNI-插件统一设置-sysctl" class="headerlink" title="方式三: 安装 tuning CNI 插件统一设置 sysctl"></a>方式三: 安装 tuning CNI 插件统一设置 sysctl</h3><p>tuning plugin 地址: <a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/tuning" target="_blank" rel="noopener">https://github.com/containernetworking/plugins/tree/master/plugins/meta/tuning</a></p>
<p>CNI 配置示例:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"mytuning"</span>,</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"tuning"</span>,</span><br><span class="line">  <span class="string">"sysctl"</span>: &#123;</span><br><span class="line">          <span class="string">"net.core.somaxconn"</span>: <span class="string">"1024"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="nginx-的-backlog"><a href="#nginx-的-backlog" class="headerlink" title="nginx 的 backlog"></a>nginx 的 backlog</h2><p>我们使用方式一尝试给 nginx pod 的 somaxconn 调高到 8096 后观察:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ss -lnt</span><br><span class="line">State      Recv-Q Send-Q Local Address:Port                Peer Address:Port</span><br><span class="line">LISTEN     512    511                *:80                             *:*</span><br></pre></td></tr></table></figure>
<p>WTF? 还是溢出了，而且调高了 somaxconn 之后虽然 accept queue 的最大大小 (<code>Send-Q</code>) 变大了，但跟 8096 还差很远呀！</p>
<p>在经过一番研究，发现 nginx 在 <code>listen()</code> 时并没有读取 somaxconn 作为 backlog 默认值传入，它有自己的默认值，也支持在配置里改。通过 <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html" target="_blank" rel="noopener">ngx_http_core_module</a> 的官方文档我们可以看到它在 linux 下的默认值就是 511:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">backlog=number</span><br><span class="line">   sets the backlog parameter in the listen() call that limits the maximum length for the queue of pending connections. By default, backlog is set to -1 on FreeBSD, DragonFly BSD, and macOS, and to 511 on other platforms.</span><br></pre></td></tr></table></figure>
<p>配置示例:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listen  80  default  backlog=1024;</span><br></pre></td></tr></table></figure>
<p>所以，在容器中使用 nginx 来支撑高并发的业务时，记得要同时调整下 <code>net.core.somaxconn</code> 内核参数和 <code>nginx.conf</code> 中的 backlog 配置。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>Using sysctls in a Kubernetes Cluster: <a href="https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/" target="_blank" rel="noopener">https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/</a></li>
<li>SYN packet handling in the wild: <a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/" target="_blank" rel="noopener">https://blog.cloudflare.com/syn-packet-handling-in-the-wild/</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://TencentCloudContainerTeam.github.io/2020/01/13/kubernetes-overflow-and-drop/" data-id="ck9ft6y8t000hbqotkost5821" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/03/27/build-cloud-native-large-scale-distributed-monitoring-system-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          打造云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段
        
      </div>
    </a>
  
  
    <a href="/2019/12/15/no-route-to-host/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">三月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/24/k8s-configmap-volume/">大规模使用ConfigMap卷的负载分析及缓解方案</a>
          </li>
        
          <li>
            <a href="/2020/03/27/build-cloud-native-large-scale-distributed-monitoring-system-1/">打造云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段</a>
          </li>
        
          <li>
            <a href="/2020/01/13/kubernetes-overflow-and-drop/">Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</a>
          </li>
        
          <li>
            <a href="/2019/12/15/no-route-to-host/">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a>
          </li>
        
          <li>
            <a href="/2019/11/26/service-topology/">k8s v1.17 新特性预告: 拓扑感知服务路由</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 腾讯云容器团队<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>