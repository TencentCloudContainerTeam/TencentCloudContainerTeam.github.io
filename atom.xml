<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>腾讯云容器团队</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://TencentCloudContainerTeam.github.io/"/>
  <updated>2019-01-10T06:19:59.604Z</updated>
  <id>https://TencentCloudContainerTeam.github.io/</id>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes 流量复制方案</title>
    <link href="https://TencentCloudContainerTeam.github.io/2019/01/10/k8s-traffic-copy/"/>
    <id>https://TencentCloudContainerTeam.github.io/2019/01/10/k8s-traffic-copy/</id>
    <published>2019-01-10T02:17:37.000Z</published>
    <updated>2019-01-10T06:19:59.604Z</updated>
    
    <content type="html"><![CDATA[<p>作者：田小康</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>测试环境没有真实的数据, 会导致很多测试工作难以展开, 尤其是一些测试任务需要使用生产环境来做时, 会极大影响现网的稳定性。</p><p>我们需要一个流量复制方案, 将现网流量复制到预发布/测试环境</p><p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/traffic-copy-diagram.png" alt="流量复制示意"></p><h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><ul><li>将线上请求拷贝一份到预发布/测试环境</li><li>不影响现网请求</li><li>可配置流量复制比例, 毕竟测试环境资源有限</li><li>零代码改动</li></ul><h1 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h1><p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/k8s-traffic-copy-diagram.png" alt="Kubernetes 流量复制方案"></p><ul><li>承载入口流量的 Pod 新增一个 <code>Nginx 容器</code> 接管流量</li><li><a href="http://nginx.org/en/docs/http/ngx_http_mirror_module.html" target="_blank" rel="noopener">Nginx Mirror</a> 模块会将流量复制一份并 proxy 到指定 URL (测试环境)</li><li><code>Nginx mirror</code> 复制流量不会影响正常请求处理流程, 镜像请求的 Resp 会被 Nginx 丢弃</li><li><code>K8s Service</code> 按照 <code>Label Selector</code> 去选择请求分发的 Pod, 意味着不同Pod, 只要有相同 <code>Label</code>, 就可以协同处理请求</li><li>通过控制有 <code>Mirror 功能的 Pod</code> 和 <code>正常的 Pod</code> 的比例, 便可以配置流量复制的比例</li></ul><p>我们的部署环境为 <a href="https://cloud.tencent.com/product/tke" target="_blank" rel="noopener">腾讯云容器服务</a>, 不过所述方案是普适于 <code>Kubernetes</code> 环境的.</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>PS: 下文假定读者了解</p><ul><li><a href="https://kubernetes.io/docs/concepts/" target="_blank" rel="noopener">Kubernetes</a> 以及 YAML</li><li><a href="https://helm.sh/" target="_blank" rel="noopener">Helm</a></li><li><a href="https://www.nginx.com/" target="_blank" rel="noopener">Nginx</a></li></ul><h3 id="Nginx-镜像"><a href="#Nginx-镜像" class="headerlink" title="Nginx 镜像"></a>Nginx 镜像</h3><p>使用 Nginx 官方镜像便已经预装了 Mirror 插件</p><p>即: <code>docker pull nginx</code></p><p><code>yum install nginx</code> 安装的版本貌似没有 Mirror 插件的哦, 需要自己装</p><h3 id="Nginx-ConfigMap"><a href="#Nginx-ConfigMap" class="headerlink" title="Nginx ConfigMap"></a>Nginx ConfigMap</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">entrance-nginx-config</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">nginx.conf:</span> <span class="string">|-</span></span><br><span class="line">    <span class="string">worker_processes</span> <span class="string">auto;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">error_log</span> <span class="string">/data/athena/logs/entrance/nginx-error.log;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">events</span> <span class="string">&#123;</span></span><br><span class="line">      <span class="string">worker_connections</span>  <span class="number">1024</span><span class="string">;</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">http</span> <span class="string">&#123;</span></span><br><span class="line">      <span class="string">default_type</span>  <span class="string">application/octet-stream;</span></span><br><span class="line">      <span class="string">sendfile</span>        <span class="string">on;</span></span><br><span class="line">      <span class="string">keepalive_timeout</span>  <span class="number">65</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line">      <span class="string">server</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-access.log;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">listen</span>       <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;;</span></span><br><span class="line">        <span class="string">server_name</span>  <span class="string">entrance;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">root</span>   <span class="string">html;</span></span><br><span class="line">          <span class="string">index</span>  <span class="string">index.html</span> <span class="string">index.htm;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/entrance/</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">mirror</span> <span class="string">/mirror;</span></span><br><span class="line">          <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-entrance-access.log;</span></span><br><span class="line">          <span class="string">proxy_pass</span> <span class="attr">http://localhost:&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePortMirror</span> <span class="string">&#125;&#125;/;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">location</span> <span class="string">/mirror</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">internal;</span></span><br><span class="line">          <span class="string">access_log</span> <span class="string">/data/athena/logs/entrance/nginx-mirror-access.log;</span></span><br><span class="line">          <span class="string">proxy_pass</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.mirrorProxyPass</span> <span class="string">&#125;&#125;;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  <span class="string">/50x.html;</span></span><br><span class="line">        <span class="string">location</span> <span class="string">=</span> <span class="string">/50x.html</span> <span class="string">&#123;</span></span><br><span class="line">          <span class="string">root</span>   <span class="string">html;</span></span><br><span class="line">        <span class="string">&#125;</span></span><br><span class="line">      <span class="string">&#125;</span></span><br><span class="line">    <span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>其中重点部分如下:</p><p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/nginx-config.png" alt=""></p><h3 id="业务方容器-Nginx-Mirror"><a href="#业务方容器-Nginx-Mirror" class="headerlink" title="业务方容器 + Nginx Mirror"></a>业务方容器 + Nginx Mirror</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.entrance.mirrorEnable</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">entrance-mirror</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.mirrorReplicaCount</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">            - weight:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">              podAffinityTerm:</span></span><br><span class="line"><span class="attr">                labelSelector:</span></span><br><span class="line"><span class="attr">                  matchExpressions:</span></span><br><span class="line"><span class="attr">                    - key:</span> <span class="string">"name"</span></span><br><span class="line"><span class="attr">                      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                      values:</span></span><br><span class="line"><span class="bullet">                        -</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">                topologyKey:</span> <span class="string">"kubernetes.io/hostname"</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">init-kafka</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"centos-dev"</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.delay</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'sleep 480s; until nslookup athena-cp-kafka; do echo "waiting for athena-cp-kafka"; sleep 2; done;'</span><span class="string">]</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">else</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['bash',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'until nslookup athena-cp-kafka; do echo "waiting for athena-cp-kafka"; sleep 2; done;'</span><span class="string">]</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.entrance.image.repository &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.entrance.image.tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_KAFKA_BOOTSTRAP</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.kafka.kafkaBootstrap &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_KAFKA_SCHEMA_REGISTRY_URL</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.kafka.kafkaSchemaRegistryUrl &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_PG_CONN</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.pg.pgConn &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_COS_CONN</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.cos.cosConn &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_DEPLOY_TYPE</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.deployType &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_SYS_ID</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsSysId &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_SYS_SECRET</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsSysSecret &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_BASE_URL</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsBaseUrl &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_TPS_RESOURCE_FLOW_PERIOD_SEC</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.tps.tpsResourceFlowPeriodSec &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_CLUSTER</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.cluster &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_POD_NAME</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_HOST_IP</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">status.hostIP</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">ATHENA_POD_IP</span></span><br><span class="line"><span class="attr">            valueFrom:</span></span><br><span class="line"><span class="attr">              fieldRef:</span></span><br><span class="line"><span class="attr">                fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        command:</span> <span class="string">['/bin/bash',</span> <span class="string">'/data/service/go_workspace/script/start-entrance.sh'</span><span class="string">,</span> <span class="string">'-host 0.0.0.0:<span class="template-variable">&#123;&#123; .Values.entrance.service.nodePortMirror &#125;&#125;</span>'</span><span class="string">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/data/athena/</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">3000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">800</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          exec:</span></span><br><span class="line"><span class="attr">            command:</span></span><br><span class="line"><span class="bullet">              -</span> <span class="string">bash</span></span><br><span class="line"><span class="bullet">              -</span> <span class="string">/data/service/go_workspace/script/health-check/check-entrance.sh</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">120</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.nginx.image.repository &#125;&#125;</span>:<span class="template-variable">&#123;&#123; .Values.nginx.image.tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">entrance-mirror</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">          - containerPort:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">          - mountPath:</span> <span class="string">/data/athena/</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">            readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">          - mountPath:</span> <span class="string">/etc/nginx/nginx.conf</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">nginx-config</span></span><br><span class="line"><span class="attr">            subPath:</span> <span class="string">nginx.conf</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">1000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">500</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          tcpSocket:</span></span><br><span class="line"><span class="attr">            port:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.entrance.service.nodePort</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">        entrance:</span> <span class="string">"true"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">athena</span></span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">"/data/athena/"</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nginx-config</span></span><br><span class="line"><span class="attr">          configMap:</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">entrance-nginx-config</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      imagePullSecrets:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">"<span class="template-variable">&#123;&#123; .Values.imagePullSecrets &#125;&#125;</span>"</span></span><br><span class="line"><span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br></pre></td></tr></table></figure><p>上面为真实在业务中使用的 Deployment 配置, 有些地方可以参考:</p><ul><li><code>valueFrom.fieldRef.fieldPath</code> 可以取到容器运行时的一些字段, 如 <code>NodeIP</code>, <code>PodIP</code> 这些可以用于全链路监控</li><li><code>ConfigMap</code> 直接 Mount 到文件系统, 覆盖默认配置的例子</li><li><code>affinity.podAntiAffinity</code> 亲和性调度, 使 Pod 在主机间均匀分布</li><li>使用了 <code>tcpSocket</code> 和 <code>exec.command</code> 两种健康检查方式</li></ul><h3 id="Helm-Values"><a href="#Helm-Values" class="headerlink" title="Helm Values"></a>Helm Values</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># entrance, Athena 上报入口模块</span></span><br><span class="line"><span class="attr">entrance:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  replicaCount:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  mirrorEnable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  mirrorReplicaCount:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  mirrorProxyPass:</span> <span class="string">"http://10.16.0.147/entrance/"</span></span><br><span class="line"><span class="attr">  image:</span></span><br><span class="line"><span class="attr">    repository:</span> <span class="string">athena-go</span></span><br><span class="line"><span class="attr">    tag:</span> <span class="string">v1901091026</span></span><br><span class="line"><span class="attr">  service:</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30081</span></span><br><span class="line"><span class="attr">    nodePortMirror:</span> <span class="number">30082</span></span><br></pre></td></tr></table></figure><p>如上, <code>replicaCount: 3</code> + <code>mirrorReplicaCount: 1</code> = 4 个容器, 有 1/4 流量复制到 <code>http://10.16.0.147/entrance/</code></p><h3 id="内网负载均衡"><a href="#内网负载均衡" class="headerlink" title="内网负载均衡"></a>内网负载均衡</h3><p>流量复制到测试环境时, 尽量使用内网负载均衡, 为了成本, 安全及性能方面的考虑</p><p><img src="https://github.com/TencentCloudContainerTeam/TencentCloudContainerTeam.github.io/raw/develop/source/_posts/res/k8s-traffic-copy/lb-inner.png" alt="LB-inner-config"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过下面几个步骤, 便可以实现流量复制啦</p><ul><li>建一个内网负载均衡, 暴漏测试环境的 <code>服务入口 Service</code></li><li><code>服务入口 Service</code> 需要有可以更换端口号的能力 (例如命令行参数/环境变量)</li><li>线上环境, 新增一个 Deployment, Label 和之前的 <code>服务入口 Service</code> 一样, 只是端口号分配一个新的</li><li>为新增的 Deployment 增加一个 Nginx 容器, 配置 nginx.conf</li><li>调节有 <code>Nginx Mirror</code> 的 Pod 和 正常的 <code>Pod</code> 比例, 便可以实现<code>按比例流量复制</code></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者：田小康&lt;/p&gt;
&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;测试环境没有真实的数据, 会导致很多测试工作难以展开, 尤其是一些测试任务需要使用生产环境来做时, 会极大影响现
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Cgroup泄漏--潜藏在你的集群中</title>
    <link href="https://TencentCloudContainerTeam.github.io/2018/12/29/cgroup-leaking/"/>
    <id>https://TencentCloudContainerTeam.github.io/2018/12/29/cgroup-leaking/</id>
    <published>2018-12-29T09:00:00.000Z</published>
    <updated>2019-01-10T06:19:59.604Z</updated>
    
    <content type="html"><![CDATA[<p>作者: <a href="https://github.com/honkiko" target="_blank" rel="noopener">洪志国</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>绝大多数的kubernetes集群都有这个隐患。只不过一般情况下，泄漏得比较慢，还没有表现出来而已。 </p><p>一个pod可能泄漏两个memory cgroup数量配额。即使pod百分之百发生泄漏， 那也需要一个节点销毁过三万多个pod之后，才会造成后续pod创建失败。</p><p>一旦表现出来，这个节点就彻底不可用了，必须重启才能恢复。</p><h2 id="故障表现"><a href="#故障表现" class="headerlink" title="故障表现"></a>故障表现</h2><p>腾讯云SCF(Serverless Cloud Function)底层使用我们的TKE(Tencent Kubernetes Engine)，并且会在节点上频繁创建和消耗容器。</p><p>SCF发现很多节点会出现类似以下报错，创建POD总是失败：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dec 24 11:54:31 VM_16_11_centos dockerd[11419]: time=&quot;2018-12-24T11:54:31.195900301+08:00&quot; level=error msg=&quot;Handler for POST /v1.31/containers/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b/start returned error: OCI runtime create failed: container_linux.go:348: starting container process caused \&quot;process_linux.go:279: applying cgroup configuration for process caused \\\&quot;mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod79fe803c-072f-11e9-90ca-525400090c71/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b: no space left on device\\\&quot;\&quot;: unknown&quot;</span><br></pre></td></tr></table></figure><p>这个时候，到节点上尝试创建几十个memory cgroup (以root权限执行 <code>for i in</code>seq 1 20<code>;do mkdir /sys/fs/cgroup/memory/${i}; done</code>)，就会碰到失败：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir: cannot create directory &apos;/sys/fs/cgroup/memory/8&apos;: No space left on device</span><br></pre></td></tr></table></figure><p>其实，dockerd出现以上报错时， 手动创建<strong><em>一个</em></strong>memory cgroup都会失败的。 不过有时候随着一些POD的运行结束，可能会多出来一些“配额”，所以这里是尝试创建20个memory cgroup。</p><p>出现这样的故障以后，重启docker，释放内存等措施都没有效果，只有重启节点才能恢复。</p><h2 id="复现条件"><a href="#复现条件" class="headerlink" title="复现条件"></a>复现条件</h2><p>docker和kubernetes社区都有关于这个问题的issue:</p><ul><li><a href="https://github.com/moby/moby/issues/29638" target="_blank" rel="noopener">https://github.com/moby/moby/issues/29638</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/70324" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/70324</a></li></ul><p>网上有文章介绍了类似问题的分析和复现方法。如：<br><a href="http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage" target="_blank" rel="noopener">http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage</a></p><p>不过按照文中的复现方法，我在<code>3.10.0-862.9.1.el7.x86_64</code>版本内核上并没有复现出来。</p><p>经过反复尝试，总结出了必现的复现条件。 一句话感慨就是，把进程加入到一个开启了kmem accounting的memory cgroup<strong><em>并且执行fork系统调用</em></strong>。</p><ol><li>centos 3.10.0-862.9.1.el7.x86_64及以下内核， 4G以上空闲内存，root权限。</li><li><p>把系统memory cgroup配额占满</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in `seq 1 65536`;do mkdir /sys/fs/cgroup/memory/$&#123;i&#125;; done</span><br></pre></td></tr></table></figure><p> 会看到报错：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir: cannot create directory ‘/sys/fs/cgroup/memory/65530’: No space left on device</span><br></pre></td></tr></table></figure><p> 这是因为这个版本内核写死了，最多只能有65535个memory cgroup共存。 systemd已经创建了一些，所以这里创建不到65535个就会遇到报错。</p><p> 确认删掉一个memory cgroup, 就能腾出一个“配额”：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rmdir /sys/fs/cgroup/memory/1</span><br><span class="line">mkdir /sys/fs/cgroup/memory/test</span><br></pre></td></tr></table></figure></li><li><p>给一个memory cgroup开启kmem accounting</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /sys/fs/cgroup/memory/test/</span><br><span class="line">echo 1 &gt; memory.kmem.limit_in_bytes</span><br><span class="line">echo -1 &gt; memory.kmem.limit_in_bytes</span><br></pre></td></tr></table></figure></li><li><p>把一个进程加进某个memory cgroup, 并执行一次fork系统调用</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">最简单的就是把当前shell进程加进去：</span><br><span class="line">echo $$ &gt; /sys/fs/cgroup/memory/test/tasks</span><br><span class="line">sleep 100 &amp;</span><br><span class="line">cat /sys/fs/cgroup/memory/test/tasks</span><br></pre></td></tr></table></figure></li><li><p>把该memory cgroup里面的进程都挪走</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for p in `cat /sys/fs/cgroup/memory/test/tasks`;do echo $&#123;p&#125; &gt; /sys/fs/cgroup/memory/tasks; done</span><br><span class="line"></span><br><span class="line">cat /sys/fs/cgroup/memory/test/tasks  //这时候应该为空</span><br></pre></td></tr></table></figure></li><li><p>删除这个memory cgroup</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmdir /sys/fs/cgroup/memory/test</span><br></pre></td></tr></table></figure></li><li><p>验证刚才删除一个memory cgroup， 所占的配额并没有释放</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /sys/fs/cgroup/memory/xx</span><br></pre></td></tr></table></figure><p> 这时候会报错：<code>mkdir: cannot create directory ‘/sys/fs/cgroup/memory/xx’: No space left on device</code></p></li></ol><h2 id="什么版本的内核有这个问题"><a href="#什么版本的内核有这个问题" class="headerlink" title="什么版本的内核有这个问题"></a>什么版本的内核有这个问题</h2><p>搜索内核commit记录，有一个commit应该是解决类似问题的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4bdfc1c4a943: 2015-01-08 memcg: fix destination cgroup leak on task charges migration [Vladimir Davydov]</span><br></pre></td></tr></table></figure><p>这个commit在3.19以及4.x版本的内核中都已经包含。 不过从docker和kubernetes相关issue里面的反馈来看，内核中应该还有其他cgroup泄漏的代码路径， 4.14版本内核都还有cgroup泄漏问题。</p><h2 id="规避办法"><a href="#规避办法" class="headerlink" title="规避办法"></a>规避办法</h2><p>不开启kmem accounting (以上复现步骤的第3步)的话，是不会发生cgroup泄漏的。</p><p>kubelet和runc都会给memory cgroup开启kmem accounting。所以要规避这个问题，就要保证kubelet和runc，都别开启kmem accounting。下面分别进行说明。 </p><h2 id="runc"><a href="#runc" class="headerlink" title="runc"></a>runc</h2><p>查看代码，发现在commit fe898e7 (2017-2-25, PR #1350)以后的runc版本中，都会默认开启kmem accounting。代码在libcontainer/cgroups/fs/kmem.go: (老一点的版本，代码在libcontainer/cgroups/fs/memory.go)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">const cgroupKernelMemoryLimit = &quot;memory.kmem.limit_in_bytes&quot;</span><br><span class="line"></span><br><span class="line">func EnableKernelMemoryAccounting(path string) error &#123;</span><br><span class="line">        // Ensure that kernel memory is available in this kernel build. If it</span><br><span class="line">        // isn&apos;t, we just ignore it because EnableKernelMemoryAccounting is</span><br><span class="line">        // automatically called for all memory limits.</span><br><span class="line">        if !cgroups.PathExists(filepath.Join(path, cgroupKernelMemoryLimit)) &#123;</span><br><span class="line">                return nil</span><br><span class="line">        &#125;</span><br><span class="line">        // We have to limit the kernel memory here as it won&apos;t be accounted at all</span><br><span class="line">        // until a limit is set on the cgroup and limit cannot be set once the</span><br><span class="line">        // cgroup has children, or if there are already tasks in the cgroup.</span><br><span class="line">        for _, i := range []int64&#123;1, -1&#125; &#123;</span><br><span class="line">                if err := setKernelMemory(path, i); err != nil &#123;</span><br><span class="line">                        return err</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>runc社区也注意到这个问题，并做了比较灵活的修复： <a href="https://github.com/opencontainers/runc/pull/1921" target="_blank" rel="noopener">https://github.com/opencontainers/runc/pull/1921</a></p><p>这个修复给runc增加了”nokmem”编译选项。缺省的release版本没有使用这个选项。 自己使用nokmem选项编译runc的方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $GO_PATH/src/github.com/opencontainers/runc/</span><br><span class="line">make BUILDTAGS=&quot;seccomp nokmem&quot;</span><br></pre></td></tr></table></figure><h2 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h2><p>kubelet在创建pod对应的cgroup目录时，也会调用libcontianer中的代码对cgroup做设置。在   <code>pkg/kubelet/cm/cgroup_manager_linux.go</code>的Create方法中，会调用Manager.Apply方法，最终调用<code>vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go</code>中的MemoryGroup.Apply方法，开启kmem accounting。</p><p>这里也需要进行处理，可以不开启kmem accounting， 或者通过命令行参数来控制是否开启。</p><p>kubernetes社区也有issue讨论这个问题：<a href="https://github.com/kubernetes/kubernetes/issues/70324" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/70324</a></p><p>但是目前还没有结论。我们TKE先直接把这部分代码注释掉了，不开启kmem accounting。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者: &lt;a href=&quot;https://github.com/honkiko&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;洪志国&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>给容器设置内核参数</title>
    <link href="https://TencentCloudContainerTeam.github.io/2018/11/19/kernel-parameters-and-container/"/>
    <id>https://TencentCloudContainerTeam.github.io/2018/11/19/kernel-parameters-and-container/</id>
    <published>2018-11-19T13:52:00.000Z</published>
    <updated>2019-01-10T06:19:59.604Z</updated>
    
    <content type="html"><![CDATA[<p>作者： 洪志国</p><h1 id="sysctl"><a href="#sysctl" class="headerlink" title="sysctl"></a>sysctl</h1><p>/proc/sys/目录下导出了一些可以在运行时修改kernel参数的proc文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ls /proc/sys</span><br><span class="line">abi  crypto  debug  dev  fs  kernel  net  vm</span><br></pre></td></tr></table></figure><p>可以通过写proc文件来修改这些内核参数。例如， 要打开ipv4的路由转发功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure><p>也可以通过sysctl命令来完成(只是对以上写proc文件操作的简单包装)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.ip_forward=1</span><br></pre></td></tr></table></figure><p>其他常用sysctl命令：</p><p>显示本机所有sysctl内核参数及当前值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a</span><br></pre></td></tr></table></figure><p>从文件(缺省使用/etc/sysctl.conf)加载多个参数和取值，并写入内核</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -p [FILE]</span><br></pre></td></tr></table></figure><p>另外， 系统启动的时候， 会自动执行一下”sysctl -p”。 所以，希望重启之后仍然生效的参数值， 应该写到/etc/sysctl.conf文件里面。</p><h1 id="容器与sysctl"><a href="#容器与sysctl" class="headerlink" title="容器与sysctl"></a>容器与sysctl</h1><p>内核方面做了大量的工作，把一部分sysctl内核参数进行了namespace化(namespaced)。 也就是多个容器和主机可以各自独立设置某些内核参数。例如， 可以通过net.ipv4.ip_local_port_range，在不同容器中设置不同的端口范围。</p><p>如何判断一个参数是不是namespaced? </p><p>运行一个具有privileged权限的容器(参考下一节内容)， 然后在容器中修改该参数，看一下在host上能否看到容器在中所做的修改。如果看不到， 那就是namespaced， 否则不是。</p><p>目前已经namespace化的sysctl内核参数：</p><ul><li>kernel.shm*,</li><li>kernel.msg*,</li><li>kernel.sem,</li><li>fs.mqueue.*,</li><li>net.*.</li></ul><p>注意， vm.*并没有namespace化。 比如vm.max_map_count， 在主机或者一个容器中设置它， 其他所有容器都会受影响，都会看到最新的值。</p><h1 id="在docker容器中修改sysctl内核参数"><a href="#在docker容器中修改sysctl内核参数" class="headerlink" title="在docker容器中修改sysctl内核参数"></a>在docker容器中修改sysctl内核参数</h1><p>正常运行的docker容器中，是不能修改任何sysctl内核参数的。因为/proc/sys是以只读方式挂载到容器里面的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)</span><br></pre></td></tr></table></figure><p>要给容器设置不一样的sysctl内核参数，有多种方式。</p><h3 id="方法一-–privileged"><a href="#方法一-–privileged" class="headerlink" title="方法一 –privileged"></a>方法一 –privileged</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker run --privileged -it ubuntu bash</span><br></pre></td></tr></table></figure><p>整个/proc目录都是以”rw”权限挂载的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)</span><br></pre></td></tr></table></figure></p><p>在容器中，可以任意修改sysctl内核参赛。</p><p>注意：<br>如果修改的是namespaced的参数， 则不会影响host和其他容器。反之，则会影响它们。</p><p>如果想在容器中修改主机的net.ipv4.ip_default_ttl参数， 则除了–privileged， 还需要加上 –net=host。</p><h3 id="方法二-把-proc-sys-bind到容器里面"><a href="#方法二-把-proc-sys-bind到容器里面" class="headerlink" title="方法二 把/proc/sys bind到容器里面"></a>方法二 把/proc/sys bind到容器里面</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># docker run -v /proc/sys:/writable-sys -it ubuntu bash</span><br></pre></td></tr></table></figure><p>然后写bind到容器内的proc文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 62 &gt; /writable-sys/net/ipv4/ip_default_ttl</span><br></pre></td></tr></table></figure><p>注意：  这样操作，效果类似于”–privileged”, 对于namespaced的参数，不会影响host和其他容器。</p><h3 id="方法三-–sysctl"><a href="#方法三-–sysctl" class="headerlink" title="方法三 –sysctl"></a>方法三 –sysctl</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># docker run -it --sysctl &apos;net.ipv4.ip_default_ttl=63&apos; ubuntu sysctl net.ipv4.ip_default_ttl</span><br><span class="line">net.ipv4.ip_default_ttl = 63</span><br></pre></td></tr></table></figure><p>注意:</p><ul><li>只有namespaced参数才可以。否则会报错”invalid argument…”</li><li>这种方式只是在容器初始化过程中完成内核参数的修改，容器运行起来以后，/proc/sys仍然是以只读方式挂载的，在容器中不能再次修改sysctl内核参数。</li></ul><h1 id="kubernetes-与-sysctl"><a href="#kubernetes-与-sysctl" class="headerlink" title="kubernetes 与 sysctl"></a>kubernetes 与 sysctl</h1><h3 id="方法一-通过sysctls和unsafe-sysctls-annotation"><a href="#方法一-通过sysctls和unsafe-sysctls-annotation" class="headerlink" title="方法一 通过sysctls和unsafe-sysctls annotation"></a>方法一 通过sysctls和unsafe-sysctls annotation</h3><p>k8s还进一步把syctl参数分为safe和unsafe。 safe的条件：</p><ul><li>must not have any influence on any other pod on the node</li><li>must not allow to harm the node’s health</li><li>must not allow to gain CPU or memory resources outside of the resource limits of a pod.</li></ul><p>非namespaced的参数，肯定是unsafe。</p><p>namespaced参数，也只有一部分被认为是safe的。</p><p>在pkg/kubelet/sysctl/whitelist.go中维护了safe sysctl参数的名单。在1.7.8的代码中，只有三个参数被认为是safe的：</p><ul><li>kernel.shm_rmid_forced,</li><li>net.ipv4.ip_local_port_range,</li><li>net.ipv4.tcp_syncookies</li></ul><p>如果要设置一个POD中safe参数，通过security.alpha.kubernetes.io/sysctls这个annotation来传递给kubelet。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">metadata:</span><br><span class="line">  name: sysctl-example</span><br><span class="line">  annotations:</span><br><span class="line">    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=1</span><br></pre></td></tr></table></figure><p>如果要设置一个namespaced， 但是unsafe的参数，要使用另一个annotation: security.alpha.kubernetes.io/unsafe-sysctls， 另外还要给kubelet一个特殊的启动参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: sysctl-example</span><br><span class="line">  annotations:</span><br><span class="line">    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=1</span><br><span class="line">    security.alpha.kubernetes.io/unsafe-sysctls: net.ipv4.route.min_pmtu=1000,kernel.msgmax=1 2 3</span><br><span class="line">spec:</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>kubelet 增加–experimental-allowed-unsafe-sysctls启动参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubelet --experimental-allowed-unsafe-sysctls &apos;kernel.msg*,net.ipv4.route.min_pmtu&apos;</span><br></pre></td></tr></table></figure><h3 id="方法二-privileged-POD"><a href="#方法二-privileged-POD" class="headerlink" title="方法二 privileged POD"></a>方法二 privileged POD</h3><p>如果要修改的是非namespaced的参数， 如vm.*， 那就没办法使用以上方法。 可以给POD privileged权限，然后在容器的初始化脚本或代码中去修改sysctl参数。</p><p>创建POD/deployment/daemonset等对象时， 给容器的spec指定securityContext.privileged=true<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: nginx:alpine</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: true</span><br></pre></td></tr></table></figure></p><p>这样跟”docker run –privileged”效果一样，在POD中/proc是以”rw”权限mount的，可以直接修改相关sysctl内核参数。</p><h1 id="ulimit"><a href="#ulimit" class="headerlink" title="ulimit"></a>ulimit</h1><p>每个进程都有若干操作系统资源的限制， 可以通过 /proc/$PID/limits 来查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/1/limits </span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        0                    unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             62394                62394                processes </span><br><span class="line">Max open files            1024                 4096                 files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       62394                62394                signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us</span><br></pre></td></tr></table></figure><p>在bash中有个ulimit内部命令，可以查看当前bash进程的这些限制。</p><p>跟ulimit属性相关的配置文件是/etc/security/limits.conf。具体配置项和语法可以通过<code>man limits.conf</code> 命令查看。</p><h3 id="systemd给docker-daemon自身配置ulimit"><a href="#systemd给docker-daemon自身配置ulimit" class="headerlink" title="systemd给docker daemon自身配置ulimit"></a>systemd给docker daemon自身配置ulimit</h3><p>在service文件中(一般是/usr/lib/systemd/system/dockerd.service)中可以配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">LimitAS=infinity</span><br><span class="line">LimitRSS=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">ExecStart=...</span><br><span class="line">WorkingDirectory=...</span><br><span class="line">User=...</span><br><span class="line">Group=...</span><br></pre></td></tr></table></figure><h3 id="dockerd-给容器的-缺省ulimit设置"><a href="#dockerd-给容器的-缺省ulimit设置" class="headerlink" title="dockerd 给容器的 缺省ulimit设置"></a>dockerd 给容器的 缺省ulimit设置</h3><p>dockerd –default-ulimit nofile=65536:65536</p><p>冒号前面是soft limit, 后面是hard limit</p><h3 id="给容器指定ulimit设置"><a href="#给容器指定ulimit设置" class="headerlink" title="给容器指定ulimit设置"></a>给容器指定ulimit设置</h3><p>docker run -d –ulimit nofile=20480:40960 nproc=1024:2048 容器名</p><h3 id="在kubernetes中给pod设置ulimit参数"><a href="#在kubernetes中给pod设置ulimit参数" class="headerlink" title="在kubernetes中给pod设置ulimit参数"></a>在kubernetes中给pod设置ulimit参数</h3><p>有一个issue在讨论这个问题： <a href="https://github.com/kubernetes/kubernetes/issues/3595" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/3595</a></p><p>目前可行的办法，是在镜像中的初始化程序中调用setrlimit()系统调用来进行设置。子进程会继承父进程的ulimit参数。</p><h1 id="参考文档："><a href="#参考文档：" class="headerlink" title="参考文档："></a>参考文档：</h1><p><a href="http://tapd.oa.com/CCCM/prong/stories/view/1010166561060564549" target="_blank" rel="noopener">http://tapd.oa.com/CCCM/prong/stories/view/1010166561060564549</a></p><p><a href="https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/</a></p><p><a href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者： 洪志国&lt;/p&gt;
&lt;h1 id=&quot;sysctl&quot;&gt;&lt;a href=&quot;#sysctl&quot; class=&quot;headerlink&quot; title=&quot;sysctl&quot;&gt;&lt;/a&gt;sysctl&lt;/h1&gt;&lt;p&gt;/proc/sys/目录下导出了一些可以在运行时修改kernel参数的proc
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NodePort, svc, LB直通Pod性能测试对比</title>
    <link href="https://TencentCloudContainerTeam.github.io/2018/11/06/NodePort-SVC-LB%E7%9B%B4%E9%80%9A%E5%AE%B9%E5%99%A8%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%AF%B9%E6%AF%94/"/>
    <id>https://TencentCloudContainerTeam.github.io/2018/11/06/NodePort-SVC-LB直通容器性能测试对比/</id>
    <published>2018-11-06T07:45:37.000Z</published>
    <updated>2019-01-10T06:19:59.604Z</updated>
    
    <content type="html"><![CDATA[<p>作者：郭志宏 </p><h3 id="1-测试背景："><a href="#1-测试背景：" class="headerlink" title="1. 测试背景："></a>1. 测试背景：</h3><p>目前基于k8s 服务的外网访问方式有以下几种：</p><ol><li>NodePort  </li><li>svc(通过k8s 的clusterip 访问)</li><li>自研 LB -&gt; Pod （比如pod ip 作为 nginx 的 upstream, 或者社区的nginx-ingress）</li></ol><p>其中第一种和第二种方案都要经过iptables 转发，第三种方案不经过iptables，本测试主要是为了测试这三种方案的性能损耗。</p><h3 id="2-测试方案"><a href="#2-测试方案" class="headerlink" title="2. 测试方案"></a>2. 测试方案</h3><p>为了做到测试的准确性和全面性，我们提供以下测试工具和测试数据：</p><ol><li><p>2核4G 的Pod</p></li><li><p>5个Node 的4核8G 集群</p></li><li><p>16核32G 的Nginx 作为统一的LB</p></li><li><p>一个测试应用，2个静态测试接口，分别对用不同大小的数据包（4k 和 100K）</p></li><li><p>测试1个pod ，10个pod的情况（service/pod 越多，一个机器上的iptables 规则数就越多，关于iptables规则数对转发性能的影响，在“ipvs和iptables模式下性能对⽐比测试报告” 已有结论： Iptables场景下，对应service在总数为2000内时，每个service 两个pod, 性能没有明显下降。当service总数达到3000、4000时，性能下降明显，service个数越多，性能越差。）所以这里就不考虑pod数太多的情况。</p></li><li><p>单独的16核32G 机器作作为压力机，使用wrk 作为压测工具, qps 作为评估标准，</p></li><li><p>那么每种访问方式对应以下4种情况</p></li></ol><table><thead><tr><th>测试用例</th><th>Pod 数</th><th>数据包大小</th><th>平均QPS</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>4k</td><td></td></tr><tr><td>2</td><td>1</td><td>100K</td><td></td></tr><tr><td>3</td><td>10</td><td>4k</td><td></td></tr><tr><td>4</td><td>10</td><td>100k</td></tr></tbody></table><ol start="8"><li>每种情况测试5次，取平均值（qps），完善上表。</li></ol><h3 id="3-测试过程"><a href="#3-测试过程" class="headerlink" title="3. 测试过程"></a>3. 测试过程</h3><ol><li><p>准备一个测试应用（基于nginx），提供两个静态文件接口，分别返回4k的数据和100K 的数据。</p><p>镜像地址：ccr.ccs.tencentyun.com/caryguo/nginx:v0.1</p><p>接口：<a href="http://0.0.0.0/4k.html" target="_blank" rel="noopener">http://0.0.0.0/4k.html</a></p><p>​            <a href="http://0.0.0.0/100k.htm" target="_blank" rel="noopener">http://0.0.0.0/100k.htm</a></p></li><li><p>部署压测工具。<a href="https://github.com/wg/wrk" target="_blank" rel="noopener">https://github.com/wg/wrk</a></p></li><li><p>部署集群，5台Node来调度测试Pod, 10.0.4.6 这台用来独部署Nginx, 作为统一的LB, 将这台机器加入集群的目的是为了 将ClusterIP 作为nginx 的upstream .</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@VM-4-6-ubuntu:/etc/nginx# kubectl get node</span><br><span class="line">NAME        STATUS                     ROLES     AGE       VERSION</span><br><span class="line">10.0.4.12   Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.3    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.5    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.6    Ready,SchedulingDisabled   &lt;none&gt;    12m       v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.7    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br><span class="line">10.0.4.9    Ready                      &lt;none&gt;    3d        v1.10.5-qcloud-rev1</span><br></pre></td></tr></table></figure></li><li><p>根据不同的测试场景，调整Nginx 的upstream, 根据不同的Pod, 调整压力，让请求的超时率控制在万分之一以内,  数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./wrk -c 200 -d 20 -t 10 http://carytest.pod.com/10k.html   单pod</span><br><span class="line">./wrk -c 1000 -d 20 -t 100 http://carytest.pod.com/4k.html  10 pod</span><br></pre></td></tr></table></figure></li><li><p>测试wrk -&gt; nginx -&gt; Pod 场景，</p></li></ol><table><thead><tr><th>测试用例</th><th>Pod 数</th><th>数据包大小</th><th>平均QPS</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>4k</td><td>12498</td></tr><tr><td>2</td><td>1</td><td>100K</td><td>2037</td></tr><tr><td>3</td><td>10</td><td>4k</td><td>82752</td></tr><tr><td>4</td><td>10</td><td>100k</td><td>7743</td></tr></tbody></table><ol start="5"><li>wrk -&gt; nginx -&gt; ClusterIP -&gt; Pod</li></ol><table><thead><tr><th>测试用例</th><th>Pod 数</th><th>数据包大小</th><th>平均QPS</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>4k</td><td>12568</td></tr><tr><td>2</td><td>1</td><td>100K</td><td>2040</td></tr><tr><td>3</td><td>10</td><td>4k</td><td>81752</td></tr><tr><td>4</td><td>10</td><td>100k</td><td>7824</td></tr></tbody></table><ol start="6"><li>NodePort 场景，wrk -&gt; nginx -&gt; NodePort -&gt; Pod</li></ol><table><thead><tr><th>测试用例</th><th>Pod 数</th><th>数据包大小</th><th>平均QPS</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>4k</td><td>12332</td></tr><tr><td>2</td><td>1</td><td>100K</td><td>2028</td></tr><tr><td>3</td><td>10</td><td>4k</td><td>76973</td></tr><tr><td>4</td><td>10</td><td>100k</td><td>5676</td></tr></tbody></table><p>压测过程中，4k 数据包的情况下，应用的负载都在80% -100% 之间， 100k 情况下，应用的负载都在20%-30%</p><p>之间，压力都在网络消耗上，没有到达服务后端。</p><h3 id="4-测试结论"><a href="#4-测试结论" class="headerlink" title="4. 测试结论"></a>4. 测试结论</h3><ol><li>在一个pod 的情况下（4k 或者100 数据包），3中网络方案差别不大，QPS 差距在3% 以内。</li><li>在10个pod，4k 数据包情况下，lb-&gt;pod 和 svc 差距不大，NodePort 损失近7% 左右。</li><li>10个Pod, 100k 数据包的情况下，lb-&gt;pod 和 svc 差距不大，NodePort 损失近 25% </li></ol><h3 id="5-附录"><a href="#5-附录" class="headerlink" title="5. 附录"></a>5. 附录</h3><ol><li>nginx 配置</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">user nginx;</span><br><span class="line">worker_processes 50;</span><br><span class="line">error_log /var/log/nginx/error.log;</span><br><span class="line">pid /run/nginx.pid;</span><br><span class="line"></span><br><span class="line"># Load dynamic modules. See /usr/share/nginx/README.dynamic.</span><br><span class="line">include /usr/share/nginx/modules/*.conf;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 100000;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">                      &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">                      &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    keepalive_timeout   65;</span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line"></span><br><span class="line">    include             /etc/nginx/mime.types;</span><br><span class="line">    default_type        application/octet-stream;</span><br><span class="line"></span><br><span class="line">    # Load modular configuration files from the /etc/nginx/conf.d directory.</span><br><span class="line">    # See http://nginx.org/en/docs/ngx_core_module.html#include</span><br><span class="line">    # for more information.</span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    </span><br><span class="line">     # pod ip</span><br><span class="line">    upstream  panda-pod &#123;</span><br><span class="line">          #ip_hash;</span><br><span class="line">          # Pod ip</span><br><span class="line">          #server   10.0.4.12:30734  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.1.5:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.2.3:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.3.5:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.4.6:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.4.5:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.3.6:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.1.4:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.0.7:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.0.6:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          #server   172.16.2.2:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">          </span><br><span class="line">          # svc ip</span><br><span class="line">          #server   172.16.255.121:80  max_fails=2 fail_timeout=30s;</span><br><span class="line">           </span><br><span class="line">          # NodePort</span><br><span class="line">          server   10.0.4.12:30734   max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.3:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.5:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.7:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">          server   10.0.4.9:30734    max_fails=2 fail_timeout=30s;</span><br><span class="line">    </span><br><span class="line">          keepalive 256;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  carytest.pod.com;</span><br><span class="line">        # root   /usr/share/nginx/html;</span><br><span class="line">        charset utf-8;</span><br><span class="line"></span><br><span class="line">        # Load configuration files for the default server block.</span><br><span class="line">        include /etc/nginx/default.d/*.conf;</span><br><span class="line">        location / &#123;</span><br><span class="line">                    proxy_pass        http://panda-pod;</span><br><span class="line">                    proxy_http_version 1.1;</span><br><span class="line">                    proxy_set_header Connection &quot;&quot;;</span><br><span class="line">                    proxy_redirect off;</span><br><span class="line">                    proxy_set_header  Host  $host;</span><br><span class="line">                    proxy_set_header  X-Real-IP  $remote_addr;</span><br><span class="line">                    proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;</span><br><span class="line">                    proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page 404 /404.html;</span><br><span class="line">            location = /40x.html &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page 500 502 503 504 /50x.html;</span><br><span class="line">            location = /50x.html &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者：郭志宏 &lt;/p&gt;
&lt;h3 id=&quot;1-测试背景：&quot;&gt;&lt;a href=&quot;#1-测试背景：&quot; class=&quot;headerlink&quot; title=&quot;1. 测试背景：&quot;&gt;&lt;/a&gt;1. 测试背景：&lt;/h3&gt;&lt;p&gt;目前基于k8s 服务的外网访问方式有以下几种：&lt;/p&gt;
&lt;ol&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>K8s Network Policy Controller之Kube-router功能介绍</title>
    <link href="https://TencentCloudContainerTeam.github.io/2018/10/30/k8s-npc-kr-function/"/>
    <id>https://TencentCloudContainerTeam.github.io/2018/10/30/k8s-npc-kr-function/</id>
    <published>2018-10-30T09:22:24.000Z</published>
    <updated>2019-01-10T06:19:59.604Z</updated>
    
    <content type="html"><![CDATA[<p>Author: <a href="https://github.com/jimmy-zh" target="_blank" rel="noopener">Jimmy Zhang</a> (张浩)</p><h1 id="Network-Policy"><a href="#Network-Policy" class="headerlink" title="Network Policy"></a>Network Policy</h1><p><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" target="_blank" rel="noopener">Network Policy</a>是k8s提供的一种资源，用于定义基于pod的网络隔离策略。它描述了一组pod是否可以与其它组pod，以及其它network endpoints进行通信。</p><h1 id="Kube-router"><a href="#Kube-router" class="headerlink" title="Kube-router"></a>Kube-router</h1><ul><li>官网:  <a href="https://www.kube-router.io" target="_blank" rel="noopener">https://www.kube-router.io</a></li><li>项目:  <a href="https://github.com/cloudnativelabs/kube-router" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router</a></li><li>目前最新版本：<a href="https://github.com/cloudnativelabs/kube-router/releases/tag/v0.2.1" target="_blank" rel="noopener">v0.2.1</a></li></ul><p>kube-router项目的三大功能：</p><ul><li>Pod Networking</li><li>IPVS/LVS based service proxy  </li><li>Network Policy Controller </li></ul><p>在腾讯云TKE上，Pod Networking功能由基于IAAS层VPC的高性能容器网络实现，service proxy功能由kube-proxy所支持的ipvs/iptables两种模式实现。建议在TKE上，只使用kube-router的Network Policy功能。</p><h1 id="在TKE上部署kube-router"><a href="#在TKE上部署kube-router" class="headerlink" title="在TKE上部署kube-router"></a>在TKE上部署kube-router</h1><h3 id="腾讯云提供的kube-router版本"><a href="#腾讯云提供的kube-router版本" class="headerlink" title="腾讯云提供的kube-router版本"></a>腾讯云提供的kube-router版本</h3><p>腾讯云PAAS团队提供的镜像”ccr.ccs.tencentyun.com/library/kube-router:v1”基于官方的最新版本：<a href="https://github.com/cloudnativelabs/kube-router/releases/tag/v0.2.1" target="_blank" rel="noopener">v0.2.1</a></p><p>在该项目的开发过程中，腾讯云PAAS团队积极参与社区，持续贡献了一些feature support和bug fix, 列表如下（均已被社区合并）：</p><ul><li><a href="https://github.com/cloudnativelabs/kube-router/pull/488" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/488</a></li><li><a href="https://github.com/cloudnativelabs/kube-router/pull/498" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/498</a></li><li><a href="https://github.com/cloudnativelabs/kube-router/pull/527" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/527</a></li><li><a href="https://github.com/cloudnativelabs/kube-router/pull/529" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/529</a></li><li><a href="https://github.com/cloudnativelabs/kube-router/pull/543" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-router/pull/543</a></li></ul><p>我们会继续贡献社区，并提供腾讯云镜像的版本升级。</p><h3 id="部署kube-router"><a href="#部署kube-router" class="headerlink" title="部署kube-router"></a>部署kube-router</h3><p>Daemonset yaml文件:</p><blockquote><p><a href="https://ask.qcloudimg.com/draft/982360/9wn7eu0bek.zip" target="_blank" rel="noopener">#kube-router-firewall-daemonset.yaml.zip#</a></p></blockquote><p>在<strong>能访问公网</strong>，也能访问TKE集群apiserver的机器上，执行以下命令即可完成kube-router部署。</p><p>如果集群节点开通了公网IP，则可以直接在集群节点上执行以下命令。</p><p>如果集群节点没有开通公网IP, 则可以手动下载和粘贴yaml文件内容到节点, 保存为kube-router-firewall-daemonset.yaml，再执行最后的kubectl create命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://ask.qcloudimg.com/draft/982360/9wn7eu0bek.zip</span><br><span class="line">unzip 9wn7eu0bek.zip</span><br><span class="line">kuebectl create -f kube-router-firewall-daemonset.yaml</span><br></pre></td></tr></table></figure><h3 id="yaml文件内容和参数说明"><a href="#yaml文件内容和参数说明" class="headerlink" title="yaml文件内容和参数说明"></a>yaml文件内容和参数说明</h3><p>kube-router-firewall-daemonset.yaml文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-router-cfg</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    tier: node</span><br><span class="line">    k8s-app: kube-router</span><br><span class="line">data:</span><br><span class="line">  cni-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;:&quot;kubernetes&quot;,</span><br><span class="line">      &quot;type&quot;:&quot;bridge&quot;,</span><br><span class="line">      &quot;bridge&quot;:&quot;kube-bridge&quot;,</span><br><span class="line">      &quot;isDefaultGateway&quot;:true,</span><br><span class="line">      &quot;ipam&quot;: &#123;</span><br><span class="line">        &quot;type&quot;:&quot;host-local&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-router</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-router</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kube-router</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kube-router</span><br><span class="line">        image: ccr.ccs.tencentyun.com/library/kube-router:v1</span><br><span class="line">        args: [&quot;--run-router=false&quot;, &quot;--run-firewall=true&quot;, &quot;--run-service-proxy=false&quot;, &quot;--kubeconfig=/var/lib/kube-router/kubeconfig&quot;, &quot;--iptables-sync-period=5m&quot;, &quot;--cache-sync-timeout=3m&quot;]</span><br><span class="line">        securityContext:</span><br><span class="line">          privileged: true</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        env:</span><br><span class="line">        - name: NODE_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: spec.nodeName</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /healthz</span><br><span class="line">            port: 20244</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          periodSeconds: 3</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: lib-modules</span><br><span class="line">          mountPath: /lib/modules</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: cni-conf-dir</span><br><span class="line">          mountPath: /etc/cni/net.d</span><br><span class="line">        - name: kubeconfig</span><br><span class="line">          mountPath: /var/lib/kube-router/kubeconfig</span><br><span class="line">          readOnly: true</span><br><span class="line">      initContainers:</span><br><span class="line">      - name: install-cni</span><br><span class="line">        image: busybox</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        command:</span><br><span class="line">        - /bin/sh</span><br><span class="line">        - -c</span><br><span class="line">        - set -e -x;</span><br><span class="line">          if [ ! -f /etc/cni/net.d/10-kuberouter.conf ]; then</span><br><span class="line">            TMP=/etc/cni/net.d/.tmp-kuberouter-cfg;</span><br><span class="line">            cp /etc/kube-router/cni-conf.json $&#123;TMP&#125;;</span><br><span class="line">            mv $&#123;TMP&#125; /etc/cni/net.d/10-kuberouter.conf;</span><br><span class="line">          fi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: cni-conf-dir</span><br><span class="line">          mountPath: /etc/cni/net.d</span><br><span class="line">        - name: kube-router-cfg</span><br><span class="line">          mountPath: /etc/kube-router</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: CriticalAddonsOnly</span><br><span class="line">        operator: Exists</span><br><span class="line">      - effect: NoSchedule</span><br><span class="line">        key: node-role.kubernetes.io/master</span><br><span class="line">        operator: Exists</span><br><span class="line">      volumes:</span><br><span class="line">      - name: lib-modules</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /lib/modules</span><br><span class="line">      - name: cni-conf-dir</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /etc/cni/net.d</span><br><span class="line">      - name: kube-router-cfg</span><br><span class="line">        configMap:</span><br><span class="line">          name: kube-router-cfg</span><br><span class="line">      - name: kubeconfig</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /root/.kube/config</span><br></pre></td></tr></table></figure><p>args说明：</p><ol><li>“–run-router=false”, “–run-firewall=true”, “–run-service-proxy=false”：只加载firewall模块；</li><li>kubeconfig：用于指定master信息，映射到主机上的kubectl配置目录/root/.kube/config；</li><li>–iptables-sync-period=5m：指定定期同步iptables规则的间隔时间，根据准确性的要求设置，默认5m；</li><li>–cache-sync-timeout=3m：指定启动时将k8s资源做缓存的超时时间，默认1m；</li></ol><h1 id="NetworkPolicy配置示例"><a href="#NetworkPolicy配置示例" class="headerlink" title="NetworkPolicy配置示例"></a>NetworkPolicy配置示例</h1><h3 id="1-nsa-namespace下的pod可互相访问，而不能被其它任何pod访问"><a href="#1-nsa-namespace下的pod可互相访问，而不能被其它任何pod访问" class="headerlink" title="1.nsa namespace下的pod可互相访问，而不能被其它任何pod访问"></a>1.nsa namespace下的pod可互相访问，而不能被其它任何pod访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  ingress: </span><br><span class="line">  - from:</span><br><span class="line">    - podSelector: &#123;&#125; </span><br><span class="line">  podSelector: &#123;&#125; </span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure><h3 id="2-nsa-namespace下的pod不能被任何pod访问"><a href="#2-nsa-namespace下的pod不能被任何pod访问" class="headerlink" title="2.nsa namespace下的pod不能被任何pod访问"></a>2.nsa namespace下的pod不能被任何pod访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure><h3 id="3-nsa-namespace下的pod只在6379-TCP端口可以被带有标签app-nsb的namespace下的pod访问，而不能被其它任何pod访问"><a href="#3-nsa-namespace下的pod只在6379-TCP端口可以被带有标签app-nsb的namespace下的pod访问，而不能被其它任何pod访问" class="headerlink" title="3.nsa namespace下的pod只在6379/TCP端口可以被带有标签app: nsb的namespace下的pod访问，而不能被其它任何pod访问"></a>3.nsa namespace下的pod只在6379/TCP端口可以被带有标签app: nsb的namespace下的pod访问，而不能被其它任何pod访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: nsb</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 6379</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure><h3 id="4-nsa-namespace下的pod可以访问CIDR为14-215-0-0-16的network-endpoint的5978-TCP端口，而不能访问其它任何network-endpoints（此方式可以用来为集群内的服务开访问外部network-endpoints的白名单）"><a href="#4-nsa-namespace下的pod可以访问CIDR为14-215-0-0-16的network-endpoint的5978-TCP端口，而不能访问其它任何network-endpoints（此方式可以用来为集群内的服务开访问外部network-endpoints的白名单）" class="headerlink" title="4.nsa namespace下的pod可以访问CIDR为14.215.0.0/16的network endpoint的5978/TCP端口，而不能访问其它任何network endpoints（此方式可以用来为集群内的服务开访问外部network endpoints的白名单）"></a>4.nsa namespace下的pod可以访问CIDR为14.215.0.0/16的network endpoint的5978/TCP端口，而不能访问其它任何network endpoints（此方式可以用来为集群内的服务开访问外部network endpoints的白名单）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npa</span><br><span class="line">  namespace: nsa</span><br><span class="line">spec:</span><br><span class="line">  egress:</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 14.215.0.0/16</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 5978</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Egress</span><br></pre></td></tr></table></figure><h3 id="5-default-namespace下的pod只在80-TCP端口可以被CIDR为14-215-0-0-16的network-endpoint访问，而不能被其它任何network-endpoints访问"><a href="#5-default-namespace下的pod只在80-TCP端口可以被CIDR为14-215-0-0-16的network-endpoint访问，而不能被其它任何network-endpoints访问" class="headerlink" title="5.default namespace下的pod只在80/TCP端口可以被CIDR为14.215.0.0/16的network endpoint访问，而不能被其它任何network endpoints访问"></a>5.default namespace下的pod只在80/TCP端口可以被CIDR为14.215.0.0/16的network endpoint访问，而不能被其它任何network endpoints访问</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: npd</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 14.215.0.0/16</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure><h1 id="附-测试情况"><a href="#附-测试情况" class="headerlink" title="附: 测试情况"></a>附: 测试情况</h1><table><thead><tr><th style="text-align:left">用例名称</th><th style="text-align:left">测试结果</th></tr></thead><tbody><tr><td style="text-align:left">不同namespace的pod互相隔离，同一namespace的pod互通</td><td style="text-align:left">通过</td></tr><tr><td style="text-align:left">不同namespace的pod互相隔离，同一namespace的pod隔离</td><td style="text-align:left">通过</td></tr><tr><td style="text-align:left">不同namespace的pod互相隔离，白名单指定B可以访问A</td><td style="text-align:left">通过</td></tr><tr><td style="text-align:left">允许某个namespace访问集群外某个CIDR，其他外部IP全部隔离</td><td style="text-align:left">通过</td></tr><tr><td style="text-align:left">不同namespace的pod互相隔离，白名单指定B可以访问A中对应的pod以及端口</td><td style="text-align:left">通过</td></tr><tr><td style="text-align:left">以上用例，当source pod 和 destination pod在一个node上时，隔离是否生效</td><td style="text-align:left">通过</td></tr></tbody></table><p>功能测试用例</p><blockquote><p><a href="https://ask.qcloudimg.com/draft/982360/dgs7x4hcly.zip" target="_blank" rel="noopener">#kube-router测试用例.xlsx.zip#</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Author: &lt;a href=&quot;https://github.com/jimmy-zh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Jimmy Zhang&lt;/a&gt; (张浩)&lt;/p&gt;
&lt;h1 id=&quot;Network-Policy&quot;&gt;&lt;a href=&quot;#N
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kubernetes集群中夺命的5秒DNS延迟</title>
    <link href="https://TencentCloudContainerTeam.github.io/2018/10/26/DNS-5-seconds-delay/"/>
    <id>https://TencentCloudContainerTeam.github.io/2018/10/26/DNS-5-seconds-delay/</id>
    <published>2018-10-26T07:52:37.000Z</published>
    <updated>2019-01-10T06:19:59.604Z</updated>
    
    <content type="html"><![CDATA[<p>作者： 洪志国</p><h2 id="超时问题"><a href="#超时问题" class="headerlink" title="超时问题"></a>超时问题</h2><p>客户反馈从pod中访问服务时，总是有些请求的响应时延会达到5秒。正常的响应只需要毫秒级别的时延。</p><h2 id="DNS-5秒延时"><a href="#DNS-5秒延时" class="headerlink" title="DNS 5秒延时"></a>DNS 5秒延时</h2><p>在pod中(通过nsenter -n tcpdump)抓包，发现是有的DNS请求没有收到响应，超时5秒后，再次发送DNS请求才成功收到响应。</p><p>在kube-dns pod抓包，发现是有DNS请求没有到达kube-dns pod， 在中途被丢弃了。</p><p>为什么是5秒？ <code>man resolv.conf</code>可以看到glibc的resolver的缺省超时时间是5s。</p><h2 id="丢包原因"><a href="#丢包原因" class="headerlink" title="丢包原因"></a>丢包原因</h2><p>经过搜索发现这是一个普遍问题。<br>根本原因是内核conntrack模块的bug。</p><p>Weave works的工程师<a href="martynas@weave.works">Martynas Pumputis</a>对这个问题做了很详细的分析：<br><a href="https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts" target="_blank" rel="noopener">https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts</a></p><p>相关结论：</p><ul><li>只有多个线程或进程，并发从同一个socket发送相同五元组的UDP报文时，才有一定概率会发生</li><li>glibc, musl(alpine linux的libc库)都使用”parallel query”, 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃</li><li>由于ipvs也使用了conntrack, 使用kube-proxy的ipvs模式，并不能避免这个问题</li></ul><h2 id="问题的根本解决"><a href="#问题的根本解决" class="headerlink" title="问题的根本解决"></a>问题的根本解决</h2><p>Martynas向内核提交了两个patch来fix这个问题，不过他说如果集群中有多个DNS server的情况下，问题并没有完全解决。</p><p>其中一个patch已经在2018-7-18被合并到linux内核主线中: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ed07d9a021df6da53456663a76999189badc432a" target="_blank" rel="noopener">netfilter: nf_conntrack: resolve clash for matching conntracks</a></p><p>目前只有4.19.rc 版本包含这个patch。</p><h2 id="规避办法"><a href="#规避办法" class="headerlink" title="规避办法"></a>规避办法</h2><h4 id="规避方案一：使用TCP发送DNS请求"><a href="#规避方案一：使用TCP发送DNS请求" class="headerlink" title="规避方案一：使用TCP发送DNS请求"></a>规避方案一：使用TCP发送DNS请求</h4><p>由于TCP没有这个问题，有人提出可以在容器的resolv.conf中增加<code>options use-vc</code>, 强制glibc使用TCP协议发送DNS query。下面是这个man resolv.conf中关于这个选项的说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">use-vc (since glibc 2.14)</span><br><span class="line">                     Sets RES_USEVC in _res.options.  This option forces the</span><br><span class="line">                     use of TCP for DNS resolutions.</span><br></pre></td></tr></table></figure><p>笔者使用镜像”busybox:1.29.3-glibc” (libc 2.24)  做了试验，并没有见到这样的效果，容器仍然是通过UDP发送DNS请求。</p><h4 id="规避方案二：避免相同五元组DNS请求的并发"><a href="#规避方案二：避免相同五元组DNS请求的并发" class="headerlink" title="规避方案二：避免相同五元组DNS请求的并发"></a>规避方案二：避免相同五元组DNS请求的并发</h4><p>resolv.conf还有另外两个相关的参数： </p><ul><li>single-request-reopen (since glibc 2.9)</li><li>single-request (since glibc 2.10)</li></ul><p>man resolv.conf中解释如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">single-request-reopen (since glibc 2.9)</span><br><span class="line">                     Sets RES_SNGLKUPREOP in _res.options.  The resolver</span><br><span class="line">                     uses the same socket for the A and AAAA requests.  Some</span><br><span class="line">                     hardware mistakenly sends back only one reply.  When</span><br><span class="line">                     that happens the client system will sit and wait for</span><br><span class="line">                     the second reply.  Turning this option on changes this</span><br><span class="line">                     behavior so that if two requests from the same port are</span><br><span class="line">                     not handled correctly it will close the socket and open</span><br><span class="line">                     a new one before sending the second request.</span><br><span class="line">                     </span><br><span class="line">single-request (since glibc 2.10)</span><br><span class="line">                     Sets RES_SNGLKUP in _res.options.  By default, glibc</span><br><span class="line">                     performs IPv4 and IPv6 lookups in parallel since</span><br><span class="line">                     version 2.9.  Some appliance DNS servers cannot handle</span><br><span class="line">                     these queries properly and make the requests time out.</span><br><span class="line">                     This option disables the behavior and makes glibc</span><br><span class="line">                     perform the IPv6 and IPv4 requests sequentially (at the</span><br><span class="line">                     cost of some slowdown of the resolving process).</span><br></pre></td></tr></table></figure><p>笔者做了试验，发现效果是这样的：</p><ul><li>single-request-reopen<br>发送A类型请求和AAAA类型请求使用不同的源端口。这样两个请求在conntrack表中不占用同一个表项，从而避免冲突。</li><li>single-request<br>避免并发，改为串行发送A类型和AAAA类型请求。没有了并发，从而也避免了冲突。</li></ul><p>要给容器的resolv.conf加上options参数，有几个办法：</p><h5 id="1-在容器的”ENTRYPOINT”或者”CMD”脚本中，执行-bin-echo-39-options-single-request-reopen-39-gt-gt-etc-resolv-conf"><a href="#1-在容器的”ENTRYPOINT”或者”CMD”脚本中，执行-bin-echo-39-options-single-request-reopen-39-gt-gt-etc-resolv-conf" class="headerlink" title="1) 在容器的”ENTRYPOINT”或者”CMD”脚本中，执行/bin/echo &#39;options single-request-reopen&#39; &gt;&gt; /etc/resolv.conf"></a>1) 在容器的”ENTRYPOINT”或者”CMD”脚本中，执行<code>/bin/echo &#39;options single-request-reopen&#39; &gt;&gt; /etc/resolv.conf</code></h5><h5 id="2-在pod的postStart-hook中："><a href="#2-在pod的postStart-hook中：" class="headerlink" title="2) 在pod的postStart hook中："></a>2) 在pod的postStart hook中：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lifecycle:</span><br><span class="line">  postStart:</span><br><span class="line">    exec:</span><br><span class="line">      command:</span><br><span class="line">      - /bin/sh</span><br><span class="line">      - -c </span><br><span class="line">      - &quot;/bin/echo &apos;options single-request-reopen&apos; &gt;&gt; /etc/resolv.conf&quot;</span><br></pre></td></tr></table></figure><h5 id="3-使用template-spec-dnsConfig-k8s-v1-9-及以上才支持"><a href="#3-使用template-spec-dnsConfig-k8s-v1-9-及以上才支持" class="headerlink" title="3) 使用template.spec.dnsConfig (k8s v1.9 及以上才支持):"></a>3) 使用template.spec.dnsConfig (k8s v1.9 及以上才支持):</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">template:</span><br><span class="line">  spec:</span><br><span class="line">    dnsConfig:</span><br><span class="line">      options:</span><br><span class="line">        - name: single-request-reopen</span><br></pre></td></tr></table></figure><h5 id="4-使用ConfigMap覆盖POD里面的-etc-resolv-conf"><a href="#4-使用ConfigMap覆盖POD里面的-etc-resolv-conf" class="headerlink" title="4) 使用ConfigMap覆盖POD里面的/etc/resolv.conf"></a>4) 使用ConfigMap覆盖POD里面的/etc/resolv.conf</h5><p>configmap:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  resolv.conf: |</span><br><span class="line">    nameserver 1.2.3.4</span><br><span class="line">    search default.svc.cluster.local svc.cluster.local cluster.local ec2.internal</span><br><span class="line">    options ndots:5 single-request-reopen timeout:1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: resolvconf</span><br></pre></td></tr></table></figure></p><p>POD spec:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">        volumeMounts:</span><br><span class="line">        - name: resolv-conf</span><br><span class="line">          mountPath: /etc/resolv.conf</span><br><span class="line">          subPath: resolv.conf</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">      volumes:</span><br><span class="line">      - name: resolv-conf</span><br><span class="line">        configMap:</span><br><span class="line">          name: resolvconf</span><br><span class="line">          items:</span><br><span class="line">          - key: resolv.conf</span><br><span class="line">            path: resolv.conf</span><br></pre></td></tr></table></figure></p><h5 id="5-使用MutatingAdmissionWebhook"><a href="#5-使用MutatingAdmissionWebhook" class="headerlink" title="5) 使用MutatingAdmissionWebhook"></a>5) 使用MutatingAdmissionWebhook</h5><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook-beta-in-1-9" target="_blank" rel="noopener">MutatingAdmissionWebhook</a> 是1.9引入的Controller，用于对一个指定的Resource的操作之前，对这个resource进行变更。<br>istio的自动sidecar注入就是用这个功能来实现的。 我们也可以通过MutatingAdmissionWebhook，来自动给所有POD，注入以上3)或者4)所需要的相关内容。</p><hr><p>以上方法中， 1)和2)都需要修改镜像， 3)和4)则只需要修改POD的spec， 能适用于所有镜像。不过还是有不方便的地方：</p><ul><li>每个工作负载的yaml都要做修改，比较麻烦</li><li>对于通过helm创建的工作负载，需要修改helm charts</li></ul><p>方法5)对集群使用者最省事，照常提交工作负载即可。不过初期需要一定的开发工作量。</p><h4 id="规避方案三：使用本地DNS缓存"><a href="#规避方案三：使用本地DNS缓存" class="headerlink" title="规避方案三：使用本地DNS缓存"></a>规避方案三：使用本地DNS缓存</h4><p>容器的DNS请求都发往本地的DNS缓存服务(dnsmasq, nscd等)，不需要走DNAT，也不会发生conntrack冲突。另外还有个好处，就是避免DNS服务成为性能瓶颈。</p><p>使用本地DNS缓存有两种方式：</p><ul><li>每个容器自带一个DNS缓存服务</li><li>每个节点运行一个DNS缓存服务，所有容器都把本节点的DNS缓存作为自己的nameserver</li></ul><p>从资源效率的角度来考虑的话，推荐后一种方式。</p><h5 id="实施办法"><a href="#实施办法" class="headerlink" title="实施办法"></a>实施办法</h5><p>条条大路通罗马，不管怎么做，最终到达上面描述的效果即可。</p><p>POD中要访问节点上的DNS缓存服务，可以使用节点的IP。 如果节点上的容器都连在一个虚拟bridge上， 也可以使用这个bridge的三层接口的IP(在TKE中，这个三层接口叫cbr0)。 要确保DNS缓存服务监听这个地址。</p><p>如何把POD的/etc/resolv.conf中的nameserver设置为节点IP呢？</p><p>一个办法，是设置POD.spec.dnsPolicy为”Default”， 意思是POD里面的/etc/resolv.conf， 使用节点上的文件。缺省使用节点上的/etc/resolv.conf(如果kubelet通过参数–resolv-conf指定了其他文件，则使用–resolv-conf所指定的文件)。</p><p>另一个办法，是给每个节点的kubelet指定不同的–cluster-dns参数，设置为节点的IP，POD.spec.dnsPolicy仍然使用缺省值”ClusterFirst”。 kops项目甚至有个issue在讨论如何在部署集群时设置好–cluster-dns指向节点IP: <a href="https://github.com/kubernetes/kops/issues/5584" target="_blank" rel="noopener">https://github.com/kubernetes/kops/issues/5584</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者： 洪志国&lt;/p&gt;
&lt;h2 id=&quot;超时问题&quot;&gt;&lt;a href=&quot;#超时问题&quot; class=&quot;headerlink&quot; title=&quot;超时问题&quot;&gt;&lt;/a&gt;超时问题&lt;/h2&gt;&lt;p&gt;客户反馈从pod中访问服务时，总是有些请求的响应时延会达到5秒。正常的响应只需要毫秒级别的时延
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>开源组件</title>
    <link href="https://TencentCloudContainerTeam.github.io/2018/10/21/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"/>
    <id>https://TencentCloudContainerTeam.github.io/2018/10/21/开源项目/</id>
    <published>2018-10-21T10:27:56.000Z</published>
    <updated>2019-01-10T06:19:59.608Z</updated>
    
    <content type="html"><![CDATA[<p>腾讯云容器团队现有开源组件：</p><ul><li>基于 csi 的 <a href="https://github.com/TencentCloud/kubernetes-csi-tencentcloud" target="_blank" rel="noopener">kubernetes volume 插件</a></li><li>基于 cni 的 <a href="https://github.com/TencentCloud/cni-bridge-networking" target="_blank" rel="noopener">bridge 插件</a></li><li>适配黑石负载均衡的 <a href="https://github.com/TencentCloud/ingress-tke-bm" target="_blank" rel="noopener">ingress 插件</a></li><li>适配腾讯云 cvm/clb/vpc 的 <a href="https://github.com/TencentCloud/tencentcloud-cloud-controller-manager" target="_blank" rel="noopener">kubernetes cloud-controller-manager</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;腾讯云容器团队现有开源组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于 csi 的 &lt;a href=&quot;https://github.com/TencentCloud/kubernetes-csi-tencentcloud&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;
      
    
    </summary>
    
    
  </entry>
  
</feed>
